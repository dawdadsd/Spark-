2025-02-14 00:09:08.782 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 23912 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 00:09:08.786 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 00:09:08.855 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 00:09:08.862 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 00:09:08.863 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 00:09:08.863 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 00:09:10.629 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 00:09:10.646 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 00:09:10.647 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 00:09:10.800 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 00:09:10.802 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1937 ms
2025-02-14 00:09:10.875 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Initializing Spark configuration with hadoop home: C:UsersxubeiDesktophadoop
2025-02-14 00:09:11.318 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 00:09:11.324 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 00:09:11.340 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 00:09:11.366 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 25 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:52)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 00:40:28.280 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 20840 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 00:40:28.285 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 00:40:28.345 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 00:40:28.346 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 00:40:28.348 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 00:40:28.348 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 00:40:30.048 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 00:40:30.068 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 00:40:30.074 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 00:40:30.075 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 00:40:30.233 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 00:40:30.236 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1886 ms
2025-02-14 00:40:30.320 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Initializing Spark configuration with hadoop home: C:UsersxubeiDesktophadoop
2025-02-14 00:40:30.754 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 00:40:30.759 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 00:40:30.777 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 00:40:30.799 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 25 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:58)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 00:44:22.852 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 6412 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 00:44:22.854 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 00:44:22.895 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 00:44:22.896 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 00:44:22.896 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 00:44:22.896 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 00:44:23.870 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 00:44:23.879 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 00:44:23.882 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 00:44:23.882 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 00:44:24.001 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 00:44:24.003 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1105 ms
2025-02-14 00:44:24.055 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Initializing Spark configuration with hadoop home: C:UsersxubeiDesktophadoop
2025-02-14 00:44:24.388 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 00:44:24.393 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 00:44:24.406 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 00:44:24.423 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 25 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:58)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 00:52:54.495 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 14452 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 00:52:54.497 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 00:52:54.538 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 00:52:54.539 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 00:52:54.539 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 00:52:54.540 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 00:52:55.505 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 00:52:55.512 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 00:52:55.515 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 00:52:55.516 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 00:52:55.637 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 00:52:55.639 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1098 ms
2025-02-14 00:52:56.002 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 00:52:56.008 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 00:52:56.023 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 00:52:56.039 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 25 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:51)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 01:52:19.820 [main] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 2804 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 01:52:19.823 [main] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 01:52:20.676 [main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 01:52:20.692 [main] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 01:52:20.695 [main] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 01:52:20.695 [main] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 01:52:20.830 [main] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 01:52:20.832 [main] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 970 ms
2025-02-14 01:52:21.195 [main] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 01:52:21.203 [main] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 01:52:21.218 [main] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 01:52:21.233 [main] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 20 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 34 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 47 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:58)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 48 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 78 common frames omitted
2025-02-14 02:13:18.542 [main] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 19332 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 02:13:18.547 [main] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 02:13:20.188 [main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 02:13:20.197 [main] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 02:13:20.201 [main] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 02:13:20.202 [main] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 02:13:20.384 [main] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 02:13:20.387 [main] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1761 ms
2025-02-14 02:13:20.865 [main] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 02:13:20.869 [main] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 02:13:20.885 [main] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 02:13:20.901 [main] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 20 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 34 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 47 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:62)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 48 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 78 common frames omitted
2025-02-14 02:18:52.634 [main] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 2804 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 02:18:52.643 [main] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 02:18:54.204 [main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 02:18:54.221 [main] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 02:18:54.227 [main] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 02:18:54.228 [main] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 02:18:54.428 [main] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 02:18:54.430 [main] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1712 ms
2025-02-14 02:18:54.867 [main] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 02:18:54.870 [main] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 02:18:54.886 [main] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 02:18:54.905 [main] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 20 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 34 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 47 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:62)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 48 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 78 common frames omitted
2025-02-14 02:21:09.984 [main] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 8188 (C:\Users\xubei\Desktop\ʵϰdemo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\ʵϰdemo\spark)
2025-02-14 02:21:09.990 [main] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 02:21:11.658 [main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 02:21:11.670 [main] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 02:21:11.673 [main] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 02:21:11.674 [main] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 02:21:11.854 [main] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-02-14 02:21:11.857 [main] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1779 ms
2025-02-14 02:21:12.325 [main] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 02:21:12.329 [main] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 02:21:12.344 [main] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 02:21:12.366 [main] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController': Unsatisfied dependency expressed through field 'dataAnalysisService': Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:11)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 20 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 34 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 47 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:62)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 48 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 78 common frames omitted
2025-02-14 02:33:23.008 [main] ERROR o.s.boot.SpringApplication - Application run failed
java.lang.NoClassDefFoundError: org/springframework/core/type/classreading/ClassFormatException
	at org.springframework.context.annotation.AnnotationConfigApplicationContext.<init>(AnnotationConfigApplicationContext.java:71)
	at org.springframework.boot.DefaultApplicationContextFactory.createDefaultApplicationContext(DefaultApplicationContextFactory.java:61)
	at org.springframework.boot.DefaultApplicationContextFactory.getFromSpringFactories(DefaultApplicationContextFactory.java:75)
	at org.springframework.boot.DefaultApplicationContextFactory.create(DefaultApplicationContextFactory.java:50)
	at org.springframework.boot.SpringApplication.createApplicationContext(SpringApplication.java:588)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:10)
Caused by: java.lang.ClassNotFoundException: org.springframework.core.type.classreading.ClassFormatException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 9 common frames omitted
2025-02-14 02:35:15.009 [main] ERROR o.s.boot.SpringApplication - Application run failed
java.lang.NoClassDefFoundError: org/springframework/core/type/classreading/ClassFormatException
	at org.springframework.context.annotation.AnnotationConfigApplicationContext.<init>(AnnotationConfigApplicationContext.java:71)
	at org.springframework.boot.DefaultApplicationContextFactory.createDefaultApplicationContext(DefaultApplicationContextFactory.java:61)
	at org.springframework.boot.DefaultApplicationContextFactory.getFromSpringFactories(DefaultApplicationContextFactory.java:75)
	at org.springframework.boot.DefaultApplicationContextFactory.create(DefaultApplicationContextFactory.java:50)
	at org.springframework.boot.SpringApplication.createApplicationContext(SpringApplication.java:588)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:10)
Caused by: java.lang.ClassNotFoundException: org.springframework.core.type.classreading.ClassFormatException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 9 common frames omitted
2025-02-14 02:37:59.343 [main] ERROR o.s.boot.SpringApplication - Application run failed
java.lang.NoClassDefFoundError: org/springframework/core/type/classreading/ClassFormatException
	at org.springframework.context.annotation.AnnotationConfigApplicationContext.<init>(AnnotationConfigApplicationContext.java:71)
	at org.springframework.boot.DefaultApplicationContextFactory.createDefaultApplicationContext(DefaultApplicationContextFactory.java:61)
	at org.springframework.boot.DefaultApplicationContextFactory.getFromSpringFactories(DefaultApplicationContextFactory.java:75)
	at org.springframework.boot.DefaultApplicationContextFactory.create(DefaultApplicationContextFactory.java:50)
	at org.springframework.boot.SpringApplication.createApplicationContext(SpringApplication.java:588)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:10)
Caused by: java.lang.ClassNotFoundException: org.springframework.core.type.classreading.ClassFormatException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 9 common frames omitted
2025-02-14 02:38:47.052 [main] ERROR o.s.boot.SpringApplication - Application run failed
java.lang.NoClassDefFoundError: org/springframework/core/type/classreading/ClassFormatException
	at org.springframework.context.annotation.AnnotationConfigApplicationContext.<init>(AnnotationConfigApplicationContext.java:71)
	at org.springframework.boot.DefaultApplicationContextFactory.createDefaultApplicationContext(DefaultApplicationContextFactory.java:61)
	at org.springframework.boot.DefaultApplicationContextFactory.getFromSpringFactories(DefaultApplicationContextFactory.java:75)
	at org.springframework.boot.DefaultApplicationContextFactory.create(DefaultApplicationContextFactory.java:50)
	at org.springframework.boot.SpringApplication.createApplicationContext(SpringApplication.java:588)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:10)
Caused by: java.lang.ClassNotFoundException: org.springframework.core.type.classreading.ClassFormatException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 9 common frames omitted
2025-02-14 03:45:24.527 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 03:45:24.529 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 03:45:24.529 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 03:45:24.645 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 03:45:25.167 [restartedMain] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\hadoop does not exist -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1712)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:99)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:334)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:59)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: java.io.FileNotFoundException: Hadoop home directory C:\hadoop does not exist
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:490)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)
	... 79 common frames omitted
2025-02-14 03:45:25.181 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-14 03:45:25.800 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 03:48:22.541 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 03:48:22.543 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 03:48:22.544 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 03:48:22.658 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 03:48:23.186 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-14 03:48:23.716 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 03:53:45.989 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 03:53:45.992 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 03:53:45.992 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 03:53:46.129 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 03:53:46.656 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-14 03:53:47.201 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 04:03:33.669 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 04:03:33.672 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 04:03:33.672 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 04:03:33.794 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 04:03:34.292 [restartedMain] WARN  org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory hadoop is not an absolute path. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1712)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:99)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:334)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:59)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: java.io.FileNotFoundException: Hadoop home directory hadoop is not an absolute path.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:486)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)
	... 79 common frames omitted
2025-02-14 04:03:34.305 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-14 04:03:34.845 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 04:12:23.348 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 04:12:23.350 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 04:12:23.351 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 04:12:23.469 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 04:12:23.523 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/hadoop
2025-02-14 04:12:23.532 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Initializing Spark Session with app name: spark-analysis
2025-02-14 04:12:24.114 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-14 04:12:24.179 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 04:12:24.726 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Cleaning up Spark resources...
2025-02-14 04:12:24.726 [restartedMain] WARN  sparkanalysis.config.SparkConfig - Error while closing Spark session
java.lang.IllegalStateException: No active or default Spark session found
	at org.apache.spark.sql.SparkSession$.$anonfun$active$2(SparkSession.scala:1202)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$.$anonfun$active$1(SparkSession.scala:1202)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$.active(SparkSession.scala:1201)
	at org.apache.spark.sql.SparkSession.active(SparkSession.scala)
	at sparkanalysis.config.SparkConfig.cleanup(SparkConfig.java:106)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeDestroyMethods(InitDestroyAnnotationBeanPostProcessor.java:415)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeDestruction(InitDestroyAnnotationBeanPostProcessor.java:239)
	at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:202)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:587)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:559)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:1200)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:520)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:1193)
	at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1125)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:628)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-14 04:12:24.728 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 04:18:29.408 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 04:18:29.410 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 04:18:29.411 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 04:18:29.530 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 04:18:29.579 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 04:18:29.589 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Initializing Spark Session with app name: spark-analysis
2025-02-14 04:18:30.041 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-14 04:18:30.087 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 04:18:30.575 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Cleaning up Spark resources...
2025-02-14 04:18:30.577 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 04:19:19.008 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 04:19:19.010 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 04:19:19.010 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 04:19:19.144 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 04:19:19.318 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 04:19:19.330 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Initializing Spark Session with app name: spark-analysis
2025-02-14 04:19:19.881 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 04:19:20.399 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Cleaning up Spark resources...
2025-02-14 04:19:20.402 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 04:21:33.643 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 04:21:33.646 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 04:21:33.646 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 04:21:33.765 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 04:21:33.889 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 04:21:33.898 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 04:21:34.408 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 04:21:34.912 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Starting cleanup of Spark resources...
2025-02-14 04:21:34.914 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 04:22:56.501 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 04:22:56.503 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 04:22:56.503 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 04:22:56.628 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 04:22:56.756 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 04:22:56.766 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 04:22:57.286 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 04:22:57.826 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Starting cleanup of Spark resources...
2025-02-14 04:22:57.828 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 04:31:29.611 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 04:31:29.613 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 04:31:29.613 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 04:31:29.732 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 04:31:29.849 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 04:31:29.850 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 04:31:29.852 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 04:31:29.852 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 04:31:29.852 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 04:31:29.852 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 04:31:29.864 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 04:31:30.352 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 04:31:30.821 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Starting cleanup of Spark resources...
2025-02-14 04:31:30.823 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 09:08:27.966 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 09:08:27.969 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 09:08:27.969 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 09:08:28.081 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 09:08:28.200 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 09:08:28.202 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 09:08:28.202 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 09:08:28.202 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 09:08:28.203 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 09:08:28.203 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 09:08:28.211 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 09:08:28.709 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 09:08:29.207 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Starting cleanup of Spark resources...
2025-02-14 09:08:29.208 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 09:21:58.701 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 09:21:58.703 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 09:21:58.703 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 09:21:58.825 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 09:21:58.949 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 09:21:58.951 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 09:21:58.952 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 09:21:58.952 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 09:21:58.952 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 09:21:58.952 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 09:21:58.961 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 09:21:59.443 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 09:21:59.912 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Starting cleanup of Spark resources...
2025-02-14 09:21:59.914 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 09:22:24.216 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 09:22:24.219 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 09:22:24.219 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 09:22:24.333 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 09:22:24.470 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 09:22:24.472 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 09:22:24.472 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 09:22:24.472 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 09:22:24.472 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 09:22:24.473 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 09:22:24.498 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 09:22:25.004 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 09:22:25.510 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Starting cleanup of Spark resources...
2025-02-14 09:22:25.513 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 09:27:24.609 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 09:27:24.612 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 09:27:24.612 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 09:27:24.731 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 09:27:24.854 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 09:27:24.856 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 09:27:24.857 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 09:27:24.857 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 09:27:24.857 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 09:27:24.857 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 09:27:24.866 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 09:27:25.337 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 09:27:25.802 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Starting cleanup of Spark resources...
2025-02-14 09:27:25.804 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 09:32:30.071 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 09:32:30.073 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 09:32:30.073 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 09:32:30.186 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 09:32:30.320 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 09:32:30.323 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 09:32:30.323 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 09:32:30.323 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 09:32:30.323 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 09:32:30.324 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 09:32:30.332 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 09:32:30.808 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 09:32:31.274 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 09:40:22.704 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 09:40:22.706 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 09:40:22.706 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 09:40:22.827 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 09:40:22.943 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 09:40:22.945 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 09:40:22.945 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 09:40:22.945 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 09:40:22.945 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 09:40:22.946 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 09:40:22.957 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 09:40:23.424 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 09:40:23.894 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:24:27.943 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 16684 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 10:24:27.945 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 10:24:27.945 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 10:24:27.988 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 10:24:27.988 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 10:24:27.988 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 10:24:27.988 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 10:24:28.964 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 10:24:28.970 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:24:28.973 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:24:28.973 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:24:29.100 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:24:29.100 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1112 ms
2025-02-14 10:24:29.158 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:24:29.159 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 10:24:29.160 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 10:24:29.160 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:24:29.160 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 10:24:29.160 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 10:24:29.171 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 10:24:29.568 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 10:24:29.569 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:24:29.587 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 10:24:29.603 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:802)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	... 24 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:116)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 10:26:41.601 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 15796 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 10:26:41.603 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 10:26:41.603 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 10:26:41.639 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 10:26:41.639 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 10:26:41.639 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 10:26:41.639 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 10:26:42.536 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 10:26:42.544 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:26:42.545 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:26:42.545 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:26:42.660 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:26:42.661 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1021 ms
2025-02-14 10:26:42.718 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:26:42.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 10:26:42.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 10:26:42.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:26:42.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 10:26:42.721 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 10:26:42.733 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 10:26:43.050 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 10:26:43.054 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:26:43.070 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 10:26:43.087 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:802)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	... 24 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:116)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 10:31:57.825 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 34176 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 10:31:57.826 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 10:31:57.827 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 10:31:57.866 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 10:31:57.866 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 10:31:57.867 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 10:31:57.867 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 10:31:58.729 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 10:31:58.738 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:31:58.739 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:31:58.739 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:31:58.854 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:31:58.854 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 987 ms
2025-02-14 10:31:58.915 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:31:58.917 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 10:31:58.917 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 10:31:58.917 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:31:58.917 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 10:31:58.918 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 10:31:58.926 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 10:31:59.288 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 10:31:59.289 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:31:59.305 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 10:31:59.319 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:802)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	... 24 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:116)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 10:37:28.904 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 7188 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 10:37:28.907 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 10:37:28.907 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 10:37:28.949 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 10:37:28.949 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 10:37:28.949 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 10:37:28.949 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 10:37:29.776 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 10:37:29.783 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:37:29.784 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:37:29.784 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:37:29.897 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:37:29.898 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 948 ms
2025-02-14 10:37:29.954 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:37:29.956 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 10:37:29.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 10:37:29.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:37:29.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 10:37:29.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 10:37:29.966 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 10:37:30.256 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 10:37:30.258 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:37:30.271 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 10:37:30.285 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:802)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	... 24 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:116)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 10:41:40.387 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 35940 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 10:41:40.389 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 10:41:40.390 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 10:41:40.430 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 10:41:40.430 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 10:41:40.430 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 10:41:40.430 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 10:41:41.253 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 10:41:41.260 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:41:41.261 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:41:41.261 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:41:41.371 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:41:41.372 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 941 ms
2025-02-14 10:41:41.425 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:41:41.426 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 10:41:41.427 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 10:41:41.427 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:41:41.427 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 10:41:41.427 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 10:41:41.436 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 10:41:41.720 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 10:41:41.721 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:41:41.735 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 10:41:41.749 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:802)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	... 24 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:116)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 10:43:19.674 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 31208 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 10:43:19.675 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 10:43:19.676 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 10:43:19.722 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 10:43:19.722 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 10:43:19.722 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 10:43:19.722 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 10:43:20.623 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 10:43:20.631 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:43:20.633 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:43:20.633 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:43:20.748 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:43:20.749 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1026 ms
2025-02-14 10:43:20.821 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:43:20.823 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkConfig': Invocation of init method failed
2025-02-14 10:43:20.825 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:43:20.844 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 10:43:20.867 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkConfig': Invocation of init method failed
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:222)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1767)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:601)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: java.lang.NoClassDefFoundError: sparkanalysis/util/HadoopConfigTest
	at sparkanalysis.config.SparkConfig.init(SparkConfig.java:52)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:401)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:219)
	... 23 common frames omitted
Caused by: java.lang.ClassNotFoundException: sparkanalysis.util.HadoopConfigTest
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.springframework.boot.devtools.restart.classloader.RestartClassLoader.loadClass(RestartClassLoader.java:121)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 31 common frames omitted
2025-02-14 10:49:50.015 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 32508 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 10:49:50.016 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 10:49:50.017 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 10:49:50.055 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 10:49:50.055 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 10:49:50.055 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 10:49:50.055 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 10:49:50.867 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 10:49:50.874 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:49:50.876 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:49:50.876 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:49:50.988 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:49:50.989 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 933 ms
2025-02-14 10:49:51.046 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:49:51.047 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 10:49:51.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 10:49:51.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:49:51.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 10:49:51.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 10:49:51.056 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 10:49:51.410 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
2025-02-14 10:49:51.411 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:49:51.426 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 10:49:51.440 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:802)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	... 24 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/slf4j/impl/StaticLoggerBinder
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
	at org.apache.spark.internal.Logging$.org$apache$spark$internal$Logging$$isLog4j12(Logging.scala:222)
	at org.apache.spark.internal.Logging.initializeLogging(Logging.scala:127)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:111)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:105)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:102)
	at org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:101)
	at org.apache.spark.SparkContext.initializeLogIfNecessary(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.log(Logging.scala:49)
	at org.apache.spark.internal.Logging.log$(Logging.scala:47)
	at org.apache.spark.SparkContext.log(SparkContext.scala:82)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.SparkContext.logInfo(SparkContext.scala:82)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:193)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:116)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 83 common frames omitted
2025-02-14 10:51:37.923 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:51:37.927 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:51:37.928 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:51:38.085 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:51:38.171 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Setting Hadoop home directory: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:51:38.173 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Validating Hadoop environment...
2025-02-14 10:51:38.174 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME: D:\jdk17\jdk-17.0.12
2025-02-14 10:51:38.174 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:51:38.175 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH includes hadoop bin: Yes
2025-02-14 10:51:38.175 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop environment validation successful
2025-02-14 10:51:38.190 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Creating new SparkSession with app name: spark-analysis
2025-02-14 10:51:39.035 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 10:51:39.837 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:52:36.661 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:52:36.663 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:52:36.664 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:52:36.780 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:52:36.834 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:52:36.836 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 10:52:36.836 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 10:52:36.836 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:52:36.836 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 10:52:36.837 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 10:52:36.846 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 10:52:37.444 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 10:52:37.957 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:54:22.987 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:54:22.990 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:54:22.990 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:54:23.106 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:54:23.159 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:54:23.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 10:54:23.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 10:54:23.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:54:23.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 10:54:23.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 10:54:23.170 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 10:54:23.773 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 10:54:24.285 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:54:32.715 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:54:32.721 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:54:32.721 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:54:32.836 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:54:32.887 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:54:32.888 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 10:54:32.889 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 10:54:32.889 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:54:32.889 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 10:54:32.889 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 10:54:32.900 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 10:55:09.624 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 10:57:50.330 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:58:09.715 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:58:09.718 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:58:09.718 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:58:09.833 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:58:09.884 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:58:09.886 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 10:58:09.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 10:58:09.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:58:09.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 10:58:09.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 10:58:09.897 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 10:58:37.353 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 10:59:40.189 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 10:59:45.745 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 10:59:45.747 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 10:59:45.749 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 10:59:45.868 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 10:59:45.925 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 10:59:45.926 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 10:59:45.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 10:59:45.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 10:59:45.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 10:59:45.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 10:59:45.938 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 10:59:54.644 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:00:13.545 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 11:00:22.871 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:00:22.874 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:00:22.874 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:00:22.988 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:00:23.046 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:00:23.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:00:23.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:00:23.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:00:23.048 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:00:23.049 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:00:23.058 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:00:23.554 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:00:24.031 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 11:06:21.235 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:06:21.237 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:06:21.237 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:06:21.348 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:06:21.399 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:06:21.401 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:06:21.402 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:06:21.402 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:06:21.402 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:06:21.402 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:06:21.411 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:06:21.911 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:06:22.632 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3916ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:06:22.722 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:06:22.755 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:06:22.756 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:06:23.031 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:20:23.187 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:20:23.190 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:20:23.190 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:20:23.307 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:20:23.354 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:20:23.355 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:20:23.357 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:20:23.357 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:20:23.357 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:20:23.357 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:20:23.366 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:20:23.843 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:20:24.517 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3798ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:20:24.600 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:20:24.621 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:20:24.623 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:20:24.897 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:20:31.425 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 11:21:24.220 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:21:24.223 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:21:24.223 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:21:24.341 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:21:24.394 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:21:24.395 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:21:24.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:21:24.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:21:24.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:21:24.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:21:24.404 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:21:24.902 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:21:25.576 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3833ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:21:25.666 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:21:25.688 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:21:25.690 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:21:25.955 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:21:36.791 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 11:22:29.384 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:22:29.386 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:22:29.387 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:22:29.501 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:22:29.552 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:22:29.553 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:22:29.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:22:29.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:22:29.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:22:29.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:22:29.568 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:22:30.061 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:22:30.738 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3813ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:22:30.827 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:22:30.859 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:22:30.859 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:22:31.146 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:22:34.992 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 11:23:15.245 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 11:23:15.548 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:23:15.549 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:23:15.549 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:23:15.642 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:23:15.651 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:23:15.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:23:15.652 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:23:15.652 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:23:15.652 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:23:15.652 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:23:15.655 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:23:15.657 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:23:15.738 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:23:15.740 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:23:15.740 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:23:15.791 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:23:25.073 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 11:23:31.191 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:23:31.193 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:23:31.194 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:23:31.305 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:23:31.352 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:23:31.354 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:23:31.354 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:23:31.354 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:23:31.354 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:23:31.355 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:23:31.365 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:23:31.840 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:23:32.530 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3818ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:23:32.614 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:23:32.636 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:23:32.638 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:23:32.906 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:23:37.042 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 11:27:43.844 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:27:43.846 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:27:43.846 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:27:43.963 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:27:44.011 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:27:44.013 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:27:44.013 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:27:44.013 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:27:44.013 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:27:44.014 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:27:44.023 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:27:44.592 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:27:45.373 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4119ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:27:45.465 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:27:45.490 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:27:45.490 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:27:45.766 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:28:31.753 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 11:32:25.772 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:32:25.774 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:32:25.775 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:32:25.883 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:32:25.933 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:32:25.935 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:32:25.935 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:32:25.935 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:32:25.936 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:32:25.936 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:32:25.944 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:32:26.429 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:32:27.108 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3819ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:32:27.192 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:32:27.218 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:32:27.219 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:32:27.484 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:36:52.007 [http-nio-8080-exec-3] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 11:44:26.837 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 11:44:26.841 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 11:44:26.841 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 11:44:26.989 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 11:44:27.048 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:44:27.049 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 11:44:27.051 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 11:44:27.051 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 11:44:27.051 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 11:44:27.051 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 11:44:27.062 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 11:44:27.562 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 11:44:28.463 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4202ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 11:44:28.580 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 11:44:28.612 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 11:44:28.613 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 11:44:28.927 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 11:44:40.657 [http-nio-8080-exec-4] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 13:17:14.927 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 开始处理 '竞彩数据.csv' 文件，路径：Data\竞彩数据.csv
2025-02-14 13:17:14.998 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 13:17:16.033 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理 '竞彩数据.csv' 时出错
org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/Data/竞彩数据.csv.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1500)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-14 13:29:53.488 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 13:29:53.491 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 13:29:53.492 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 13:29:53.612 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 13:29:53.674 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:29:53.675 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 13:29:53.676 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 13:29:53.676 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 13:29:53.676 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 13:29:53.676 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 13:29:53.689 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 13:29:54.324 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 13:29:55.137 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4748ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 13:29:55.244 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 13:29:55.761 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:29:55.761 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 13:29:56.419 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 13:30:25.483 [http-nio-8080-exec-3] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 13:30:32.882 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 开始处理 '竞彩数据.csv' 文件，路径：Data\竞彩数据.csv
2025-02-14 13:30:32.981 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 13:30:34.593 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理 '竞彩数据.csv' 时出错
org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/Data/竞彩数据.csv.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1500)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:64)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec$$$capture(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-14 13:31:19.617 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 开始处理 '竞彩数据.csv' 文件，路径：Data\竞彩数据.csv
2025-02-14 13:31:31.162 [http-nio-8080-exec-6] ERROR s.service.impl.FileServiceImpl - 处理 '竞彩数据.csv' 时出错
org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/Data/竞彩数据.csv.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1500)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:64)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec$$$capture(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-14 13:38:58.804 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 13:38:58.807 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 13:38:58.807 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 13:38:58.918 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 13:38:58.969 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:38:58.971 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 13:38:58.971 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 13:38:58.971 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 13:38:58.971 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 13:38:58.972 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 13:38:58.981 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 13:38:59.478 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 13:39:00.171 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3959ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 13:39:00.262 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 13:39:00.288 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:39:00.288 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 13:39:00.575 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 13:39:08.060 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 13:39:08.079 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 开始处理 '竞彩数据.csv' 文件，路径：Data\竞彩数据.csv
2025-02-14 13:39:08.127 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 13:39:08.914 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理 '竞彩数据.csv' 时出错
org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/Data/竞彩数据.csv.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1500)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-14 13:46:01.898 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 13:46:01.900 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 13:46:01.900 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 13:46:02.011 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 13:46:02.061 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:46:02.062 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 13:46:02.062 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 13:46:02.062 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 13:46:02.062 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 13:46:02.063 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 13:46:02.076 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 13:46:02.552 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 13:46:03.241 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4004ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 13:46:03.329 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 13:46:03.352 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:46:03.352 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 13:46:03.625 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 13:46:09.382 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 13:46:09.401 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 开始处理 '竞彩数据.csv' 文件，路径：Data\竞彩数据.csv
2025-02-14 13:46:09.452 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 13:46:10.248 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理 '竞彩数据.csv' 时出错
org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/Data/竞彩数据.csv.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1500)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-14 13:50:20.613 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 13:50:21.004 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 13:50:21.005 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 13:50:21.005 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 13:50:21.117 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 13:50:21.129 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:50:21.130 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 13:50:21.131 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 13:50:21.131 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 13:50:21.131 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 13:50:21.132 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 13:50:21.135 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 13:50:21.137 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 13:50:21.237 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 13:50:21.240 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:50:21.240 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 13:50:21.320 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 13:50:24.808 [Thread-13] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 13:50:25.083 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 13:50:25.083 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 13:50:25.083 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 13:50:25.176 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 13:50:25.184 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:50:25.185 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 13:50:25.185 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 13:50:25.185 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 13:50:25.185 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 13:50:25.186 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 13:50:25.188 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 13:50:25.190 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 13:50:25.272 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 13:50:25.273 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:50:25.274 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 13:50:25.322 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 13:59:48.622 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 13:59:48.624 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 13:59:48.626 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 13:59:48.737 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 13:59:48.785 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:59:48.787 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 13:59:48.787 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 13:59:48.787 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 13:59:48.787 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 13:59:48.788 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 13:59:48.799 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 13:59:49.294 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 13:59:49.983 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3943ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 13:59:50.073 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 13:59:50.099 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 13:59:50.100 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 13:59:50.379 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 13:59:54.544 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:01:08.939 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:01:09.264 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:01:09.264 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:01:09.264 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:01:09.351 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:01:09.360 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:01:09.361 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:01:09.362 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:01:09.362 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:01:09.362 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:01:09.362 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:01:09.364 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:01:09.366 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:01:09.453 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:01:09.464 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-02-14 14:01:21.588 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:01:21.590 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:01:21.590 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:01:21.719 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:01:21.787 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:01:21.788 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:01:21.789 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:01:21.789 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:01:21.789 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:01:21.789 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:01:21.800 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:01:22.380 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:01:23.096 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4203ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:01:23.182 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:01:23.210 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:01:23.211 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:01:23.492 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:01:39.153 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:02:46.858 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:02:46.861 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:02:46.861 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:02:46.976 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:02:47.031 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:02:47.032 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:02:47.033 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:02:47.033 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:02:47.033 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:02:47.033 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:02:47.042 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:02:47.535 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:02:48.236 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3935ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:02:48.320 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:02:48.348 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:02:48.348 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:02:48.625 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:02:57.155 [http-nio-8080-exec-3] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:04:49.361 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:04:49.363 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:04:49.363 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:04:49.477 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:04:49.527 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:04:49.528 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:04:49.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:04:49.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:04:49.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:04:49.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:04:49.538 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:04:50.039 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:04:50.752 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3921ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:04:50.837 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:04:50.861 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:04:50.862 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:04:51.149 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:07:59.592 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:07:59.594 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:07:59.594 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:07:59.706 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:07:59.753 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:07:59.754 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:07:59.756 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:07:59.756 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:07:59.756 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:07:59.756 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:07:59.765 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:08:00.245 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:08:00.922 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3989ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:08:01.015 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:08:01.042 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:08:01.043 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:08:01.326 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:08:07.161 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:08:07.186 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:08:07.186 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 竞彩数据文件不存在: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:08:07.187 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.RuntimeException: 文件不存在: 竞彩数据.csv
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:149)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:08:07.188 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: 文件不存在: 竞彩数据.csv
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:10:35.473 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:10:35.475 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:10:35.475 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:10:35.586 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:10:35.639 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:10:35.641 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:10:35.641 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:10:35.641 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:10:35.641 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:10:35.642 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:10:35.650 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:10:36.150 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:10:36.834 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4089ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:10:36.929 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:10:36.954 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:10:36.954 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:10:37.249 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:10:41.104 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:10:41.128 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:10:41.180 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:10:43.031 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.RuntimeException: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:51)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:660)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.$anonfun$complete$2(SymbolLoaders.scala:37)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:34)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:221)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.info(SynchronizedSymbols.scala:221)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:356)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:411)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.$anonfun$complete$3(SymbolLoaders.scala:83)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete(SymbolLoaders.scala:80)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:209)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.info(SynchronizedSymbols.scala:209)
	at scala.reflect.internal.Types$TypeRef.baseClasses(Types.scala:2280)
	at scala.reflect.internal.tpe.FindMembers$FindMemberBase.init(FindMembers.scala:36)
	at scala.reflect.internal.tpe.FindMembers$FindMember.init(FindMembers.scala:257)
	at scala.reflect.internal.Types$Type.$anonfun$findMember$1(Types.scala:1042)
	at scala.reflect.internal.Types$Type.findMemberInternal$1(Types.scala:1041)
	at scala.reflect.internal.Types$Type.findMember(Types.scala:1046)
	at scala.reflect.internal.Types$Type.memberBasedOnName(Types.scala:672)
	at scala.reflect.internal.Types$Type.member(Types.scala:636)
	at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:55)
	at scala.reflect.internal.Mirrors$RootsBase.requiredClass(Mirrors.scala:121)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass$lzycompute(Definitions.scala:472)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass(Definitions.scala:472)
	at scala.reflect.runtime.JavaUniverseForce.force(JavaUniverseForce.scala:291)
	at scala.reflect.runtime.JavaUniverseForce.force$(JavaUniverseForce.scala:18)
	at scala.reflect.runtime.JavaUniverse.force(JavaUniverse.scala:30)
	at scala.reflect.runtime.JavaUniverse.init(JavaUniverse.scala:162)
	at scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:93)
	at scala.reflect.runtime.package$.universe$lzycompute(package.scala:29)
	at scala.reflect.runtime.package$.universe(package.scala:29)
	at org.apache.spark.sql.Encoders$.STRING(Encoders.scala:93)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:157)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:110)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:10:43.033 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:16:28.967 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:16:28.969 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:16:28.969 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:16:29.092 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:16:29.152 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:16:29.154 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:16:29.154 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:16:29.154 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:16:29.155 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:16:29.155 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:16:29.164 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:16:29.785 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:16:30.637 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4781ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:16:30.743 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:16:30.778 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:16:30.779 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:16:31.092 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:16:34.120 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:16:34.146 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:16:34.211 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:16:37.307 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.RuntimeException: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:51)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:660)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.$anonfun$complete$2(SymbolLoaders.scala:37)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:34)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:221)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.info(SynchronizedSymbols.scala:221)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:356)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:411)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.$anonfun$complete$3(SymbolLoaders.scala:83)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete(SymbolLoaders.scala:80)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:209)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.info(SynchronizedSymbols.scala:209)
	at scala.reflect.internal.Types$TypeRef.baseClasses(Types.scala:2280)
	at scala.reflect.internal.tpe.FindMembers$FindMemberBase.init(FindMembers.scala:36)
	at scala.reflect.internal.tpe.FindMembers$FindMember.init(FindMembers.scala:257)
	at scala.reflect.internal.Types$Type.$anonfun$findMember$1(Types.scala:1042)
	at scala.reflect.internal.Types$Type.findMemberInternal$1(Types.scala:1041)
	at scala.reflect.internal.Types$Type.findMember(Types.scala:1046)
	at scala.reflect.internal.Types$Type.memberBasedOnName(Types.scala:672)
	at scala.reflect.internal.Types$Type.member(Types.scala:636)
	at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:55)
	at scala.reflect.internal.Mirrors$RootsBase.requiredClass(Mirrors.scala:121)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass$lzycompute(Definitions.scala:472)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass(Definitions.scala:472)
	at scala.reflect.runtime.JavaUniverseForce.force(JavaUniverseForce.scala:291)
	at scala.reflect.runtime.JavaUniverseForce.force$(JavaUniverseForce.scala:18)
	at scala.reflect.runtime.JavaUniverse.force(JavaUniverse.scala:30)
	at scala.reflect.runtime.JavaUniverse.init(JavaUniverse.scala:162)
	at scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:93)
	at scala.reflect.runtime.package$.universe$lzycompute(package.scala:29)
	at scala.reflect.runtime.package$.universe(package.scala:29)
	at org.apache.spark.sql.Encoders$.STRING(Encoders.scala:93)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:157)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:110)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:16:37.310 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:16:38.344 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:16:38.719 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:16:38.719 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:16:38.719 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:16:38.807 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:16:38.817 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:16:38.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:16:38.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:16:38.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:16:38.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:16:38.819 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:16:38.822 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:16:38.823 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:16:38.906 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:16:38.909 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:16:38.909 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:16:38.971 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:30.473 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:30.475 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:20:30.477 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:20:30.606 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:20:30.661 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:20:30.662 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:20:30.663 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:20:30.663 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:20:30.663 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:20:30.664 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:20:30.677 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:20:31.172 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:20:31.892 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4321ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:20:31.982 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:20:32.007 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:20:32.007 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:20:32.294 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:34.796 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:20:34.822 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:20:34.872 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:20:36.758 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.RuntimeException: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:51)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:660)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.$anonfun$complete$2(SymbolLoaders.scala:37)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:34)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:221)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.info(SynchronizedSymbols.scala:221)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:356)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:411)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.$anonfun$complete$3(SymbolLoaders.scala:83)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete(SymbolLoaders.scala:80)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:209)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.info(SynchronizedSymbols.scala:209)
	at scala.reflect.internal.Types$TypeRef.baseClasses(Types.scala:2280)
	at scala.reflect.internal.tpe.FindMembers$FindMemberBase.init(FindMembers.scala:36)
	at scala.reflect.internal.tpe.FindMembers$FindMember.init(FindMembers.scala:257)
	at scala.reflect.internal.Types$Type.$anonfun$findMember$1(Types.scala:1042)
	at scala.reflect.internal.Types$Type.findMemberInternal$1(Types.scala:1041)
	at scala.reflect.internal.Types$Type.findMember(Types.scala:1046)
	at scala.reflect.internal.Types$Type.memberBasedOnName(Types.scala:672)
	at scala.reflect.internal.Types$Type.member(Types.scala:636)
	at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:55)
	at scala.reflect.internal.Mirrors$RootsBase.requiredClass(Mirrors.scala:121)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass$lzycompute(Definitions.scala:472)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass(Definitions.scala:472)
	at scala.reflect.runtime.JavaUniverseForce.force(JavaUniverseForce.scala:291)
	at scala.reflect.runtime.JavaUniverseForce.force$(JavaUniverseForce.scala:18)
	at scala.reflect.runtime.JavaUniverse.force(JavaUniverse.scala:30)
	at scala.reflect.runtime.JavaUniverse.init(JavaUniverse.scala:162)
	at scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:93)
	at scala.reflect.runtime.package$.universe$lzycompute(package.scala:29)
	at scala.reflect.runtime.package$.universe(package.scala:29)
	at org.apache.spark.sql.Encoders$.STRING(Encoders.scala:93)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:157)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:110)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:20:36.760 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:20:37.200 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:37.562 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:37.562 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:20:37.562 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:20:37.650 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:20:37.659 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:20:37.660 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:20:37.660 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:20:37.660 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:20:37.660 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:20:37.661 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:20:37.663 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:20:37.664 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:20:37.754 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:20:37.756 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:20:37.757 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:20:37.831 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:39.288 [Thread-13] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:39.649 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:20:39.649 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:20:39.649 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:20:39.753 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:20:39.764 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:20:39.765 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:20:39.765 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:20:39.766 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:20:39.766 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:20:39.766 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:20:39.770 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:20:39.773 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:20:39.876 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:20:39.879 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:20:39.879 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:20:39.936 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:21:18.495 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:21:18.499 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:21:18.499 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:21:18.632 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:21:18.686 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:21:18.689 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:21:18.690 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:21:18.690 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:21:18.690 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:21:18.690 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:21:18.699 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:21:19.214 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:21:19.989 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4402ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:21:20.079 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:21:20.104 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:21:20.104 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:21:20.428 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:21:26.066 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:21:26.417 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:21:26.418 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:21:26.418 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:21:26.518 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:21:26.527 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:21:26.529 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:21:26.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:21:26.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:21:26.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:21:26.530 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:21:26.533 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:21:26.536 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:21:26.649 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:21:26.653 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:21:26.655 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:21:26.726 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:33:28.555 [restartedMain] INFO  sparkanalysis.SparkApplication - Starting SparkApplication using Java 17.0.12 with PID 19760 (C:\Users\xubei\Desktop\实习demo\spark\target\classes started by xubei in C:\Users\xubei\Desktop\实习demo\spark)
2025-02-14 14:33:28.557 [restartedMain] DEBUG sparkanalysis.SparkApplication - Running with Spring Boot v3.2.0, Spring v6.1.1
2025-02-14 14:33:28.557 [restartedMain] INFO  sparkanalysis.SparkApplication - No active profile set, falling back to 1 default profile: "default"
2025-02-14 14:33:28.597 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-impl\4.0.4\jaxb-impl-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/jaxb-core.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-impl/4.0.4/angus-activation.jar
2025-02-14 14:33:28.597 [restartedMain] INFO  o.s.b.d.restart.ChangeableUrls - The Class-Path manifest attribute in C:\Users\xubei\.m2\repository\com\sun\xml\bind\jaxb-core\4.0.4\jaxb-core-4.0.4.jar referenced one or more files that do not exist: file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.activation-api.jar,file:/C:/Users/xubei/.m2/repository/com/sun/xml/bind/jaxb-core/4.0.4/jakarta.xml.bind-api.jar
2025-02-14 14:33:28.597 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-02-14 14:33:28.597 [restartedMain] INFO  o.s.b.d.e.DevToolsPropertyDefaultsPostProcessor - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-02-14 14:33:29.498 [restartedMain] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8080 (http)
2025-02-14 14:33:29.622 [restartedMain] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1024 ms
2025-02-14 14:33:29.669 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:33:29.670 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:33:29.671 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:33:29.671 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:33:29.671 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:33:29.672 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:33:29.680 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:33:29.683 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/apache/logging/log4j/core/Filter
2025-02-14 14:33:29.700 [restartedMain] INFO  o.s.b.a.l.ConditionEvaluationReportLogger - 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-02-14 14:33:29.718 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'analysisController' defined in file [C:\Users\xubei\Desktop\实习demo\spark\target\classes\sparkanalysis\controller\AnalysisController.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/apache/logging/log4j/core/Filter
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:802)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataAnalysisServiceImpl': Unsatisfied dependency expressed through field 'sparkSession': Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/apache/logging/log4j/core/Filter
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:772)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	... 24 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sparkSession' defined in class path resource [sparkanalysis/config/SparkConfig.class]: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/apache/logging/log4j/core/Filter
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	... 39 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.spark.sql.SparkSession]: Factory method 'sparkSession' threw exception with message: org/apache/logging/log4j/core/Filter
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:178)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	... 52 common frames omitted
Caused by: java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/Filter
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:84)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	... 53 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.Filter
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 64 common frames omitted
2025-02-14 14:33:50.228 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:33:50.230 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:33:50.231 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:33:50.357 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:33:50.408 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:33:50.410 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:33:50.411 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:33:50.411 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:33:50.411 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:33:50.412 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:33:50.425 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:33:51.012 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:33:51.810 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4630ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:33:51.916 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:33:51.951 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:33:51.951 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:33:52.385 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:33:55.530 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:33:55.861 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:33:55.862 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:33:55.862 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:33:55.948 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:33:55.956 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:33:55.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:33:55.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:33:55.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:33:55.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:33:55.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:33:55.962 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:33:55.963 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:33:56.048 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:33:56.051 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:33:56.051 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:33:56.111 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:37:53.985 [Thread-12] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:39:40.520 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:39:40.521 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:39:40.625 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:39:40.634 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:39:40.634 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:39:40.635 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:39:40.635 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:39:40.635 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:39:40.635 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:39:40.638 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:39:40.641 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:39:40.741 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:39:40.745 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:39:40.745 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:40:36.078 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:40:36.079 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:40:36.164 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:40:36.171 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:40:36.171 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:40:36.172 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:40:36.172 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:40:36.172 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:40:36.172 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:40:36.174 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:40:36.176 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:40:36.250 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:40:36.254 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:40:36.254 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:42:12.260 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:42:12.264 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:42:12.264 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:42:12.383 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:42:12.435 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:42:12.436 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:42:12.437 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:42:12.437 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:42:12.437 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:42:12.437 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:42:12.446 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:42:12.984 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:42:13.735 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4288ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:42:13.823 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:42:13.849 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:42:13.849 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:42:14.135 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:42:17.301 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:42:17.326 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:42:17.393 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:42:19.488 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.RuntimeException: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:51)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:660)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.$anonfun$complete$2(SymbolLoaders.scala:37)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:34)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:221)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.info(SynchronizedSymbols.scala:221)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:356)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:411)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.$anonfun$complete$3(SymbolLoaders.scala:83)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete(SymbolLoaders.scala:80)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:209)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.info(SynchronizedSymbols.scala:209)
	at scala.reflect.internal.Types$TypeRef.baseClasses(Types.scala:2280)
	at scala.reflect.internal.tpe.FindMembers$FindMemberBase.init(FindMembers.scala:36)
	at scala.reflect.internal.tpe.FindMembers$FindMember.init(FindMembers.scala:257)
	at scala.reflect.internal.Types$Type.$anonfun$findMember$1(Types.scala:1042)
	at scala.reflect.internal.Types$Type.findMemberInternal$1(Types.scala:1041)
	at scala.reflect.internal.Types$Type.findMember(Types.scala:1046)
	at scala.reflect.internal.Types$Type.memberBasedOnName(Types.scala:672)
	at scala.reflect.internal.Types$Type.member(Types.scala:636)
	at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:55)
	at scala.reflect.internal.Mirrors$RootsBase.requiredClass(Mirrors.scala:121)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass$lzycompute(Definitions.scala:472)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass(Definitions.scala:472)
	at scala.reflect.runtime.JavaUniverseForce.force(JavaUniverseForce.scala:291)
	at scala.reflect.runtime.JavaUniverseForce.force$(JavaUniverseForce.scala:18)
	at scala.reflect.runtime.JavaUniverse.force(JavaUniverse.scala:30)
	at scala.reflect.runtime.JavaUniverse.init(JavaUniverse.scala:162)
	at scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:93)
	at scala.reflect.runtime.package$.universe$lzycompute(package.scala:29)
	at scala.reflect.runtime.package$.universe(package.scala:29)
	at org.apache.spark.sql.Encoders$.STRING(Encoders.scala:93)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:157)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:110)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:42:19.490 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:44:08.348 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:44:08.739 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:44:08.740 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:44:08.740 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:44:08.837 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:44:08.845 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:44:08.846 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:44:08.847 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:44:08.847 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:44:08.847 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:44:08.847 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:44:08.850 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:44:08.852 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:44:08.953 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:44:08.956 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:44:08.956 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:44:09.037 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:44:18.268 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:44:18.271 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:44:18.271 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:44:18.379 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:44:18.436 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:44:18.438 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:44:18.439 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:44:18.439 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:44:18.439 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:44:18.439 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:44:18.455 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:44:18.953 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:44:19.703 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4120ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:44:19.793 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:44:19.820 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:44:19.820 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:44:20.152 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:44:25.303 [http-nio-8080-exec-4] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-02-14 14:44:25.327 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:44:25.375 [http-nio-8080-exec-4] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:44:27.075 [http-nio-8080-exec-4] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.RuntimeException: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:51)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:660)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.$anonfun$complete$2(SymbolLoaders.scala:37)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:34)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:221)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$13.info(SynchronizedSymbols.scala:221)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:356)
	at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:411)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.$anonfun$complete$3(SymbolLoaders.scala:83)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
	at scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete(SymbolLoaders.scala:80)
	at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:209)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
	at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.info(SynchronizedSymbols.scala:209)
	at scala.reflect.internal.Types$TypeRef.baseClasses(Types.scala:2280)
	at scala.reflect.internal.tpe.FindMembers$FindMemberBase.init(FindMembers.scala:36)
	at scala.reflect.internal.tpe.FindMembers$FindMember.init(FindMembers.scala:257)
	at scala.reflect.internal.Types$Type.$anonfun$findMember$1(Types.scala:1042)
	at scala.reflect.internal.Types$Type.findMemberInternal$1(Types.scala:1041)
	at scala.reflect.internal.Types$Type.findMember(Types.scala:1046)
	at scala.reflect.internal.Types$Type.memberBasedOnName(Types.scala:672)
	at scala.reflect.internal.Types$Type.member(Types.scala:636)
	at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:55)
	at scala.reflect.internal.Mirrors$RootsBase.requiredClass(Mirrors.scala:121)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass$lzycompute(Definitions.scala:472)
	at scala.reflect.internal.Definitions$DefinitionsClass.ConsClass(Definitions.scala:472)
	at scala.reflect.runtime.JavaUniverseForce.force(JavaUniverseForce.scala:291)
	at scala.reflect.runtime.JavaUniverseForce.force$(JavaUniverseForce.scala:18)
	at scala.reflect.runtime.JavaUniverse.force(JavaUniverse.scala:30)
	at scala.reflect.runtime.JavaUniverse.init(JavaUniverse.scala:162)
	at scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:93)
	at scala.reflect.runtime.package$.universe$lzycompute(package.scala:29)
	at scala.reflect.runtime.package$.universe(package.scala:29)
	at org.apache.spark.sql.Encoders$.STRING(Encoders.scala:93)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.createBaseDataset(CSVDataSource.scala:157)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:110)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:44:27.077 [http-nio-8080-exec-4] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: error reading Scala signature of scala.collection.immutable.package: Scala signature package has wrong version
 expected: 5.0
 found: 5.2 in scala.collection.immutable.package
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:47:41.613 [Thread-10] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:47:41.931 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:47:41.932 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:47:41.932 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:47:42.019 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:47:42.028 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:47:42.029 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:47:42.030 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:47:42.030 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:47:42.030 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:47:42.030 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:47:42.032 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:47:42.033 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:47:42.115 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:47:42.119 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:47:42.119 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:47:42.176 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:48:23.716 [Thread-13] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-02-14 14:48:23.974 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-02-14 14:48:23.974 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-02-14 14:48:23.974 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
2025-02-14 14:48:24.062 [restartedMain] INFO  o.a.c.c.C.[.[localhost].[/api] - Initializing Spring embedded WebApplicationContext
2025-02-14 14:48:24.106 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-02-14 14:48:59.551 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:48:59.553 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:48:59.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:48:59.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:48:59.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:48:59.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:48:59.564 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:49:00.103 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:49:00.852 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4381ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:49:00.925 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:49:00.950 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:49:00.951 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:49:06.858 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:49:06.859 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:49:06.860 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:49:06.860 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:49:06.860 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:49:06.860 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:49:06.863 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:49:06.865 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:49:06.958 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:49:06.961 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:49:06.961 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:49:32.462 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:49:32.465 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:49:32.465 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:49:32.465 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:49:32.466 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:49:32.466 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:49:32.475 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:49:32.958 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:49:33.648 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3871ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:49:33.719 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:49:33.746 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:49:33.746 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:49:44.960 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:49:45.013 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:49:47.888 [http-nio-8080-exec-3] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_0 fail, remove all pieces of the broadcast
2025-02-14 14:49:47.895 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @49c386c8
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 14:49:47.898 [http-nio-8080-exec-3] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:53:40.384 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:53:40.385 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:53:40.385 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:53:40.385 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:53:40.385 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:53:40.386 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:53:40.388 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:53:40.391 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:53:40.474 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:53:40.476 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:53:40.476 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:54:18.743 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:54:18.745 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:54:18.745 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:54:18.746 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:54:18.746 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:54:18.746 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:54:18.754 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:54:19.233 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:54:19.953 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4032ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:54:20.031 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:54:20.061 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:54:20.061 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:54:25.959 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:54:26.015 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:54:28.820 [http-nio-8080-exec-2] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_0 fail, remove all pieces of the broadcast
2025-02-14 14:54:28.826 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @1e1a0406
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 14:54:28.829 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 14:58:16.709 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:58:16.710 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:58:16.711 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:58:16.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:58:16.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:58:16.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:58:16.718 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:58:16.722 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:58:16.867 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:58:16.870 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:58:16.871 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:58:24.648 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:58:24.650 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 14:58:24.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 14:58:24.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 14:58:24.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 14:58:24.652 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 14:58:24.661 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 14:58:25.135 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 14:58:25.831 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4072ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 14:58:25.903 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 14:58:25.926 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 14:58:25.928 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 14:58:29.090 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 14:58:29.139 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 14:58:31.846 [http-nio-8080-exec-2] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_0 fail, remove all pieces of the broadcast
2025-02-14 14:58:31.852 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @201a4587
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 14:58:31.855 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:01:06.040 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:01:06.041 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:01:06.041 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:01:06.041 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:01:06.041 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:01:06.042 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:01:06.045 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:01:06.047 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:01:06.125 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:01:06.128 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:01:06.128 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:04:28.482 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:04:28.483 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:04:28.483 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:04:28.483 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:04:28.483 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:04:28.483 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:04:28.486 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:04:28.487 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:04:28.571 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:04:28.573 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:04:28.573 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:04:56.141 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:04:56.141 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:04:56.142 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:04:56.142 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:04:56.142 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:04:56.142 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:04:56.144 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:04:56.146 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:04:56.282 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:04:56.286 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:04:56.286 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:05:27.981 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:05:27.982 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:05:27.983 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:05:27.983 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:05:27.983 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:05:27.984 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:05:27.988 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:05:27.989 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:05:28.077 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:05:28.079 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:05:28.079 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:05:33.104 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:05:33.105 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:05:33.105 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:05:33.105 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:05:33.105 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:05:33.107 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:05:33.109 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:05:33.111 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:05:33.195 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:05:33.198 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:05:33.199 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:06:05.973 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:06:05.974 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:06:05.975 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:06:05.975 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:06:05.975 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:06:05.975 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:06:05.978 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:06:05.979 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:06:06.086 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:06:06.089 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:06:06.089 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:06:38.026 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:06:38.029 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:06:38.029 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:06:38.029 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:06:38.029 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:06:38.030 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:06:38.038 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:06:38.520 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:06:39.215 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3881ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:06:39.291 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:06:39.318 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:06:39.318 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:06:44.847 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:06:44.896 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:06:47.625 [http-nio-8080-exec-3] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_0 fail, remove all pieces of the broadcast
2025-02-14 15:06:47.631 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @49c386c8
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 15:06:47.634 [http-nio-8080-exec-3] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:14:41.369 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:14:41.371 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:14:41.371 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:14:41.371 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:14:41.371 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:14:41.372 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:14:41.381 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:14:41.869 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:14:42.579 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4019ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:14:42.656 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:14:42.681 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:14:42.681 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:14:45.662 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:14:45.713 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:14:48.468 [http-nio-8080-exec-2] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_0 fail, remove all pieces of the broadcast
2025-02-14 15:14:48.474 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @3cebbb30
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 15:14:48.477 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:15:29.079 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:15:29.081 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:15:29.082 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:15:29.082 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:15:29.082 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:15:29.082 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:15:29.091 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:15:29.571 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:15:30.261 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3873ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:15:30.329 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:15:30.356 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:15:30.357 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:15:33.744 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:15:33.797 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:15:36.515 [http-nio-8080-exec-2] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_0 fail, remove all pieces of the broadcast
2025-02-14 15:15:36.520 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @3cebbb30
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 15:15:36.524 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:15:38.884 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:15:39.023 [http-nio-8080-exec-4] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_1 fail, remove all pieces of the broadcast
2025-02-14 15:15:39.024 [http-nio-8080-exec-4] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @3cebbb30
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 15:15:39.025 [http-nio-8080-exec-4] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:21:23.151 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:21:23.152 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:21:23.153 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:21:23.153 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:21:23.153 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:21:23.154 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:21:23.157 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:21:23.158 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:21:23.246 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:21:23.249 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:21:23.250 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:21:39.032 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:21:39.034 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:21:39.035 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:21:39.036 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:21:39.036 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:21:39.036 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:21:39.046 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:21:39.555 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:21:40.251 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3963ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:21:40.322 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:21:40.353 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:21:40.353 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:21:43.930 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:21:43.980 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:21:46.691 [http-nio-8080-exec-2] ERROR o.a.spark.broadcast.TorrentBroadcast - Store broadcast broadcast_0 fail, remove all pieces of the broadcast
2025-02-14 15:21:46.696 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
java.lang.IllegalArgumentException: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:65)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:43)
	at com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:396)
	at com.twitter.chill.KryoBase.newDefaultSerializer(KryoBase.scala:62)
	at com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:380)
	at com.esotericsoftware.kryo.Kryo.register(Kryo.java:410)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$2(KryoSerializer.scala:144)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:143)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)
	at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)
	at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)
	at org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1662)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)
	at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)
	at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)
	at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3326)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3549)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:156)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.reflect.InvocationTargetException: null
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:51)
	... 126 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final byte[] java.nio.ByteBuffer.hb accessible: module java.base does not "opens java.nio" to unnamed module @56528192
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.buildValidFields(FieldSerializer.java:283)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:216)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:157)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:150)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:130)
	... 132 common frames omitted
2025-02-14 15:21:46.699 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to create serializer "com.esotericsoftware.kryo.serializers.FieldSerializer" for class: java.nio.HeapByteBuffer
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:194)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:23:43.180 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:23:43.182 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:23:43.183 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:23:43.183 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:23:43.183 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:23:43.183 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:23:43.192 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:23:43.712 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:37:31.322 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:37:31.325 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:37:31.325 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:37:31.325 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:37:31.326 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:37:31.326 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:37:31.334 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:37:31.816 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:38:01.364 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:38:01.366 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:38:01.367 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:38:01.367 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:38:01.367 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:38:01.367 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:38:01.376 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:38:01.865 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:40:28.693 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:40:28.695 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:40:28.696 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:40:28.696 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:40:28.696 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:40:28.696 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:40:28.706 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:40:29.197 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:40:29.887 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3812ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:40:29.955 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:40:29.977 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:40:29.978 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:40:34.698 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:40:34.747 [http-nio-8080-exec-4] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:40:47.472 [http-nio-8080-exec-4] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`Ʊ��`, `��Ϸ`, `����`, `Ʊ����`, `�ŵ���`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('orderStatus = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [Ʊ��#17,ʡ��������#18,����������#19,�ŵ���#20L,�����ն˱��#21L,��������#22,����ʱ��#23,������Ŀ#24,���ط�ʽ#25,��Ϸ#26,Ʊ����#27,Ͷע����#28,����#29])
         +- Relation [Ʊ��#17,ʡ��������#18,����������#19,�ŵ���#20L,�����ն˱��#21L,��������#22,����ʱ��#23,������Ŀ#24,���ط�ʽ#25,��Ϸ#26,Ʊ����#27,Ͷע����#28,����#29] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:165)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:40:47.477 [http-nio-8080-exec-4] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`Ʊ��`, `��Ϸ`, `����`, `Ʊ����`, `�ŵ���`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('orderStatus = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [Ʊ��#17,ʡ��������#18,����������#19,�ŵ���#20L,�����ն˱��#21L,��������#22,����ʱ��#23,������Ŀ#24,���ط�ʽ#25,��Ϸ#26,Ʊ����#27,Ͷע����#28,����#29])
         +- Relation [Ʊ��#17,ʡ��������#18,����������#19,�ŵ���#20L,�����ն˱��#21L,��������#22,����ʱ��#23,������Ŀ#24,���ط�ʽ#25,��Ϸ#26,Ʊ����#27,Ͷע����#28,����#29] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:200)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:45:33.777 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:45:33.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:45:33.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:45:33.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:45:33.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:45:33.779 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:45:33.782 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:45:33.784 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:45:33.865 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:45:33.869 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:45:33.869 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:46:17.087 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:46:17.089 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:46:17.090 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:46:17.090 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:46:17.090 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:46:17.090 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:46:17.100 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:46:17.598 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:46:18.297 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3923ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:46:18.364 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:46:18.388 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:46:18.389 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:46:23.441 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:46:23.493 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:46:36.331 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`体育项目`, `倍数`, `投注内容`, `游戏`, `票号`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('orderStatus = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
         +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:165)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:46:36.336 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`体育项目`, `倍数`, `投注内容`, `游戏`, `票号`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('orderStatus = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
         +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:200)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:50:20.239 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:50:20.241 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:50:20.241 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:50:20.242 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:50:20.242 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:50:20.242 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:50:20.244 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:50:20.245 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:50:20.323 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:50:20.326 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:50:20.326 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:51:48.661 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:51:48.662 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:51:48.662 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:51:48.662 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:51:48.662 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:51:48.663 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:51:48.666 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:51:48.667 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:51:48.742 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:51:48.745 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:51:48.745 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:51:56.000 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:51:56.003 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:51:56.003 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:51:56.003 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:51:56.003 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:51:56.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:51:56.012 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:51:56.507 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:51:57.215 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3893ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:51:57.281 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:51:57.305 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:51:57.306 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:52:01.162 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:52:01.214 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:52:13.821 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `orderStatus` cannot be resolved. Did you mean one of the following? [`体育项目`, `倍数`, `市中心名称`, `投注内容`, `游戏`].; line 1 pos 47;
'Project [*]
+- 'Filter ((cast(票面金额 as int) > 0) AND ('orderStatus = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
         +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:165)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:52:13.827 [http-nio-8080-exec-3] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `orderStatus` cannot be resolved. Did you mean one of the following? [`体育项目`, `倍数`, `市中心名称`, `投注内容`, `游戏`].; line 1 pos 47;
'Project [*]
+- 'Filter ((cast(票面金额 as int) > 0) AND ('orderStatus = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
         +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:200)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:55:33.514 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:55:33.514 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:55:33.515 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:55:33.515 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:55:33.516 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:55:33.516 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:55:33.519 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:55:33.520 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:55:33.597 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:55:33.600 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:55:33.601 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:57:49.499 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:57:49.501 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 15:57:49.502 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 15:57:49.502 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 15:57:49.502 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 15:57:49.502 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 15:57:49.511 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 15:57:49.984 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 15:57:50.673 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3824ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 15:57:50.740 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 15:57:50.763 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 15:57:50.764 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 15:57:54.818 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 15:57:54.869 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 15:58:07.701 [http-nio-8080-exec-1] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `matchId` cannot be resolved. Did you mean one of the following? [`体育项目`, `倍数`, `市中心名称`, `投注内容`, `游戏`].; line 1 pos 7;
'Sort ['totalAmount DESC NULLS LAST], true
+- 'Aggregate ['matchId, 'matchName], ['matchId, 'matchName, 'COUNT('orderId) AS orderCount#60, sum(票面金额#31) AS totalAmount#61L, avg(票面金额#31) AS avgAmount#62]
   +- SubqueryAlias jingcai_processed
      +- View (`jingcai_processed`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
         +- Project [票号#21, 省中心名称#22, 市中心名称#23, 门店编号#24L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
            +- Filter ((票面金额#31 > 0) AND (过关方式#29 = 已完成))
               +- SubqueryAlias jingcai_raw
                  +- View (`jingcai_raw`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
                     +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:176)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 15:58:07.709 [http-nio-8080-exec-1] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `matchId` cannot be resolved. Did you mean one of the following? [`体育项目`, `倍数`, `市中心名称`, `投注内容`, `游戏`].; line 1 pos 7;
'Sort ['totalAmount DESC NULLS LAST], true
+- 'Aggregate ['matchId, 'matchName], ['matchId, 'matchName, 'COUNT('orderId) AS orderCount#60, sum(票面金额#31) AS totalAmount#61L, avg(票面金额#31) AS avgAmount#62]
   +- SubqueryAlias jingcai_processed
      +- View (`jingcai_processed`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
         +- Project [票号#21, 省中心名称#22, 市中心名称#23, 门店编号#24L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
            +- Filter ((票面金额#31 > 0) AND (过关方式#29 = 已完成))
               +- SubqueryAlias jingcai_raw
                  +- View (`jingcai_raw`, [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33])
                     +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:206)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 16:00:25.405 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:00:25.406 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:00:25.407 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:00:25.407 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:00:25.407 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:00:25.407 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:00:25.410 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:00:25.411 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:00:25.499 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:00:25.502 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:00:25.502 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:01:10.619 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:01:10.620 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:01:10.620 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:01:10.620 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:01:10.620 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:01:10.621 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:01:10.624 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:01:10.626 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:01:10.701 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:01:10.703 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:01:10.703 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:03:52.978 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:03:52.979 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:03:52.979 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:03:52.979 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:03:52.979 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:03:52.980 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:03:52.983 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:03:52.985 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:03:53.069 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:03:53.072 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:03:53.072 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:05:24.447 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:05:24.448 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:05:24.449 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:05:24.449 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:05:24.449 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:05:24.449 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:05:24.452 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:05:24.453 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:05:24.530 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:06:48.626 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:06:48.627 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:06:48.627 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:06:48.628 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:06:48.628 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:06:48.628 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:06:48.631 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:06:48.633 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:06:48.708 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:06:48.710 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:06:48.711 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:07:17.444 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:07:17.447 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:07:17.447 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:07:17.447 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:07:17.447 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:07:17.448 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:07:17.457 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:07:17.946 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:07:18.639 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3877ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 16:07:18.706 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:07:18.731 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:07:18.731 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:07:22.688 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 16:07:22.743 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 16:07:35.966 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:07:35.969 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:07:36.018 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 16:07:50.956 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:07:50.957 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:07:50.958 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 16:07:51.007 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-14 16:11:35.469 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:11:35.470 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:11:35.471 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:11:35.471 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:11:35.471 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:11:35.471 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:11:35.474 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:11:35.476 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:11:35.560 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:11:35.563 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:11:35.564 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:16:14.811 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:16:14.813 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:16:14.814 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:16:14.814 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:16:14.814 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:16:14.814 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:16:14.823 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:16:15.451 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:16:16.243 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4219ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 16:16:16.321 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:16:16.352 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:16:16.352 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:16:24.513 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 16:16:24.580 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 16:16:38.257 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:16:38.261 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:16:38.311 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 16:16:53.281 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:16:53.281 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:16:53.282 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 16:17:09.858 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-14 16:19:48.898 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:19:48.901 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:19:48.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:19:48.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:19:48.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:19:48.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:19:48.912 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:19:49.606 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:19:50.513 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4515ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 16:19:50.590 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:19:50.626 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:19:50.627 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:20:43.621 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 16:20:43.687 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 16:20:57.756 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:20:57.760 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:20:57.808 [Executor task launch worker for task 0.0 in stage 2.0 (TID 20)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 16:21:12.315 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:21:12.316 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 16:21:12.317 [Executor task launch worker for task 0.0 in stage 4.0 (TID 58)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 16:21:29.276 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-14 16:46:18.024 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:46:18.025 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:46:18.026 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:46:18.026 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:46:18.026 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:46:18.026 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:46:18.029 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:46:18.032 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:46:18.125 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:46:18.128 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:46:18.128 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:47:09.451 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:47:09.452 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:47:09.454 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:47:09.454 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:47:09.454 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:47:09.454 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:47:09.456 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:47:09.458 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:47:09.534 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:47:09.538 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:47:09.538 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:47:53.557 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:47:53.558 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:47:53.559 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:47:53.559 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:47:53.559 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:47:53.559 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:47:53.561 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:47:53.562 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:47:53.636 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:47:53.638 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:47:53.639 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:49:36.195 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:49:36.195 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:49:36.196 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:49:36.196 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:49:36.196 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:49:36.196 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:49:36.198 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:49:36.199 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:49:36.281 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:49:36.283 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:49:36.283 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 16:59:32.188 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:59:32.189 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 16:59:32.190 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 16:59:32.190 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 16:59:32.190 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 16:59:32.190 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 16:59:32.192 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 16:59:32.193 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 16:59:32.278 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 16:59:32.283 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 16:59:32.283 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:12:38.286 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:12:38.287 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:12:38.288 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:12:38.288 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:12:38.288 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:12:38.288 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:12:38.291 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:12:38.292 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:12:38.384 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:12:38.388 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:12:38.388 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:49:21.982 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:49:21.985 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:49:21.986 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:49:21.986 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:49:21.986 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:49:21.987 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:49:21.995 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:49:22.622 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:49:23.401 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4331ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 17:49:23.480 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:49:23.508 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:49:23.508 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:49:29.126 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 17:49:29.190 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 17:49:44.205 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 17:49:44.281 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ticket_amount` cannot be resolved. Did you mean one of the following? [`amount`, `status`, `orderId`, `sportType`, `shopId`].; line 1 pos 32;
'Project [*]
+- 'Filter (('ticket_amount > 0) AND ('shop_id = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [orderId#47,省中心名称#22,市中心名称#23,shopId#62L,销售终端编号#25L,销售日期#26,销售时间#27,sportType#76,status#104,游戏#30,amount#90,投注内容#32,倍数#33])
         +- Project [orderId#47, 省中心名称#22, 市中心名称#23, shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, sportType#76, 过关方式#29 AS status#104, 游戏#30, amount#90, 投注内容#32, 倍数#33]
            +- Project [orderId#47, 省中心名称#22, 市中心名称#23, shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, sportType#76, 过关方式#29, 游戏#30, 票面金额#31 AS amount#90, 投注内容#32, 倍数#33]
               +- Project [orderId#47, 省中心名称#22, 市中心名称#23, shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28 AS sportType#76, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
                  +- Project [orderId#47, 省中心名称#22, 市中心名称#23, 门店编号#24L AS shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
                     +- Project [票号#21 AS orderId#47, 省中心名称#22, 市中心名称#23, 门店编号#24L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
                        +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:185)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 17:49:44.288 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ticket_amount` cannot be resolved. Did you mean one of the following? [`amount`, `status`, `orderId`, `sportType`, `shopId`].; line 1 pos 32;
'Project [*]
+- 'Filter (('ticket_amount > 0) AND ('shop_id = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [orderId#47,省中心名称#22,市中心名称#23,shopId#62L,销售终端编号#25L,销售日期#26,销售时间#27,sportType#76,status#104,游戏#30,amount#90,投注内容#32,倍数#33])
         +- Project [orderId#47, 省中心名称#22, 市中心名称#23, shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, sportType#76, 过关方式#29 AS status#104, 游戏#30, amount#90, 投注内容#32, 倍数#33]
            +- Project [orderId#47, 省中心名称#22, 市中心名称#23, shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, sportType#76, 过关方式#29, 游戏#30, 票面金额#31 AS amount#90, 投注内容#32, 倍数#33]
               +- Project [orderId#47, 省中心名称#22, 市中心名称#23, shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28 AS sportType#76, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
                  +- Project [orderId#47, 省中心名称#22, 市中心名称#23, 门店编号#24L AS shopId#62L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
                     +- Project [票号#21 AS orderId#47, 省中心名称#22, 市中心名称#23, 门店编号#24L, 销售终端编号#25L, 销售日期#26, 销售时间#27, 体育项目#28, 过关方式#29, 游戏#30, 票面金额#31, 投注内容#32, 倍数#33]
                        +- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:258)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 17:52:34.108 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:52:34.109 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:52:34.109 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:52:34.109 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:52:34.110 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:52:34.110 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:52:34.112 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:52:34.114 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:52:34.191 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:52:34.194 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:52:34.194 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:53:44.965 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:53:44.967 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:53:44.968 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:53:44.968 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:53:44.968 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:53:44.968 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:53:44.970 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:53:44.971 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:53:45.045 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:53:45.049 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:53:45.050 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:54:01.308 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:54:01.309 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:54:01.310 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:54:01.310 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:54:01.310 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:54:01.310 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:54:01.312 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:54:01.313 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:54:01.385 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:54:01.389 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:54:01.389 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:55:52.199 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:55:52.200 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:55:52.201 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:55:52.201 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:55:52.201 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:55:52.202 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:55:52.204 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:55:52.205 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:55:52.276 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:55:52.278 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:55:52.279 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:57:44.102 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:57:44.104 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:57:44.104 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:57:44.104 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:57:44.104 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:57:44.105 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:57:44.107 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:57:44.108 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:57:44.178 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:57:44.181 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:57:44.181 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:57:59.407 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:57:59.408 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:57:59.409 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:57:59.409 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:57:59.409 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:57:59.409 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:57:59.411 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:57:59.412 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:57:59.483 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:57:59.485 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:57:59.485 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 17:59:45.145 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:59:45.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 17:59:45.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 17:59:45.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 17:59:45.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 17:59:45.148 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 17:59:45.150 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 17:59:45.151 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 17:59:45.222 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 17:59:45.224 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 17:59:45.224 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 18:00:47.739 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 18:00:47.739 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 18:00:47.739 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 18:00:47.740 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 18:00:47.740 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 18:00:47.740 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 18:00:47.742 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 18:00:47.743 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 18:00:47.813 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 18:00:47.815 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 18:00:47.815 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 18:01:24.213 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 18:01:24.215 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 18:01:24.216 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 18:01:24.216 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 18:01:24.216 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 18:01:24.216 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 18:01:24.225 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 18:01:24.699 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 18:01:25.378 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3979ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 18:01:25.446 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 18:01:25.469 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 18:01:25.470 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 18:01:30.849 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 18:01:30.901 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 18:01:45.340 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 18:01:54.507 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 0
2025-02-14 18:01:54.507 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 0
2025-02-14 18:01:54.507 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 100.00%
2025-02-14 18:01:54.507 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-14 18:02:07.487 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-14 18:02:31.975 [Executor task launch worker for task 0.0 in stage 17.0 (TID 156)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 18:02:31.981 [Executor task launch worker for task 0.0 in stage 17.0 (TID 156)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 18:02:32.003 [Executor task launch worker for task 0.0 in stage 17.0 (TID 156)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 18:02:50.638 [Executor task launch worker for task 0.0 in stage 19.0 (TID 194)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 18:02:50.639 [Executor task launch worker for task 0.0 in stage 19.0 (TID 194)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-14 18:02:50.640 [Executor task launch worker for task 0.0 in stage 19.0 (TID 194)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-14 18:02:50.696 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-14 22:55:56.732 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 22:55:56.735 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 22:55:56.735 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 22:55:56.735 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 22:55:56.736 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 22:55:56.736 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 22:55:56.745 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 22:56:08.816 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 22:56:09.527 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @15683ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 22:56:09.618 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 22:56:09.653 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 22:56:09.654 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 22:56:49.287 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 22:56:49.352 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 22:57:04.633 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 22:57:12.401 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 0
2025-02-14 22:57:12.401 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 0
2025-02-14 22:57:12.401 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 100.00%
2025-02-14 22:57:12.401 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-14 22:57:23.144 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-14 22:57:41.901 [http-nio-8080-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc]: it still exists.
2025-02-14 22:57:41.902 [http-nio-8080-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\._SUCCESS.crc]: it still exists.
2025-02-14 22:57:41.903 [http-nio-8080-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet]: it still exists.
2025-02-14 22:57:41.904 [http-nio-8080-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\_SUCCESS]: it still exists.
2025-02-14 22:57:41.907 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:242)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 22:57:41.912 [http-nio-8080-exec-3] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:257)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 22:57:54.498 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 22:58:05.707 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 22:58:13.058 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 0
2025-02-14 22:58:13.058 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 0
2025-02-14 22:58:13.058 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 100.00%
2025-02-14 22:58:13.058 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-14 22:58:23.623 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-14 22:58:41.194 [http-nio-8080-exec-4] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc]: it still exists.
2025-02-14 22:58:41.195 [http-nio-8080-exec-4] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\._SUCCESS.crc]: it still exists.
2025-02-14 22:58:41.195 [http-nio-8080-exec-4] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet]: it still exists.
2025-02-14 22:58:41.195 [http-nio-8080-exec-4] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\_SUCCESS]: it still exists.
2025-02-14 22:58:41.196 [http-nio-8080-exec-4] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:242)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 22:58:41.196 [http-nio-8080-exec-4] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:257)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 22:59:53.205 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 22:59:53.209 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 22:59:53.210 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 22:59:53.210 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 22:59:53.210 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 22:59:53.210 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 22:59:53.225 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 22:59:53.756 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 22:59:54.522 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4658ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 22:59:54.615 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 22:59:54.654 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 22:59:54.655 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 23:00:05.141 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:00:05.198 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 23:07:14.714 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 23:18:05.322 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job 6 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1248)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1246)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1246)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3075)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2961)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2961)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2216)
	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:264)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:18:05.328 [http-nio-8080-exec-3] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Job 6 cancelled because SparkContext was shut down
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:257)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:20:30.898 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:20:30.901 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 23:20:30.901 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 23:20:30.901 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 23:20:30.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 23:20:30.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 23:20:30.911 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 23:20:31.473 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 23:20:32.213 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4290ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 23:20:32.304 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 23:20:32.331 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:20:32.331 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 23:20:36.450 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:20:36.657 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 23:20:41.175 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 23:20:41.184 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-14 23:24:36.152 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:24:36.154 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 23:24:36.155 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 23:24:36.155 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 23:24:36.155 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 23:24:36.155 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 23:24:36.166 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 23:24:36.701 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 23:24:37.426 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4098ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 23:24:37.515 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 23:24:37.542 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:24:37.542 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 23:24:41.847 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:24:41.911 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 23:24:46.304 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 23:24:46.463 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`票号`, `票面金额`, `过关方式`, `门店编号`, `省中心名称`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('status = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4])
         +- Relation [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:186)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:24:46.469 [http-nio-8080-exec-3] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`票号`, `票面金额`, `过关方式`, `门店编号`, `省中心名称`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('status = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4])
         +- Relation [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:260)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:33:46.963 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:33:46.966 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 23:33:46.967 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 23:33:46.967 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 23:33:46.967 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 23:33:46.967 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 23:33:46.977 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 23:33:47.495 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 23:33:48.223 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4136ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 23:33:48.312 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 23:33:48.339 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:33:48.339 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 23:33:52.216 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:33:52.282 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 23:33:56.800 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据总行数: 5006329
2025-02-14 23:33:56.964 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`票号`, `票面金额`, `过关方式`, `门店编号`, `省中心名称`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('status = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4])
         +- Relation [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:188)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:33:56.970 [http-nio-8080-exec-3] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `amount` cannot be resolved. Did you mean one of the following? [`票号`, `票面金额`, `过关方式`, `门店编号`, `省中心名称`].; line 1 pos 32;
'Project [*]
+- 'Filter (('amount > 0) AND ('status = 已完成))
   +- SubqueryAlias jingcai_raw
      +- View (`jingcai_raw`, [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4])
         +- Relation [票号#0,省中心名称#1,门店编号#2,票面金额#3,过关方式#4] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:262)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:40:04.956 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:40:04.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 23:40:04.959 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 23:40:04.959 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 23:40:04.959 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 23:40:04.959 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 23:40:04.968 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 23:40:05.477 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 23:40:06.200 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4169ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 23:40:06.286 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 23:40:06.316 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:40:06.316 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 23:40:18.350 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:40:18.424 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 23:40:21.345 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - CSV header does not conform to the schema.
 Header: 门店编号, 销售终端编号
 Schema: 票面金额, 过关方式
Expected: 票面金额 but found: 门店编号
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:40:25.546 [Executor task launch worker for task 0.0 in stage 3.0 (TID 20)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - CSV header does not conform to the schema.
 Header: 省中心名称, 门店编号, 销售终端编号, 销售日期
 Schema: 省中心名称, 票面金额, 过关方式, 游戏
Expected: 票面金额 but found: 门店编号
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:40:29.266 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 0
2025-02-14 23:40:29.267 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 0
2025-02-14 23:40:29.267 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 100.00%
2025-02-14 23:40:29.267 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-14 23:40:29.392 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 8
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:40:49.841 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-14 23:40:50.053 [Executor task launch worker for task 0.0 in stage 9.0 (TID 59)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - CSV header does not conform to the schema.
 Header: 票号, 省中心名称, 门店编号, 销售终端编号, 销售日期
 Schema: 票号, 省中心名称, 票面金额, 过关方式, 游戏
Expected: 票面金额 but found: 门店编号
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:40:55.210 [Executor task launch worker for task 0.0 in stage 10.0 (TID 78)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 8
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:41:11.289 [Executor task launch worker for task 0.0 in stage 11.0 (TID 97)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - CSV header does not conform to the schema.
 Header: 票号, 省中心名称, 门店编号, 销售终端编号, 销售日期
 Schema: 票号, 省中心名称, 票面金额, 过关方式, 游戏
Expected: 票面金额 but found: 门店编号
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:41:15.125 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc]: it still exists.
2025-02-14 23:41:15.125 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\._SUCCESS.crc]: it still exists.
2025-02-14 23:41:15.125 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet]: it still exists.
2025-02-14 23:41:15.126 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\_SUCCESS]: it still exists.
2025-02-14 23:41:15.128 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:248)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:41:15.129 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:263)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:51:44.872 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:51:44.875 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 23:51:44.875 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 23:51:44.876 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 23:51:44.876 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 23:51:44.876 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 23:51:44.885 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 23:51:45.411 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 23:51:46.173 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4196ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 23:51:46.266 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 23:51:46.295 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:51:46.295 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 23:51:49.707 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:51:49.776 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 23:51:51.824 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `province` cannot be resolved. Did you mean one of the following? [`amount`, `status`, `体育项目`, `倍数`, `市中心名称`].; line 1 pos 182;
'Sort ['totalAmount DESC NULLS LAST], true
+- 'Aggregate ['province, sportType#69], [shengName#55, sportType#69, count(票号#0) AS orderCount#96L, sum(amount#26) AS totalAmount#97, avg(amount#26) AS avgAmount#98]
   +- Filter ((amount#26 > cast(0 as double)) AND (status#41 = FSGL))
      +- SubqueryAlias jingcai_raw
         +- View (`jingcai_raw`, [票号#0,shengName#55,市中心名称#2,门店编号#3,销售终端编号#4L,销售日期#5,销售时间#6,体育项目#7,status#41,sportType#69,amount#26,投注内容#11,倍数#12])
            +- Project [票号#0, shengName#55, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, status#41, 游戏#9 AS sportType#69, amount#26, 投注内容#11, 倍数#12]
               +- Project [票号#0, 省中心名称#1 AS shengName#55, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, status#41, 游戏#9, amount#26, 投注内容#11, 倍数#12]
                  +- Project [票号#0, 省中心名称#1, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, 过关方式#8 AS status#41, 游戏#9, amount#26, 投注内容#11, 倍数#12]
                     +- Project [票号#0, 省中心名称#1, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, 过关方式#8, 游戏#9, 票面金额#10 AS amount#26, 投注内容#11, 倍数#12]
                        +- Relation [票号#0,省中心名称#1,市中心名称#2,门店编号#3,销售终端编号#4L,销售日期#5,销售时间#6,体育项目#7,过关方式#8,游戏#9,票面金额#10,投注内容#11,倍数#12] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:200)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:51:51.834 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `province` cannot be resolved. Did you mean one of the following? [`amount`, `status`, `体育项目`, `倍数`, `市中心名称`].; line 1 pos 182;
'Sort ['totalAmount DESC NULLS LAST], true
+- 'Aggregate ['province, sportType#69], [shengName#55, sportType#69, count(票号#0) AS orderCount#96L, sum(amount#26) AS totalAmount#97, avg(amount#26) AS avgAmount#98]
   +- Filter ((amount#26 > cast(0 as double)) AND (status#41 = FSGL))
      +- SubqueryAlias jingcai_raw
         +- View (`jingcai_raw`, [票号#0,shengName#55,市中心名称#2,门店编号#3,销售终端编号#4L,销售日期#5,销售时间#6,体育项目#7,status#41,sportType#69,amount#26,投注内容#11,倍数#12])
            +- Project [票号#0, shengName#55, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, status#41, 游戏#9 AS sportType#69, amount#26, 投注内容#11, 倍数#12]
               +- Project [票号#0, 省中心名称#1 AS shengName#55, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, status#41, 游戏#9, amount#26, 投注内容#11, 倍数#12]
                  +- Project [票号#0, 省中心名称#1, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, 过关方式#8 AS status#41, 游戏#9, amount#26, 投注内容#11, 倍数#12]
                     +- Project [票号#0, 省中心名称#1, 市中心名称#2, 门店编号#3, 销售终端编号#4L, 销售日期#5, 销售时间#6, 体育项目#7, 过关方式#8, 游戏#9, 票面金额#10 AS amount#26, 投注内容#11, 倍数#12]
                        +- Relation [票号#0,省中心名称#1,市中心名称#2,门店编号#3,销售终端编号#4L,销售日期#5,销售时间#6,体育项目#7,过关方式#8,游戏#9,票面金额#10,投注内容#11,倍数#12] csv

	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:268)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:52:31.860 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:52:31.863 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-14 23:52:31.864 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-14 23:52:31.864 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-14 23:52:31.864 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-14 23:52:31.864 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-14 23:52:31.875 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-14 23:52:32.394 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-14 23:52:33.123 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4169ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-14 23:52:33.213 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-14 23:52:33.243 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-14 23:52:33.243 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-14 23:52:36.604 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-14 23:52:36.667 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-14 23:52:48.120 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-14 23:52:48.120 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-14 23:52:48.120 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: -107571700.00%
2025-02-14 23:52:48.120 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-14 23:52:48.894 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-14 23:53:37.104 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc]: it still exists.
2025-02-14 23:53:37.104 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\._SUCCESS.crc]: it still exists.
2025-02-14 23:53:37.105 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet]: it still exists.
2025-02-14 23:53:37.105 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\_SUCCESS]: it still exists.
2025-02-14 23:53:37.167 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:253)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-14 23:53:37.169 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:268)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 00:03:58.038 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:03:58.042 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 00:03:58.042 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 00:03:58.043 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 00:03:58.043 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 00:03:58.043 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 00:03:58.054 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 00:03:58.577 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 00:03:59.319 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4176ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 00:03:59.409 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 00:03:59.435 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:03:59.435 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 00:04:02.745 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 00:04:02.808 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 00:04:14.324 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-15 00:04:14.324 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-15 00:04:14.324 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: -107571700.00%
2025-02-15 00:04:14.325 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-15 00:04:15.104 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-15 00:05:01.920 [http-nio-8080-exec-2] WARN  s.service.impl.FileServiceImpl - 删除旧目录失败: 4 exception(s): [org.apache.commons.io.IOIndexedException: IOException #0: Cannot delete file: Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc, org.apache.commons.io.IOIndexedException: IOException #1: Cannot delete file: Data\123\processed\details\._SUCCESS.crc, org.apache.commons.io.IOIndexedException: IOException #2: Cannot delete file: Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet, org.apache.commons.io.IOIndexedException: IOException #3: Cannot delete file: Data\123\processed\details\_SUCCESS]
2025-02-15 00:05:01.959 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc]: it still exists.
2025-02-15 00:05:01.960 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\._SUCCESS.crc]: it still exists.
2025-02-15 00:05:01.961 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet]: it still exists.
2025-02-15 00:05:01.961 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\_SUCCESS]: it still exists.
2025-02-15 00:05:02.019 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:264)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 00:05:02.021 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:276)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 00:07:04.108 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:07:04.111 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 00:07:04.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 00:07:04.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 00:07:04.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 00:07:04.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 00:07:04.124 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 00:07:04.653 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 00:07:05.410 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @5067ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 00:07:05.506 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 00:07:05.533 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:07:05.533 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 00:07:15.449 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 00:07:15.515 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 00:07:27.046 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-15 00:07:27.046 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-15 00:07:27.046 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: -107571700.00%
2025-02-15 00:07:27.046 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-15 00:07:27.972 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-15 00:08:15.790 [http-nio-8080-exec-2] WARN  s.service.impl.FileServiceImpl - 删除旧目录失败: 4 exception(s): [org.apache.commons.io.IOIndexedException: IOException #0: Cannot delete file: Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc, org.apache.commons.io.IOIndexedException: IOException #1: Cannot delete file: Data\123\processed\details\._SUCCESS.crc, org.apache.commons.io.IOIndexedException: IOException #2: Cannot delete file: Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet, org.apache.commons.io.IOIndexedException: IOException #3: Cannot delete file: Data\123\processed\details\_SUCCESS]
2025-02-15 00:08:15.830 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\.part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet.crc]: it still exists.
2025-02-15 00:08:15.831 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\._SUCCESS.crc]: it still exists.
2025-02-15 00:08:15.831 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\part-00000-c381cadc-2239-4db5-b78d-3a3f6ec39668-c000.snappy.parquet]: it still exists.
2025-02-15 00:08:15.832 [http-nio-8080-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\123\processed\details\_SUCCESS]: it still exists.
2025-02-15 00:08:15.890 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:264)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 00:08:15.892 [http-nio-8080-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/123/processed/details prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:276)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:90)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 00:09:07.217 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:09:07.220 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 00:09:07.220 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 00:09:07.220 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 00:09:07.221 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 00:09:07.221 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 00:09:07.231 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 00:09:07.721 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 00:09:08.417 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4689ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 00:09:08.502 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 00:09:08.527 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:09:08.527 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 00:09:13.991 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 00:09:14.050 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 00:09:25.298 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-15 00:09:25.298 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-15 00:09:25.298 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: -107571700.00%
2025-02-15 00:09:25.298 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-15 00:09:26.072 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-15 00:10:13.837 [Executor task launch worker for task 0.0 in stage 22.0 (TID 103)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.839 [Executor task launch worker for task 0.0 in stage 22.0 (TID 103)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.850 [Executor task launch worker for task 15.0 in stage 22.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.851 [Executor task launch worker for task 15.0 in stage 22.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.852 [Executor task launch worker for task 9.0 in stage 22.0 (TID 112)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.852 [Executor task launch worker for task 9.0 in stage 22.0 (TID 112)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.855 [Executor task launch worker for task 7.0 in stage 22.0 (TID 110)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.855 [Executor task launch worker for task 8.0 in stage 22.0 (TID 111)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.855 [Executor task launch worker for task 7.0 in stage 22.0 (TID 110)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.856 [Executor task launch worker for task 8.0 in stage 22.0 (TID 111)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.857 [Executor task launch worker for task 4.0 in stage 22.0 (TID 107)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.857 [Executor task launch worker for task 14.0 in stage 22.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.857 [Executor task launch worker for task 11.0 in stage 22.0 (TID 114)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.857 [Executor task launch worker for task 3.0 in stage 22.0 (TID 106)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.857 [Executor task launch worker for task 11.0 in stage 22.0 (TID 114)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.857 [Executor task launch worker for task 14.0 in stage 22.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.857 [Executor task launch worker for task 4.0 in stage 22.0 (TID 107)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 3.0 in stage 22.0 (TID 106)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 10.0 in stage 22.0 (TID 113)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 5.0 in stage 22.0 (TID 108)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 6.0 in stage 22.0 (TID 109)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 10.0 in stage 22.0 (TID 113)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 12.0 in stage 22.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 5.0 in stage 22.0 (TID 108)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 13.0 in stage 22.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 6.0 in stage 22.0 (TID 109)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 1.0 in stage 22.0 (TID 104)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 2.0 in stage 22.0 (TID 105)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 12.0 in stage 22.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 13.0 in stage 22.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 2.0 in stage 22.0 (TID 105)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.858 [Executor task launch worker for task 1.0 in stage 22.0 (TID 104)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:13.861 [Executor task launch worker for task 11.0 in stage 22.0 (TID 114)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 3.0 in stage 22.0 (TID 106)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 13.0 in stage 22.0 (TID 116)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 12.0 in stage 22.0 (TID 115)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 15.0 in stage 22.0 (TID 118)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 6.0 in stage 22.0 (TID 109)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 8.0 in stage 22.0 (TID 111)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 10.0 in stage 22.0 (TID 113)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 2.0 in stage 22.0 (TID 105)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 4.0 in stage 22.0 (TID 107)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 0.0 in stage 22.0 (TID 103)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 14.0 in stage 22.0 (TID 117)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 9.0 in stage 22.0 (TID 112)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 7.0 in stage 22.0 (TID 110)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 5.0 in stage 22.0 (TID 108)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:13.861 [Executor task launch worker for task 1.0 in stage 22.0 (TID 104)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:18.596 [Executor task launch worker for task 16.0 in stage 22.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:18.597 [Executor task launch worker for task 16.0 in stage 22.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:18.599 [Executor task launch worker for task 16.0 in stage 22.0 (TID 119)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:18.625 [Executor task launch worker for task 17.0 in stage 22.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:18.626 [Executor task launch worker for task 17.0 in stage 22.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:18.627 [Executor task launch worker for task 17.0 in stage 22.0 (TID 120)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:19.408 [Executor task launch worker for task 18.0 in stage 22.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:19.409 [Executor task launch worker for task 18.0 in stage 22.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:19.410 [Executor task launch worker for task 18.0 in stage 22.0 (TID 121)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:29.337 [Executor task launch worker for task 0.0 in stage 30.0 (TID 143)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:29.338 [Executor task launch worker for task 0.0 in stage 30.0 (TID 143)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:10:29.338 [Executor task launch worker for task 0.0 in stage 30.0 (TID 143)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:10:29.374 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-15 00:33:20.662 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:33:20.664 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 00:33:20.665 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 00:33:20.665 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 00:33:20.665 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 00:33:20.665 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 00:33:20.675 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 00:33:21.174 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 00:33:21.866 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3915ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 00:33:21.949 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 00:33:21.974 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 00:33:21.974 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 00:33:34.515 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 00:33:34.577 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 00:33:47.794 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 5006329
2025-02-15 00:33:47.794 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-15 00:33:47.794 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-15 00:33:47.794 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 78.51%
2025-02-15 00:33:47.794 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-15 00:33:48.604 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-15 00:34:38.183 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.185 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.189 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.189 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.190 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.191 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.192 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.193 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.194 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.195 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.195 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.195 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.197 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.197 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.197 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.197 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.197 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.197 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.197 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.198 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.199 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:38.214 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:38.214 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:42.764 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:42.765 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:42.765 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:42.765 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:42.766 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:42.766 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:43.178 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:43.179 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:43.180 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:52.749 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:52.750 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 00:34:52.751 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 00:34:52.784 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-15 14:31:49.249 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 14:31:49.252 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 14:31:49.252 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 14:31:49.252 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 14:31:49.252 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 14:31:49.253 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 14:31:49.261 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 14:31:49.978 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 14:31:50.859 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @5063ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 14:31:50.970 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 14:31:51.004 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 14:31:51.005 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 14:31:58.738 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 14:31:58.818 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 14:32:11.476 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 5006329
2025-02-15 14:32:11.477 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-15 14:32:11.477 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-15 14:32:11.477 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 78.51%
2025-02-15 14:32:11.477 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-15 14:32:12.329 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-15 14:32:55.634 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.637 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.642 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.642 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.643 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.643 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.643 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.644 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.646 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.646 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.647 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.647 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.647 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.647 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.648 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.649 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:32:55.668 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:32:55.668 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:33:00.167 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:00.167 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:00.169 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:00.169 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:33:00.169 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:00.170 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:33:00.701 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:00.702 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:00.703 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:33:09.177 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:09.177 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 14:33:09.178 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 14:33:09.210 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-15 14:49:49.392 [http-nio-8080-exec-5] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据
2025-02-15 14:49:49.393 [http-nio-8080-exec-5] ERROR s.service.impl.FileServiceImpl - 文件不存在: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据
2025-02-15 14:49:49.393 [http-nio-8080-exec-5] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据' 时出错
java.lang.RuntimeException: 文件不存在: 竞彩数据
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:79)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:64)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 14:49:49.401 [http-nio-8080-exec-5] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: 文件不存在: 竞彩数据
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:105)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:64)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 14:50:04.924 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 14:50:14.355 [http-nio-8080-exec-6] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 14:50:14.598 [Executor task launch worker for task 0.0 in stage 38.0 (TID 203)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 38.0 (TID 203)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x5ccddd20) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x5ccddd20
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 14:50:14.608 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 38.0 (TID 203) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x5ccddd20) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x5ccddd20
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 14:50:14.609 [task-result-getter-1] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 38.0 failed 1 times; aborting job
2025-02-15 14:50:14.613 [http-nio-8080-exec-6] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 203) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x5ccddd20) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x5ccddd20
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:94)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:64)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x5ccddd20) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x5ccddd20
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 14:50:14.613 [http-nio-8080-exec-6] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 203) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x5ccddd20) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x5ccddd20
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:105)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:64)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:10:00.676 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:10:00.678 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:10:00.679 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:10:00.679 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:10:00.679 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:10:00.679 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:10:00.687 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:10:01.290 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:10:02.030 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4204ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:10:02.118 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:10:02.148 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:10:02.148 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:10:14.073 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:10:14.137 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:10:26.674 [http-nio-8080-exec-3] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:10:27.255 [Executor task launch worker for task 0.0 in stage 4.0 (TID 39)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 39)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:10:27.264 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:10:27.265 [task-result-getter-3] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
2025-02-15 15:10:27.269 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:94)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:10:27.269 [http-nio-8080-exec-3] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:105)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:13:14.684 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:13:14.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:13:14.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:13:14.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:13:14.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:13:14.688 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:13:14.697 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:13:15.158 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:13:15.821 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3898ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:13:15.904 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:13:15.927 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:13:15.927 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:13:20.188 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:13:20.236 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:13:32.097 [http-nio-8080-exec-1] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:13:32.654 [Executor task launch worker for task 0.0 in stage 4.0 (TID 39)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 39)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:13:32.665 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:13:32.665 [task-result-getter-3] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
2025-02-15 15:13:32.670 [http-nio-8080-exec-1] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:94)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:13:32.671 [http-nio-8080-exec-1] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:105)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:19:46.285 [main] INFO  sparkanalysis.config.SparkConfig - HadoopĿ¼: C:/Users/xubei/Desktop/ʵϰdemo/spark/src/main/hadoop
2025-02-15 15:19:46.292 [main] INFO  sparkanalysis.util.HadoopConfigTest - ֤Hadoop...
2025-02-15 15:19:46.293 [main] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME·: D:\jdk17\jdk-17.0.12
2025-02-15 15:19:46.293 [main] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME·: C:\Users\xubei\Desktop\ʵϰdemo\spark\src\main\hadoop
2025-02-15 15:19:46.293 [main] INFO  sparkanalysis.util.HadoopConfigTest - PATHǷhadoop binĿ¼: 
2025-02-15 15:19:46.294 [main] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop֤ɹ
2025-02-15 15:19:46.336 [main] INFO  sparkanalysis.config.SparkConfig - ڴµSparkSessionӦ: spark-analysis
2025-02-15 15:19:47.037 [main] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:20:16.582 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:20:16.585 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:20:16.585 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:20:16.585 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:20:16.585 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:20:16.586 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:20:16.595 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:20:17.070 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:20:17.723 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3782ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:20:17.804 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:20:17.827 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:20:17.828 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:20:24.410 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:20:24.460 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:20:36.635 [http-nio-8080-exec-2] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:20:37.164 [Executor task launch worker for task 0.0 in stage 4.0 (TID 39)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 39)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:20:37.173 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:20:37.174 [task-result-getter-0] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
2025-02-15 15:20:37.179 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:94)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:20:37.180 [http-nio-8080-exec-2] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x96def03) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x96def03
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:105)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:26:43.931 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:26:43.934 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:26:43.934 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:26:43.934 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:26:43.934 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:26:43.935 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:26:43.944 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:26:44.404 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:26:45.075 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3908ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:26:45.157 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:26:45.182 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:26:45.183 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:26:49.249 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:26:49.301 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:27:01.409 [http-nio-8080-exec-1] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:27:01.963 [Executor task launch worker for task 0.0 in stage 4.0 (TID 39)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 39)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x67f639d3) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x67f639d3
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:27:01.973 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x67f639d3) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x67f639d3
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:27:01.974 [task-result-getter-3] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
2025-02-15 15:27:01.979 [http-nio-8080-exec-1] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x67f639d3) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x67f639d3
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:94)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x67f639d3) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x67f639d3
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:27:01.980 [http-nio-8080-exec-1] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x67f639d3) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x67f639d3
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:105)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:27:36.618 [Thread-10] ERROR o.a.spark.storage.DiskBlockManager - Exception while deleting local spark dir: C:\Users\xubei\spark-temp\blockmgr-4e70b394-b70d-4673-b892-20b547430395
java.io.IOException: Failed to delete: C:\Users\xubei\spark-temp\blockmgr-4e70b394-b70d-4673-b892-20b547430395\0c\shuffle_0_20_0.data
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:146)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:129)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:129)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)
	at org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)
	at org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)
	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2124)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2310)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2216)
	at org.apache.spark.sql.SparkSession.stop(SparkSession.scala:836)
	at org.apache.spark.sql.SparkSession.close(SparkSession.scala:844)
	at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:232)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:587)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:559)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:1200)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:520)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:1193)
	at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1125)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:1086)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:174)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:1037)
	at org.springframework.boot.devtools.restart.Restarter.stop(Restarter.java:308)
	at org.springframework.boot.devtools.restart.Restarter.lambda$restart$1(Restarter.java:250)
	at org.springframework.boot.devtools.restart.Restarter$LeakSafeThread.run(Restarter.java:607)
2025-02-15 15:27:39.852 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:27:39.853 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:27:39.855 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:27:39.855 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:27:39.855 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:27:39.855 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:27:39.857 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:27:39.858 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:27:39.930 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:27:39.933 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:27:39.933 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:32:43.394 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:32:43.395 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:32:43.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:32:43.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:32:43.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:32:43.396 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:32:43.405 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:32:43.862 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:32:44.526 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3671ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:32:44.613 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:32:44.637 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:32:44.638 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:32:50.495 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:32:50.545 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:33:02.455 [http-nio-8080-exec-1] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:33:03.009 [Executor task launch worker for task 0.0 in stage 4.0 (TID 39)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 39)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:33:03.018 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:33:03.019 [task-result-getter-3] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
2025-02-15 15:33:03.024 [http-nio-8080-exec-1] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:94)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:33:03.025 [http-nio-8080-exec-1] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:105)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:05.367 [http-nio-8080-exec-8] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:35:14.413 [http-nio-8080-exec-8] INFO  s.s.impl.DataAnalysisServiceImpl - 导出结果到MySQL表:clean_data_table
2025-02-15 15:35:40.900 [Executor task launch worker for task 2.0 in stage 12.0 (TID 85)] ERROR org.apache.spark.executor.Executor - Exception in task 2.0 in stage 12.0 (TID 85)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.900 [Executor task launch worker for task 1.0 in stage 12.0 (TID 84)] ERROR org.apache.spark.executor.Executor - Exception in task 1.0 in stage 12.0 (TID 84)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.900 [Executor task launch worker for task 3.0 in stage 12.0 (TID 86)] ERROR org.apache.spark.executor.Executor - Exception in task 3.0 in stage 12.0 (TID 86)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.900 [Executor task launch worker for task 7.0 in stage 12.0 (TID 90)] ERROR org.apache.spark.executor.Executor - Exception in task 7.0 in stage 12.0 (TID 90)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.900 [Executor task launch worker for task 6.0 in stage 12.0 (TID 89)] ERROR org.apache.spark.executor.Executor - Exception in task 6.0 in stage 12.0 (TID 89)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.900 [Executor task launch worker for task 4.0 in stage 12.0 (TID 87)] ERROR org.apache.spark.executor.Executor - Exception in task 4.0 in stage 12.0 (TID 87)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.900 [Executor task launch worker for task 0.0 in stage 12.0 (TID 83)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 12.0 (TID 83)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.900 [Executor task launch worker for task 5.0 in stage 12.0 (TID 88)] ERROR org.apache.spark.executor.Executor - Exception in task 5.0 in stage 12.0 (TID 88)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:35:40.903 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 3.0 in stage 12.0 (TID 86) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:35:40.903 [task-result-getter-3] ERROR o.a.spark.scheduler.TaskSetManager - Task 3 in stage 12.0 failed 1 times; aborting job
2025-02-15 15:35:40.908 [http-nio-8080-exec-8] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 12.0 failed 1 times, most recent failure: Lost task 3.0 in stage 12.0 (TID 86) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at sparkanalysis.service.impl.DataAnalysisServiceImpl.exportResult(DataAnalysisServiceImpl.java:79)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:137)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:35:40.909 [http-nio-8080-exec-8] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: Job aborted due to stage failure: Task 3 in stage 12.0 failed 1 times, most recent failure: Lost task 3.0 in stage 12.0 (TID 86) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:732)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:145)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:44:18.370 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:44:18.373 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:44:18.373 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:44:18.373 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:44:18.373 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:44:18.374 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:44:18.383 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:44:18.853 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:44:19.536 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3797ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:44:19.622 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:44:19.646 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:44:19.646 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:44:35.318 [http-nio-8080-exec-5] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:44:35.369 [http-nio-8080-exec-5] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:44:47.727 [http-nio-8080-exec-5] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:44:48.290 [Executor task launch worker for task 0.0 in stage 4.0 (TID 39)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 39)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:44:48.300 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:44:48.301 [task-result-getter-3] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
2025-02-15 15:44:48.306 [http-nio-8080-exec-5] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:95)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:44:48.308 [http-nio-8080-exec-5] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x437da279) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x437da279
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:106)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:45:27.668 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:45:27.671 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:45:27.672 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:45:27.672 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:45:27.672 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:45:27.672 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:45:27.680 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:45:28.151 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:46:38.900 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:46:38.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:46:38.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:46:38.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:46:38.902 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:46:38.903 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:46:38.913 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:46:39.375 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:46:40.059 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3758ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:46:40.147 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:46:40.171 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:46:40.171 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:52:03.204 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:52:03.207 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:52:03.207 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:52:03.207 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:52:03.207 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:52:03.208 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:52:03.216 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:52:03.689 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:52:45.729 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:52:45.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:52:45.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:52:45.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:52:45.732 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:52:45.732 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:52:45.741 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:52:46.216 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:56:13.765 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:56:13.767 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:56:13.767 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:56:13.767 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:56:13.767 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:56:13.769 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:56:13.778 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:56:14.305 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:56:15.063 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4175ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:56:15.155 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:56:15.180 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:56:15.180 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:56:18.192 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:56:18.242 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:56:31.828 [http-nio-8080-exec-2] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:56:32.449 [Executor task launch worker for task 0.0 in stage 4.0 (TID 39)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 39)
java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x778d1062) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x778d1062
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:56:32.459 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x778d1062) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x778d1062
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-15 15:56:32.460 [task-result-getter-3] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
2025-02-15 15:56:32.465 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 预览文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x778d1062) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x778d1062
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsList$1(Dataset.scala:3597)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:3596)
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:95)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x778d1062) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x778d1062
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-15 15:56:32.466 [http-nio-8080-exec-2] ERROR s.controller.FileController - 预览文件失败
java.lang.RuntimeException: 无法预览文件: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 39) (198.18.0.1 executor driver): java.lang.IllegalAccessError: class org.apache.spark.sql.catalyst.util.SparkDateTimeUtils (in unnamed module @0x778d1062) cannot access class sun.util.calendar.ZoneInfo (in module java.base) because module java.base does not export sun.util.calendar to unnamed module @0x778d1062
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate(SparkDateTimeUtils.scala:215)
	at org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.toJavaDate$(SparkDateTimeUtils.scala:211)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:40)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaDate(DateTimeUtils.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4067)
	at org.apache.spark.sql.Dataset$$anon$1.next(Dataset.scala:4063)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at sparkanalysis.service.impl.FileServiceImpl.previewData(FileServiceImpl.java:106)
	at sparkanalysis.controller.FileController.previewFile(FileController.java:62)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:903)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:564)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-15 15:58:02.573 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:58:02.575 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 15:58:02.575 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 15:58:02.575 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 15:58:02.575 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 15:58:02.576 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 15:58:02.585 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 15:58:03.045 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 15:58:03.716 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3755ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 15:58:03.801 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 15:58:03.826 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 15:58:03.826 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 15:58:07.002 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 读取文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 15:58:07.052 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 15:58:19.823 [http-nio-8080-exec-1] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-15 15:59:53.067 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 16:00:01.791 [http-nio-8080-exec-6] INFO  s.s.impl.DataAnalysisServiceImpl - 导出结果到MySQL表:clean_data_table
2025-02-15 16:03:19.490 [Executor task launch worker for task 1.0 in stage 18.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.490 [Executor task launch worker for task 0.0 in stage 18.0 (TID 114)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.490 [Executor task launch worker for task 2.0 in stage 18.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.490 [Executor task launch worker for task 4.0 in stage 18.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.490 [Executor task launch worker for task 7.0 in stage 18.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.490 [Executor task launch worker for task 6.0 in stage 18.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.490 [Executor task launch worker for task 3.0 in stage 18.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.490 [Executor task launch worker for task 5.0 in stage 18.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 6.0 in stage 18.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 7.0 in stage 18.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 4.0 in stage 18.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 2.0 in stage 18.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 0.0 in stage 18.0 (TID 114)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 5.0 in stage 18.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 1.0 in stage 18.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.495 [Executor task launch worker for task 3.0 in stage 18.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:03:19.515 [Executor task launch worker for task 7.0 in stage 18.0 (TID 121)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:03:19.515 [Executor task launch worker for task 0.0 in stage 18.0 (TID 114)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:03:19.515 [Executor task launch worker for task 2.0 in stage 18.0 (TID 116)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:03:19.515 [Executor task launch worker for task 6.0 in stage 18.0 (TID 120)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:03:19.515 [Executor task launch worker for task 5.0 in stage 18.0 (TID 119)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:03:19.515 [Executor task launch worker for task 1.0 in stage 18.0 (TID 115)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:03:19.515 [Executor task launch worker for task 3.0 in stage 18.0 (TID 117)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:03:19.515 [Executor task launch worker for task 4.0 in stage 18.0 (TID 118)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:17:31.835 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 16:17:31.837 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-15 16:17:31.838 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-15 16:17:31.838 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-15 16:17:31.838 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-15 16:17:31.838 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-15 16:17:31.847 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-15 16:17:32.441 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-15 16:17:33.188 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4169ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-15 16:17:33.277 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-15 16:17:33.309 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-15 16:17:33.309 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-15 16:17:35.860 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据.csv
2025-02-15 16:17:35.924 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-15 16:18:08.883 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 过滤率: 0.00%
2025-02-15 16:18:08.884 [http-nio-8080-exec-2] INFO  s.s.impl.DataAnalysisServiceImpl - 导出结果到MySQL表:clean_data_table
2025-02-15 16:21:30.949 [Executor task launch worker for task 2.0 in stage 26.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.949 [Executor task launch worker for task 6.0 in stage 26.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.949 [Executor task launch worker for task 4.0 in stage 26.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.949 [Executor task launch worker for task 3.0 in stage 26.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.949 [Executor task launch worker for task 1.0 in stage 26.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.949 [Executor task launch worker for task 5.0 in stage 26.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.949 [Executor task launch worker for task 7.0 in stage 26.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.949 [Executor task launch worker for task 0.0 in stage 26.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.954 [Executor task launch worker for task 4.0 in stage 26.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.954 [Executor task launch worker for task 7.0 in stage 26.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.954 [Executor task launch worker for task 1.0 in stage 26.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.954 [Executor task launch worker for task 5.0 in stage 26.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.954 [Executor task launch worker for task 2.0 in stage 26.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.955 [Executor task launch worker for task 3.0 in stage 26.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.955 [Executor task launch worker for task 0.0 in stage 26.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.955 [Executor task launch worker for task 6.0 in stage 26.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-15 16:21:30.979 [Executor task launch worker for task 0.0 in stage 26.0 (TID 126)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:21:30.979 [Executor task launch worker for task 3.0 in stage 26.0 (TID 129)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:21:30.979 [Executor task launch worker for task 6.0 in stage 26.0 (TID 132)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:21:30.979 [Executor task launch worker for task 4.0 in stage 26.0 (TID 130)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:21:30.979 [Executor task launch worker for task 1.0 in stage 26.0 (TID 127)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:21:30.979 [Executor task launch worker for task 2.0 in stage 26.0 (TID 128)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:21:30.979 [Executor task launch worker for task 7.0 in stage 26.0 (TID 133)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-15 16:21:30.979 [Executor task launch worker for task 5.0 in stage 26.0 (TID 131)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 22:27:18.039 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:27:18.042 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:27:18.043 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:27:18.043 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:27:18.043 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:27:18.043 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:27:18.053 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:27:18.707 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:27:19.578 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4743ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:27:19.678 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 22:27:19.718 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:27:19.718 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 22:33:11.246 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:33:11.248 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:33:11.248 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:33:11.249 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:33:11.249 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:33:11.249 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:33:11.258 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:33:11.762 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:33:12.347 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3826ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:33:12.434 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:33:12.452 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3931ms
2025-02-16 22:33:12.497 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@76c67838{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:33:12.515 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2af75797{/,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.657 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2af75797{/,null,STOPPED,@Spark}
2025-02-16 22:33:12.658 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51f27684{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.659 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40565d51{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e941f50{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c7cd75a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d33923a{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.661 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@209f96ab{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4d3f90{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50ca7a78{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d792a26{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.663 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74e0e0ee{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.663 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e47f99e{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.663 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68625b50{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.664 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de00d7d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.664 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d0458cd{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.664 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68237290{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.664 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d31655{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.666 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f1aea3a{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.666 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f3e22ac{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7eac7277{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c20226{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@326c29bd{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.668 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47dcd289{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.675 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d7629b6{/static,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@801c5cc{/,null,AVAILABLE,@Spark}
2025-02-16 22:33:12.678 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:33:12.688 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@76c67838{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:33:12.718 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:33:59.036 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:33:59.038 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:33:59.039 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:33:59.039 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:33:59.039 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:33:59.039 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:33:59.050 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:33:59.570 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:34:00.180 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3961ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:34:00.250 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:34:00.264 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4044ms
2025-02-16 22:34:00.299 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6c5ea707{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:34:00.313 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ed0089c{/,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2ed0089c{/,null,STOPPED,@Spark}
2025-02-16 22:34:00.456 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26a43abe{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.457 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50224cdb{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.457 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1711248b{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.458 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@254c7550{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.458 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51698bde{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.459 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fed4be8{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.460 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22e60419{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.460 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ace5221{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.460 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20fb0d1c{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.462 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55d955a5{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.462 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67db60f4{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.462 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@372a7a63{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.463 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@148a2fc9{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.463 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53c0aaef{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.463 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fd55db3{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.464 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67af7943{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.464 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ee07831{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.468 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72656a7d{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.469 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58325fcb{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.469 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c50f90a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.470 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476c7124{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.470 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d587f7{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f7c1b49{/static,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32868942{/,null,AVAILABLE,@Spark}
2025-02-16 22:34:00.480 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:34:00.489 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6c5ea707{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:34:00.519 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:39:17.887 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:39:17.889 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:39:17.889 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:39:17.889 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:39:17.890 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:39:17.890 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:39:17.900 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:39:18.417 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:39:19.007 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3892ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:39:19.076 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:39:19.090 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3975ms
2025-02-16 22:39:19.124 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4bcb561f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:39:19.140 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cb5963c{/,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6cb5963c{/,null,STOPPED,@Spark}
2025-02-16 22:39:19.281 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74fb659a{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fb2a414{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@174c666e{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b580486{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.283 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1583c313{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.283 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36ffa2af{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.284 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15f2bcb8{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.284 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57c193f4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.285 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d14d0bb{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.285 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b8afe81{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.286 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10a5222d{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.286 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60ceeba5{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.286 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f2d6176{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2718c895{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71c62615{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44447519{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@779d94ee{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f6a1eb3{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c5c58d1{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@317fd629{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24acfbcc{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@be46ad3{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47460678{/static,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456ad9d3{/,null,AVAILABLE,@Spark}
2025-02-16 22:39:19.299 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:39:19.307 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4bcb561f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:39:19.338 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:40:17.272 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:40:17.275 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:40:17.275 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:40:17.275 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:40:17.275 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:40:17.276 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:40:17.285 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:40:17.782 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:40:18.368 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3799ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:40:18.437 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:40:18.449 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3880ms
2025-02-16 22:40:18.484 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7aaab3bb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:40:18.498 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74021982{/,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.636 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@74021982{/,null,STOPPED,@Spark}
2025-02-16 22:40:18.640 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@291c2b04{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.641 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c7d4fe6{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.641 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cda6f4f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.642 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c42a6c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.642 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43c37b8d{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.642 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a6344ba{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.644 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@591edc2f{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.644 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f46b3e1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.644 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f1a2bea{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.645 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77fa5dd3{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.645 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a0bc0e3{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.645 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b762f40{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.647 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c6a9267{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.647 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474633ae{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.648 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5451ca0{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.648 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e8ef779{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.648 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c5eaad3{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.649 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36c6fb9f{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.649 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76ef0daa{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.650 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fb7643e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.650 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f4ed9e9{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.650 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@763e2d5a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.656 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e04da0a{/static,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.656 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d86128a{/,null,AVAILABLE,@Spark}
2025-02-16 22:40:18.658 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:40:18.668 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7aaab3bb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:40:18.699 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:42:57.223 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:42:57.225 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:42:57.226 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:42:57.226 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:42:57.226 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:42:57.226 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:42:57.236 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:42:57.751 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:42:58.388 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3876ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:42:58.456 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:42:58.471 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3960ms
2025-02-16 22:42:58.507 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2e26dfd5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:42:58.521 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18d83763{/,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.663 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@18d83763{/,null,STOPPED,@Spark}
2025-02-16 22:42:58.665 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@291b18f8{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.666 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dc76302{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58107ef0{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4199cbeb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.668 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a166a12{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.668 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77a4c8b1{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dccb862{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397f1309{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1615ec32{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a3ef689{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1faca46e{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bf72757{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a0f17b8{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.671 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e2b7b26{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.671 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e937ab1{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.671 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26be49f7{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.673 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67e8a0f0{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.673 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f4f2f79{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.673 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57c423bf{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.673 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b165f45{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.674 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@679cd739{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.674 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57af730d{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.684 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@590cdfbf{/static,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.685 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53428759{/,null,AVAILABLE,@Spark}
2025-02-16 22:42:58.687 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:42:58.697 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2e26dfd5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:42:58.726 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:43:10.001 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:43:10.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:43:10.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:43:10.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:43:10.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:43:10.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:43:10.014 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:43:10.508 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:43:11.213 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3897ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:43:11.303 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 22:43:11.330 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:43:11.330 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 22:52:42.837 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:52:42.839 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:52:42.839 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:52:42.839 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:52:42.839 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:52:42.840 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:52:42.850 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:52:43.360 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:52:43.937 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3830ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:52:44.007 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:52:44.020 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3914ms
2025-02-16 22:52:44.055 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@23a6dab{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:52:44.069 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36a4932a{/,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.210 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@36a4932a{/,null,STOPPED,@Spark}
2025-02-16 22:52:44.211 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52f7c7a9{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.212 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183989fa{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.212 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@740b7ee7{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.213 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a55a8a8{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.213 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f6fc148{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.214 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19661cda{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@253f3c8c{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@462f7363{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25ee6b0a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.216 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66aa3298{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.216 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68a145d{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7553021f{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27d8bcda{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e09ca38{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.218 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e031a85{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.218 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c97e23{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.218 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@661c8eeb{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.219 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34e16238{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.219 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d14860c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.220 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5aed6a74{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.220 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15fd0559{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.220 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50061b47{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47378f45{/static,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b510fba{/,null,AVAILABLE,@Spark}
2025-02-16 22:52:44.229 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:52:44.238 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@23a6dab{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:52:44.268 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:54:42.105 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:54:42.108 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:54:42.108 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:54:42.109 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:54:42.109 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:54:42.109 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:54:42.118 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:54:42.621 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:54:43.200 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3931ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:54:43.267 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:54:43.281 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4012ms
2025-02-16 22:54:43.315 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@c26bf63{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:54:43.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46ea3df9{/,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.469 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@46ea3df9{/,null,STOPPED,@Spark}
2025-02-16 22:54:43.470 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a78e95d{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.470 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@291c2b04{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.471 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@85a1a54{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.471 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cda6f4f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.471 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c42a6c{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.472 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43c37b8d{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.472 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39fda45c{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.473 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@591edc2f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.473 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f46b3e1{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.473 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f1a2bea{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.474 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77fa5dd3{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.474 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a0bc0e3{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.474 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b762f40{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.475 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c6a9267{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.475 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474633ae{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.476 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5451ca0{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.476 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e8ef779{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.476 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c5eaad3{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36c6fb9f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76ef0daa{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fb7643e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f4ed9e9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@763e2d5a{/static,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.485 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16107e1b{/,null,AVAILABLE,@Spark}
2025-02-16 22:54:43.487 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:54:43.496 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@c26bf63{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:54:43.526 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:57:16.836 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:57:16.839 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:57:16.840 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:57:16.840 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:57:16.840 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:57:16.840 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:57:16.850 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:57:17.367 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:57:17.953 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3832ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:57:18.021 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:57:18.034 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3914ms
2025-02-16 22:57:18.069 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5605e073{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:57:18.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c8df1{/,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.228 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7a9c8df1{/,null,STOPPED,@Spark}
2025-02-16 22:57:18.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74081e1c{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d59b96{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6026b705{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.230 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@343f08ab{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.230 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4abd5e3c{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.231 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4970baa4{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.232 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ee73a00{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.232 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b87d500{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.232 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a0601d6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.233 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9f01bf6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.233 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e73e4f0{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.233 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c11448d{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.234 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632389eb{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.234 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e4f93b7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.234 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@260416ba{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@686c05b5{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a2dcc45{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f7ea721{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124332e2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.237 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558ab7ae{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.237 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c33f4cb{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59d0bd04{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.245 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2daa21c8{/static,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.245 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@413ec117{/,null,AVAILABLE,@Spark}
2025-02-16 22:57:18.248 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:57:18.256 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5605e073{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:57:18.286 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:58:31.924 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:58:31.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:58:31.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:58:31.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:58:31.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:58:31.928 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:58:31.937 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:58:32.437 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:58:33.018 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3807ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:58:33.089 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:58:33.103 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3891ms
2025-02-16 22:58:33.138 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@c26bf63{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:58:33.152 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46ea3df9{/,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@46ea3df9{/,null,STOPPED,@Spark}
2025-02-16 22:58:33.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a78e95d{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@291c2b04{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@85a1a54{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cda6f4f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.290 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c42a6c{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.290 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43c37b8d{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.291 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39fda45c{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.291 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@591edc2f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f46b3e1{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f1a2bea{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.293 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77fa5dd3{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.293 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a0bc0e3{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.293 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b762f40{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c6a9267{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474633ae{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5451ca0{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.295 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e8ef779{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.295 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c5eaad3{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36c6fb9f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76ef0daa{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fb7643e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f4ed9e9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.306 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@763e2d5a{/static,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16107e1b{/,null,AVAILABLE,@Spark}
2025-02-16 22:58:33.311 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:58:33.319 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@c26bf63{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:58:33.349 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:59:15.797 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 22:59:15.799 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 22:59:15.800 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 22:59:15.800 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 22:59:15.800 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 22:59:15.800 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 22:59:15.810 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 22:59:16.301 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 22:59:16.883 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3781ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 22:59:16.950 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 22:59:16.962 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3859ms
2025-02-16 22:59:16.994 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6a527d1b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:59:17.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4644d6f1{/,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.145 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4644d6f1{/,null,STOPPED,@Spark}
2025-02-16 22:59:17.146 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18706d45{/jobs,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.147 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50ce65d9{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.147 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3f58fd{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.147 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ec72851{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.148 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@174a95ba{/stages,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.148 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fabbf0c{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.149 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40bc73fe{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.149 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef1f744{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.150 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae6bf8b{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.150 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ac8c5ff{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.151 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cf471b0{/storage,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.151 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5924e2e9{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.151 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79e9f3e7{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.151 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b23f6f4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.152 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@506a9361{/environment,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.152 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42dad62e{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.156 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a6664f{/executors,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.157 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33a775a9{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.157 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdf5ba8{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.158 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760122d1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.158 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad7b49{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.158 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19742f53{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.164 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7d20de{/static,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.165 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@245448a7{/,null,AVAILABLE,@Spark}
2025-02-16 22:59:17.167 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$0(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 22:59:17.176 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6a527d1b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 22:59:17.204 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$0(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:02:08.328 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:02:08.332 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:02:08.332 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:02:08.332 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:02:08.332 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:02:08.333 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:02:08.342 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:02:08.860 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:02:09.462 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4272ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:02:09.533 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:02:09.548 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4358ms
2025-02-16 23:02:09.585 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6efb0415{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:02:09.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34955c51{/,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.748 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@34955c51{/,null,STOPPED,@Spark}
2025-02-16 23:02:09.749 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3054e1c6{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59ac14c0{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a013b9{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7bb06c79{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.751 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d2d0a{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.751 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f9e211{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.752 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65a5bdaf{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.752 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f2853b7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.752 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a0b827c{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.753 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46f72e66{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.753 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a327941{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75cc21ab{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@700b2d77{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@538a72c8{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46ded82a{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.756 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fb36345{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.756 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@195c88e5{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.756 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78a9c584{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.757 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@345e9a58{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.757 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f142d94{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77a96b4a{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a3d9d38{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.764 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd0f5b2{/static,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.764 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cb99814{/,null,AVAILABLE,@Spark}
2025-02-16 23:02:09.766 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:02:09.776 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6efb0415{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:02:09.810 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:143)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:04:19.273 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:04:19.275 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:04:19.276 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:04:19.276 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:04:19.276 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:04:19.276 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:04:19.287 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:04:19.813 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:04:20.522 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3939ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:04:20.612 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:04:20.639 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:04:20.639 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:05:11.336 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:05:11.338 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:05:11.340 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:05:11.340 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:05:11.341 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:05:11.341 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:05:11.347 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:05:11.351 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:05:11.507 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:05:11.513 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:05:11.513 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:05:27.816 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:05:27.817 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:05:27.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:05:27.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:05:27.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:05:27.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:05:27.821 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:05:27.824 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:05:27.928 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:05:27.936 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:05:27.936 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:09:27.377 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:09:27.378 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:09:27.379 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:09:27.379 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:09:27.379 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:09:27.379 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:09:27.382 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:09:27.383 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:09:27.464 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:09:27.466 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:09:27.466 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:10:24.829 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:10:24.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:10:24.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:10:24.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:10:24.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:10:24.831 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:10:24.834 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:10:24.835 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:10:24.937 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:10:24.952 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @368369ms
2025-02-16 23:10:24.987 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1d33161f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:10:25.002 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42c809da{/,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.043 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@42c809da{/,null,STOPPED,@Spark}
2025-02-16 23:10:25.044 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e1406e{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.044 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78c7f2ca{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.045 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b1ad9b9{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.045 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7eb70cd{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.045 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@774a96a3{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@186c802e{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@145bda03{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64fc87cf{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d74b33d{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@573d3876{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17b514a1{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d832c4{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3141d71{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ee654da{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79d7e536{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74ccf87c{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@345de045{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.051 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e21429c{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.051 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5db305fc{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.051 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e2a7dc6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.052 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51eb9b81{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.052 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9111dfe{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.058 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79a8b14{/static,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.058 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@190a0f50{/,null,AVAILABLE,@Spark}
2025-02-16 23:10:25.062 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:10:25.070 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1d33161f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:10:25.081 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:11:22.428 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:11:22.429 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:11:22.430 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:11:22.430 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:11:22.430 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:11:22.430 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:11:22.433 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:11:22.435 [restartedMain] WARN  org.apache.spark.SparkContext - Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.base/java.lang.reflect.Method.invoke(Method.java:568)
org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
2025-02-16 23:11:22.435 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:11:22.480 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:11:22.481 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @425898ms
2025-02-16 23:11:22.494 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@58448b3d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:11:22.494 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4145a1f1{/,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.536 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4145a1f1{/,null,STOPPED,@Spark}
2025-02-16 23:11:22.536 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e3b6e81{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.536 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58ab4735{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.537 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78144225{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.537 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@743f3045{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.538 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30039f83{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.538 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@318a9d90{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.538 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10648171{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.539 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@184a487c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.539 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f521869{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.539 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@518ee88a{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.539 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e123ca8{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.540 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69932ee2{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.540 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20fca59b{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.540 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@658b12f9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.540 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e69141d{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.541 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e87e36{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.541 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30313609{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.541 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3775c81{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.541 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1e9176{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.542 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3136c664{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.542 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8e619c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.542 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@207d16a2{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.544 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a021ef{/static,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.544 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15898c88{/,null,AVAILABLE,@Spark}
2025-02-16 23:11:22.545 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:11:22.547 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@58448b3d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:11:22.556 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:11:34.801 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:11:34.804 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:11:34.804 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:11:34.804 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:11:34.804 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:11:34.805 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:11:34.815 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:11:35.326 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:11:35.902 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4165ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:11:35.969 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:11:35.982 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4245ms
2025-02-16 23:11:36.017 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3a1de48e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:11:36.030 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a1e975{/,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6a1e975{/,null,STOPPED,@Spark}
2025-02-16 23:11:36.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39982887{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fc2cbd1{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36289987{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68671d8e{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f46e1fd{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72bde6a6{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fb00df3{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@284176b3{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28ea36e0{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@375d2705{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ae4db41{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31ee3a43{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b1d8824{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e9c738c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75be90bd{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f4752{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d6abb45{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68c6bb4c{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b3db85b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@423ddf31{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b319c36{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@800e5a1{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.187 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12477f5f{/static,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.188 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29477743{/,null,AVAILABLE,@Spark}
2025-02-16 23:11:36.190 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:11:36.199 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3a1de48e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:11:36.230 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:17:12.374 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:17:12.377 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:17:12.378 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:17:12.378 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:17:12.378 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:17:12.379 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:17:12.389 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:17:12.908 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:17:13.488 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3945ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:17:13.556 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:17:13.569 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4026ms
2025-02-16 23:17:13.603 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@37c406db{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:17:13.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a50064e{/,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.757 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@a50064e{/,null,STOPPED,@Spark}
2025-02-16 23:17:13.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@945ec13{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dccb862{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1615ec32{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a3ef689{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1faca46e{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.764 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bf72757{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.765 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e937ab1{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.765 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26be49f7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.766 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67e8a0f0{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.766 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f4f2f79{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.766 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57c423bf{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.767 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b165f45{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.767 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@679cd739{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.767 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57af730d{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@590cdfbf{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cdbf4a5{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5673c7{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@399814dd{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70580be8{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e52f089{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d355008{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a739e5e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.776 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@207647ac{/static,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.777 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29502b96{/,null,AVAILABLE,@Spark}
2025-02-16 23:17:13.779 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:150)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:17:13.788 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@37c406db{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:17:13.819 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:150)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:23:32.728 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:23:32.730 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:23:32.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:23:32.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:23:32.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:23:32.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:23:32.742 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:23:33.286 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:23:33.891 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3950ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:23:33.959 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:23:33.971 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4031ms
2025-02-16 23:23:34.006 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@42d7cff1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:23:34.021 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33143fdb{/,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@33143fdb{/,null,STOPPED,@Spark}
2025-02-16 23:23:34.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cf58924{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64d43dde{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48cb4ac2{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78dd1033{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422d9147{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5597ee76{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@294d5d64{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36289987{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68671d8e{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f46e1fd{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72bde6a6{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@322f018c{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d6e5773{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fb00df3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@284176b3{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28ea36e0{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@375d2705{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ae4db41{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31ee3a43{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b1d8824{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e9c738c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75be90bd{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.186 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f4752{/static,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.186 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@118af275{/,null,AVAILABLE,@Spark}
2025-02-16 23:23:34.189 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:23:34.198 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@42d7cff1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:23:34.231 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:28:33.161 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:28:33.163 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:28:33.164 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:28:33.164 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:28:33.164 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:28:33.165 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:28:33.175 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:28:33.691 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:28:34.265 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3829ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:28:34.332 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:28:34.344 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3909ms
2025-02-16 23:28:34.379 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5059b217{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:28:34.394 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f0110fd{/,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.533 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6f0110fd{/,null,STOPPED,@Spark}
2025-02-16 23:28:34.534 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f556fc{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.534 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75fa560a{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.534 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e92582a{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.535 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b1847b8{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.535 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2661b8ec{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.535 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@362005a4{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.536 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c631652{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.536 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23c9f4e3{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.536 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@723b0af4{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.536 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bea2e02{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.538 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7509e2ce{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.538 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d048d7b{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.539 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1133d77b{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.539 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edae1e4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.539 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58a8ee8c{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.540 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4040d0bb{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.540 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7318bb3c{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.540 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad5b7fe{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.541 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f6f8989{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.541 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5795385a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.542 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d02070b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.542 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@139cf21c{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.552 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6784fe44{/static,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.552 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d03a358{/,null,AVAILABLE,@Spark}
2025-02-16 23:28:34.555 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:28:34.565 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5059b217{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:28:34.595 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
javax.servlet.UnavailableException: Servlet class org.glassfish.jersey.servlet.ServletContainer is not a javax.servlet.Servlet
	at org.sparkproject.jetty.servlet.ServletHolder.checkServletType(ServletHolder.java:514)
	at org.sparkproject.jetty.servlet.ServletHolder.doStart(ServletHolder.java:386)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749)
	at java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)
	at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)
	at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)
	at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:492)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$2$adapted(SparkUI.scala:79)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1(SparkUI.scala:79)
	at org.apache.spark.ui.SparkUI.$anonfun$attachAllHandlers$1$adapted(SparkUI.scala:77)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.ui.SparkUI.attachAllHandlers(SparkUI.scala:77)
	at org.apache.spark.SparkContext.$anonfun$new$30(SparkContext.scala:674)
	at org.apache.spark.SparkContext.$anonfun$new$30$adapted(SparkContext.scala:674)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:674)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-16 23:30:51.202 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:30:51.205 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:30:51.205 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:30:51.205 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:30:51.206 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:30:51.206 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:30:51.215 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:30:51.713 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:30:52.342 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4248ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:30:52.413 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:30:52.426 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4333ms
2025-02-16 23:30:52.460 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@11fd7087{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:30:52.473 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b07441a{/,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.616 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2b07441a{/,null,STOPPED,@Spark}
2025-02-16 23:30:52.617 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@db8fbaa{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.617 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12e178cc{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c16132{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35b300be{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d83a0cf{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@708f9d57{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.620 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76a081c5{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.620 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@309c8ee0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5544db1a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25795d43{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37abfa80{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18614c1b{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17fd8424{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.623 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e4fdcd5{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.623 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a58a886{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.623 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e532f8{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.624 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f7ffb1{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.624 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a598e9a{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.624 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3972a67e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.625 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28bb68e4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.625 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18563be7{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.626 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25709312{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.635 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61548774{/static,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.636 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a57b6e{/,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.638 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4393d7f3{/api,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.638 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2695f3bc{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.638 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@543a7ebb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.643 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a70f030{/metrics/json,null,AVAILABLE,@Spark}
2025-02-16 23:30:52.717 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:30:52.744 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:30:52.744 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:37:58.337 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@11fd7087{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:38:02.215 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:38:02.218 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:38:02.218 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:38:02.219 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:38:02.219 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:38:02.219 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:38:02.229 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:38:02.738 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:38:03.344 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4002ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:38:03.410 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:38:03.424 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4082ms
2025-02-16 23:38:03.459 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4acb889b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:38:03.472 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45d531d3{/,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.609 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@45d531d3{/,null,STOPPED,@Spark}
2025-02-16 23:38:03.609 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60068b55{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.610 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3211f4f9{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.610 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74081e1c{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.611 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d59b96{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.611 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2202e6f{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.611 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6026b705{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.613 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4970baa4{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.613 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44255545{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.613 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39bc851a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.614 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ee73a00{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.614 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b87d500{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.615 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a0601d6{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.615 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9f01bf6{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e73e4f0{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c11448d{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632389eb{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e4f93b7{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@260416ba{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@686c05b5{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a2dcc45{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f7ea721{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.623 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124332e2{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558ab7ae{/static,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cc6e858{/,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@320a28ef{/api,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.632 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1041ed48{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.632 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15fb1217{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.636 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1483ac55{/metrics/json,null,AVAILABLE,@Spark}
2025-02-16 23:38:03.709 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:38:03.737 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:38:03.737 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:38:46.075 [http-nio-8081-exec-7] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据
2025-02-16 23:38:46.075 [http-nio-8081-exec-7] ERROR s.service.impl.FileServiceImpl - 竞彩数据文件不存在: C:/Users/xubei/Desktop/实习demo/spark/Data/123/竞彩数据
2025-02-16 23:38:46.076 [http-nio-8081-exec-7] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据' 时出错
java.lang.RuntimeException: 文件不存在: 竞彩数据
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:169)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:88)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-16 23:38:46.077 [http-nio-8081-exec-7] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: 文件不存在: 竞彩数据
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:283)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:88)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-16 23:39:45.125 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4acb889b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:41:46.762 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:41:46.764 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:41:46.766 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:41:46.766 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:41:46.766 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:41:46.766 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:41:46.775 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:41:47.293 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:41:47.916 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4017ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:41:47.988 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:41:48.001 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4103ms
2025-02-16 23:41:48.035 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@66a44612{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:41:48.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e932282{/,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.187 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7e932282{/,null,STOPPED,@Spark}
2025-02-16 23:41:48.188 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ba47730{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.189 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@83adcc1{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.190 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cdb13a7{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.190 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b7e06fb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.190 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15508780{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.191 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fe8834d{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.192 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d20b26{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.192 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ae98bf9{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.192 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51af1371{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.193 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dd9ac62{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.193 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41ef6b54{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.193 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7068b940{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.194 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34a6ae7e{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.194 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ea06c9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.194 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45ded420{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.195 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48aa9be4{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.195 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dc6db1a{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.196 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7dd6531f{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.196 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bcd9799{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.196 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7965fcd6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.197 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a0bfcc6{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.197 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a51c74c{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.203 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c2a717a{/static,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.204 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24aff37{/,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.206 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28600043{/api,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.206 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@178fc78{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7942d503{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.210 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@557c70e2{/metrics/json,null,AVAILABLE,@Spark}
2025-02-16 23:41:48.285 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:41:48.313 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:41:48.314 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:42:03.581 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据
2025-02-16 23:42:03.582 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 竞彩数据文件不存在: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据
2025-02-16 23:42:03.582 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据' 时出错
java.lang.RuntimeException: 文件不存在: 竞彩数据
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:169)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:88)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-16 23:42:03.583 [http-nio-8081-exec-2] ERROR s.controller.FileController - 处理竞彩数据文件失败
java.lang.RuntimeException: 竞彩数据文件处理失败: 文件不存在: 竞彩数据
	at sparkanalysis.service.impl.FileServiceImpl.processJingcaiDataFile(FileServiceImpl.java:283)
	at sparkanalysis.controller.FileController.processJingcaiFile(FileController.java:88)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-16 23:42:17.579 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-16 23:42:17.637 [http-nio-8081-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-16 23:42:17.652 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14d9e9c8{/SQL,null,AVAILABLE,@Spark}
2025-02-16 23:42:17.653 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@298c0100{/SQL/json,null,AVAILABLE,@Spark}
2025-02-16 23:42:17.653 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b6127a9{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-16 23:42:17.653 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a89df43{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-16 23:42:17.655 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5237b427{/static/sql,null,AVAILABLE,@Spark}
2025-02-16 23:42:30.937 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 5006329
2025-02-16 23:42:30.937 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-16 23:42:30.937 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-16 23:42:30.938 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 78.51%
2025-02-16 23:42:30.938 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-16 23:42:31.843 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-16 23:43:22.555 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.561 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.568 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.569 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.570 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.571 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.573 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.574 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.574 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.574 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.575 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.576 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.576 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.576 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.576 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.577 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.578 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.580 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:22.593 [Executor task launch worker for task 7.0 in stage 25.0 (TID 130)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 8.0 in stage 25.0 (TID 131)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 6.0 in stage 25.0 (TID 129)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 1.0 in stage 25.0 (TID 124)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 12.0 in stage 25.0 (TID 135)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 0.0 in stage 25.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 3.0 in stage 25.0 (TID 126)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 5.0 in stage 25.0 (TID 128)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 10.0 in stage 25.0 (TID 133)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 9.0 in stage 25.0 (TID 132)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 14.0 in stage 25.0 (TID 137)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 2.0 in stage 25.0 (TID 125)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 11.0 in stage 25.0 (TID 134)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 4.0 in stage 25.0 (TID 127)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 15.0 in stage 25.0 (TID 138)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:22.593 [Executor task launch worker for task 13.0 in stage 25.0 (TID 136)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:27.555 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:27.555 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:27.555 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:27.555 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:27.557 [Executor task launch worker for task 17.0 in stage 25.0 (TID 140)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:27.557 [Executor task launch worker for task 16.0 in stage 25.0 (TID 139)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:28.606 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:28.607 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:28.608 [Executor task launch worker for task 18.0 in stage 25.0 (TID 141)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:38.323 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:38.323 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-16 23:43:38.324 [Executor task launch worker for task 0.0 in stage 33.0 (TID 163)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-16 23:43:38.361 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-16 23:58:43.980 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@66a44612{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:58:50.476 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:58:50.479 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-16 23:58:50.479 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-16 23:58:50.479 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-16 23:58:50.479 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-16 23:58:50.480 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-16 23:58:50.488 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-16 23:58:51.033 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-16 23:58:51.633 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3891ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-16 23:58:51.707 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-16 23:58:51.722 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3980ms
2025-02-16 23:58:51.763 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1639ef26{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-16 23:58:51.778 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ec9df26{/,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.922 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4ec9df26{/,null,STOPPED,@Spark}
2025-02-16 23:58:51.923 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ae98bf9{/jobs,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.923 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51af1371{/jobs/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.925 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41ef6b54{/jobs/job,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.925 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7068b940{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.925 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34a6ae7e{/stages,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.925 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ea06c9{/stages/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.926 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dc6db1a{/stages/stage,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.926 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7dd6531f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.926 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bcd9799{/stages/pool,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.927 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7965fcd6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.931 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a0bfcc6{/storage,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.931 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a51c74c{/storage/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.932 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c2a717a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.932 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e7df60{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.932 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@502a7554{/environment,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ad8e9e8{/environment/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75b0a26d{/executors,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.935 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@399754e7{/executors/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.935 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20d77361{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.935 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c773c5b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37d227a9{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e436f99{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ca24ee4{/static,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44c9c49a{/,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@716cfd68{/api,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30b8001b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40cfd8f0{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-16 23:58:51.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d6f687d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-16 23:58:52.021 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-16 23:58:52.050 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-16 23:58:52.051 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-16 23:59:00.516 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-16 23:59:00.576 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-16 23:59:00.597 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66d8aaba{/SQL,null,AVAILABLE,@Spark}
2025-02-16 23:59:00.597 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21482495{/SQL/json,null,AVAILABLE,@Spark}
2025-02-16 23:59:00.598 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e4103f7{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-16 23:59:00.598 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49076da5{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-16 23:59:00.599 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e434ca7{/static/sql,null,AVAILABLE,@Spark}
2025-02-16 23:59:34.994 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 过滤率: 0.00%
2025-02-16 23:59:34.994 [http-nio-8081-exec-2] INFO  s.s.impl.DataAnalysisServiceImpl - 导出结果到MySQL表:clean_data_table
2025-02-17 00:02:59.576 [Executor task launch worker for task 7.0 in stage 26.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.576 [Executor task launch worker for task 4.0 in stage 26.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.576 [Executor task launch worker for task 6.0 in stage 26.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.576 [Executor task launch worker for task 0.0 in stage 26.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.577 [Executor task launch worker for task 5.0 in stage 26.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.576 [Executor task launch worker for task 3.0 in stage 26.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.576 [Executor task launch worker for task 2.0 in stage 26.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.576 [Executor task launch worker for task 1.0 in stage 26.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 4.0 in stage 26.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 1.0 in stage 26.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 3.0 in stage 26.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 6.0 in stage 26.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 7.0 in stage 26.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 5.0 in stage 26.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 2.0 in stage 26.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.582 [Executor task launch worker for task 0.0 in stage 26.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 00:02:59.604 [Executor task launch worker for task 6.0 in stage 26.0 (TID 132)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:02:59.604 [Executor task launch worker for task 2.0 in stage 26.0 (TID 128)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:02:59.604 [Executor task launch worker for task 7.0 in stage 26.0 (TID 133)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:02:59.604 [Executor task launch worker for task 1.0 in stage 26.0 (TID 127)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:02:59.604 [Executor task launch worker for task 4.0 in stage 26.0 (TID 130)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:02:59.604 [Executor task launch worker for task 0.0 in stage 26.0 (TID 126)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:02:59.604 [Executor task launch worker for task 5.0 in stage 26.0 (TID 131)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:02:59.604 [Executor task launch worker for task 3.0 in stage 26.0 (TID 129)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 00:04:48.415 [SparkUI-233] WARN  o.s.jetty.server.HttpChannel - /api/v1/applications
java.lang.NoClassDefFoundError: javax/ws/rs/ProcessingException
	at java.base/java.lang.ClassLoader.defineClass1(Native Method)
	at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
	at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
	at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
	at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)
	at java.base/java.lang.Class.getConstructor0(Class.java:3578)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2754)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createInstance(ContextHandler.java:2902)
	at org.sparkproject.jetty.servlet.ServletContextHandler$Context.createInstance(ServletContextHandler.java:1299)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createServlet(ContextHandler.java:2919)
	at org.sparkproject.jetty.servlet.ServletHolder.newInstance(ServletHolder.java:1202)
	at org.sparkproject.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:592)
	at org.sparkproject.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:486)
	at org.sparkproject.jetty.servlet.ServletHolder.prepare(ServletHolder.java:759)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:549)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.sparkproject.jetty.server.Server.handle(Server.java:516)
	at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.ClassNotFoundException: javax.ws.rs.ProcessingException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 47 common frames omitted
2025-02-17 00:05:21.757 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1639ef26{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 09:18:23.879 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 09:18:23.882 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 09:18:23.883 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 09:18:23.883 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 09:18:23.883 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 09:18:23.884 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 09:18:23.895 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 09:18:24.429 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 09:18:25.030 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4030ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 09:18:25.100 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 09:18:25.115 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4114ms
2025-02-17 09:18:25.149 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1802831f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 09:18:25.163 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9c8df1{/,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7a9c8df1{/,null,STOPPED,@Spark}
2025-02-17 09:18:25.312 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2202e6f{/jobs,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.312 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6026b705{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.313 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4abd5e3c{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.313 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4970baa4{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.313 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44255545{/stages,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.314 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39bc851a{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.314 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a0601d6{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.315 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9f01bf6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.315 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e73e4f0{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c11448d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632389eb{/storage,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e4f93b7{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@260416ba{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@686c05b5{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a2dcc45{/environment,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f7ea721{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124332e2{/executors,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558ab7ae{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c33f4cb{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59d0bd04{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2daa21c8{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.321 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12f893b8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50492599{/static,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7473763d{/,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25ad1a15{/api,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19a88dcc{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1024dabe{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.334 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f592f5{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 09:18:25.409 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 09:18:25.437 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 09:18:25.437 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 09:19:23.122 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 09:19:23.178 [http-nio-8081-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 09:19:23.195 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46be17f4{/SQL,null,AVAILABLE,@Spark}
2025-02-17 09:19:23.196 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d63fbd6{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 09:19:23.196 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@567b0fc0{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 09:19:23.197 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a7af9ea{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 09:19:23.198 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69e83e32{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 09:19:56.864 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 过滤率: 0.00%
2025-02-17 09:20:20.189 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00000-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.190 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00001-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.191 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00002-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.192 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00003-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.193 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00004-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.193 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00005-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.194 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00006-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.194 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00007-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet.crc]: it still exists.
2025-02-17 09:20:20.195 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\._SUCCESS.crc]: it still exists.
2025-02-17 09:20:20.196 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00000-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.196 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00001-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.197 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00002-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.197 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00003-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.198 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00004-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.198 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00005-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.199 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00006-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.200 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00007-a8e41fb5-eb2f-416a-be8a-86696a387a77-c000.snappy.parquet]: it still exists.
2025-02-17 09:20:20.200 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\_SUCCESS]: it still exists.
2025-02-17 09:20:20.202 [http-nio-8081-exec-3] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/processed prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:149)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 09:20:20.205 [http-nio-8081-exec-3] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/processed prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:152)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 09:21:03.741 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1802831f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 09:25:41.998 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 09:25:42.001 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 09:25:42.001 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 09:25:42.001 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 09:25:42.001 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 09:25:42.002 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 09:25:42.010 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 09:25:42.591 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 09:25:43.206 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3930ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 09:25:43.280 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 09:25:43.294 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4018ms
2025-02-17 09:25:43.334 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6427d328{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 09:25:43.348 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@538f7d69{/,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.495 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@538f7d69{/,null,STOPPED,@Spark}
2025-02-17 09:25:43.496 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67db60f4{/jobs,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.496 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@372a7a63{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.497 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53c0aaef{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.497 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fd55db3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.497 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67af7943{/stages,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.498 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ee07831{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.498 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c50f90a{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.499 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476c7124{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.499 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d587f7{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.499 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f7c1b49{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.500 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d9802be{/storage,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.500 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2468624a{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.501 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45409bed{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.501 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75da356f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.501 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ba698d9{/environment,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.502 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36679b11{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.502 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4912398{/executors,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.502 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9ba8b11{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.503 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f2cf514{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.503 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58dd189a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.503 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56528be6{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.504 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74422a1e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.514 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ae46d93{/static,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.514 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d517a50{/,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.516 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22548637{/api,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.516 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a0a2808{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.517 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21be52b2{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.520 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75e07e5a{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:43.593 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 09:25:43.620 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 09:25:43.621 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 09:25:54.173 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 09:25:54.236 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 09:25:54.257 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26cf19db{/SQL,null,AVAILABLE,@Spark}
2025-02-17 09:25:54.257 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3950179a{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:54.258 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3188c8b3{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 09:25:54.258 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4acbac34{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 09:25:54.259 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63bc7633{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 09:26:08.552 [SparkUI-79] WARN  o.s.jetty.server.HttpChannel - /api/v1/applications
java.lang.NoClassDefFoundError: javax/ws/rs/ProcessingException
	at java.base/java.lang.ClassLoader.defineClass1(Native Method)
	at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
	at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
	at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
	at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)
	at java.base/java.lang.Class.getConstructor0(Class.java:3578)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2754)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createInstance(ContextHandler.java:2902)
	at org.sparkproject.jetty.servlet.ServletContextHandler$Context.createInstance(ServletContextHandler.java:1299)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createServlet(ContextHandler.java:2919)
	at org.sparkproject.jetty.servlet.ServletHolder.newInstance(ServletHolder.java:1202)
	at org.sparkproject.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:592)
	at org.sparkproject.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:486)
	at org.sparkproject.jetty.servlet.ServletHolder.prepare(ServletHolder.java:759)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:549)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.sparkproject.jetty.server.Server.handle(Server.java:516)
	at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.ClassNotFoundException: javax.ws.rs.ProcessingException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 47 common frames omitted
2025-02-17 09:26:28.870 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 过滤率: 0.00%
2025-02-17 09:26:51.876 [Executor task launch worker for task 0.0 in stage 20.0 (TID 95)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.876 [Executor task launch worker for task 1.0 in stage 20.0 (TID 96)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.876 [Executor task launch worker for task 5.0 in stage 20.0 (TID 100)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.876 [Executor task launch worker for task 4.0 in stage 20.0 (TID 99)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.876 [Executor task launch worker for task 3.0 in stage 20.0 (TID 98)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.876 [Executor task launch worker for task 6.0 in stage 20.0 (TID 101)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.876 [Executor task launch worker for task 2.0 in stage 20.0 (TID 97)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.876 [Executor task launch worker for task 7.0 in stage 20.0 (TID 102)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 3.0 in stage 20.0 (TID 98)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 4.0 in stage 20.0 (TID 99)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 1.0 in stage 20.0 (TID 96)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 7.0 in stage 20.0 (TID 102)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 6.0 in stage 20.0 (TID 101)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 5.0 in stage 20.0 (TID 100)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 0.0 in stage 20.0 (TID 95)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.881 [Executor task launch worker for task 2.0 in stage 20.0 (TID 97)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:26:51.903 [Executor task launch worker for task 5.0 in stage 20.0 (TID 100)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:26:51.903 [Executor task launch worker for task 4.0 in stage 20.0 (TID 99)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:26:51.903 [Executor task launch worker for task 3.0 in stage 20.0 (TID 98)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:26:51.903 [Executor task launch worker for task 6.0 in stage 20.0 (TID 101)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:26:51.903 [Executor task launch worker for task 0.0 in stage 20.0 (TID 95)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:26:51.903 [Executor task launch worker for task 1.0 in stage 20.0 (TID 96)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:26:51.903 [Executor task launch worker for task 2.0 in stage 20.0 (TID 97)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:26:51.903 [Executor task launch worker for task 7.0 in stage 20.0 (TID 102)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:33:12.388 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6427d328{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 09:37:16.955 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 09:37:16.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 09:37:16.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 09:37:16.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 09:37:16.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 09:37:16.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 09:37:16.967 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 09:37:17.542 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 09:37:18.151 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3900ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 09:37:18.224 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 09:37:18.238 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3986ms
2025-02-17 09:37:18.279 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3ea55bc6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 09:37:18.293 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36c0cd4d{/,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.439 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@36c0cd4d{/,null,STOPPED,@Spark}
2025-02-17 09:37:18.440 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fabbf0c{/jobs,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.441 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20295541{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.441 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40bc73fe{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.441 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef1f744{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.442 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae6bf8b{/stages,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.442 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ac8c5ff{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.443 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79e9f3e7{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.443 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b23f6f4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.443 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@506a9361{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.444 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42dad62e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.444 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a6664f{/storage,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.444 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33a775a9{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.444 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdf5ba8{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.445 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760122d1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.445 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad7b49{/environment,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.445 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19742f53{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.446 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7d20de{/executors,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.446 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6522244a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.446 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ca0da27{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.447 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610963d8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.447 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9d9282{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.447 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d0a33d8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.459 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5be95bbb{/static,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.459 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf2a44{/,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.461 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31209089{/api,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.462 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab1a44f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.462 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5eda8de1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.465 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e41da9a{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:18.536 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 09:37:18.564 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 09:37:18.564 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 09:37:32.067 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 09:37:32.143 [http-nio-8081-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 09:37:32.163 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@633eea0c{/SQL,null,AVAILABLE,@Spark}
2025-02-17 09:37:32.163 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7648c332{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:32.164 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2702e63d{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 09:37:32.164 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8250d2d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 09:37:32.165 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@625b2b35{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 09:37:45.460 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 5006329
2025-02-17 09:37:45.460 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1075718
2025-02-17 09:37:45.460 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 162
2025-02-17 09:37:45.460 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 78.51%
2025-02-17 09:37:45.460 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 09:37:46.306 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 09:37:51.098 [Executor task launch worker for task 0.0 in stage 16.0 (TID 82)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.100 [Executor task launch worker for task 0.0 in stage 16.0 (TID 82)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.107 [Executor task launch worker for task 3.0 in stage 16.0 (TID 85)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.108 [Executor task launch worker for task 3.0 in stage 16.0 (TID 85)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.112 [Executor task launch worker for task 14.0 in stage 16.0 (TID 96)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.112 [Executor task launch worker for task 14.0 in stage 16.0 (TID 96)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.114 [Executor task launch worker for task 7.0 in stage 16.0 (TID 89)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.114 [Executor task launch worker for task 7.0 in stage 16.0 (TID 89)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.114 [Executor task launch worker for task 8.0 in stage 16.0 (TID 90)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.115 [Executor task launch worker for task 8.0 in stage 16.0 (TID 90)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.115 [Executor task launch worker for task 5.0 in stage 16.0 (TID 87)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.115 [Executor task launch worker for task 12.0 in stage 16.0 (TID 94)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.115 [Executor task launch worker for task 5.0 in stage 16.0 (TID 87)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.115 [Executor task launch worker for task 12.0 in stage 16.0 (TID 94)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 11.0 in stage 16.0 (TID 93)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 15.0 in stage 16.0 (TID 97)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 9.0 in stage 16.0 (TID 91)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 11.0 in stage 16.0 (TID 93)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 4.0 in stage 16.0 (TID 86)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 9.0 in stage 16.0 (TID 91)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 15.0 in stage 16.0 (TID 97)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 4.0 in stage 16.0 (TID 86)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 2.0 in stage 16.0 (TID 84)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 10.0 in stage 16.0 (TID 92)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.116 [Executor task launch worker for task 2.0 in stage 16.0 (TID 84)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.117 [Executor task launch worker for task 10.0 in stage 16.0 (TID 92)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.117 [Executor task launch worker for task 13.0 in stage 16.0 (TID 95)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.117 [Executor task launch worker for task 13.0 in stage 16.0 (TID 95)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.117 [Executor task launch worker for task 1.0 in stage 16.0 (TID 83)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.117 [Executor task launch worker for task 6.0 in stage 16.0 (TID 88)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.117 [Executor task launch worker for task 1.0 in stage 16.0 (TID 83)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.117 [Executor task launch worker for task 6.0 in stage 16.0 (TID 88)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:51.131 [Executor task launch worker for task 2.0 in stage 16.0 (TID 84)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 8.0 in stage 16.0 (TID 90)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 7.0 in stage 16.0 (TID 89)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 3.0 in stage 16.0 (TID 85)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 9.0 in stage 16.0 (TID 91)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 1.0 in stage 16.0 (TID 83)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 15.0 in stage 16.0 (TID 97)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 4.0 in stage 16.0 (TID 86)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 11.0 in stage 16.0 (TID 93)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 0.0 in stage 16.0 (TID 82)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 6.0 in stage 16.0 (TID 88)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 14.0 in stage 16.0 (TID 96)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 5.0 in stage 16.0 (TID 87)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 10.0 in stage 16.0 (TID 92)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 12.0 in stage 16.0 (TID 94)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:51.131 [Executor task launch worker for task 13.0 in stage 16.0 (TID 95)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:56.412 [Executor task launch worker for task 17.0 in stage 16.0 (TID 99)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:56.413 [Executor task launch worker for task 17.0 in stage 16.0 (TID 99)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:56.414 [Executor task launch worker for task 17.0 in stage 16.0 (TID 99)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:56.421 [Executor task launch worker for task 16.0 in stage 16.0 (TID 98)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:56.422 [Executor task launch worker for task 16.0 in stage 16.0 (TID 98)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:56.424 [Executor task launch worker for task 16.0 in stage 16.0 (TID 98)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:37:57.460 [Executor task launch worker for task 18.0 in stage 16.0 (TID 100)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:57.460 [Executor task launch worker for task 18.0 in stage 16.0 (TID 100)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:37:57.462 [Executor task launch worker for task 18.0 in stage 16.0 (TID 100)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:38:06.706 [Executor task launch worker for task 0.0 in stage 24.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:38:06.706 [Executor task launch worker for task 0.0 in stage 24.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 09:38:06.707 [Executor task launch worker for task 0.0 in stage 24.0 (TID 122)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 09:38:06.747 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 竞彩数据处理完成，文件：竞彩数据.csv
2025-02-17 09:55:29.319 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3ea55bc6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:13:24.551 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:13:24.553 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:13:24.553 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:13:24.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:13:24.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:13:24.554 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:13:24.562 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:13:25.021 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:13:25.565 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3573ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:13:25.628 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:13:25.641 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3649ms
2025-02-17 10:13:25.673 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@28abed{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:13:25.686 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e5aec7d{/,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.815 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6e5aec7d{/,null,STOPPED,@Spark}
2025-02-17 10:13:25.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40fa441d{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cb43878{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23c9f4e3{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@723b0af4{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bea2e02{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7509e2ce{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edae1e4{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58a8ee8c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4040d0bb{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7318bb3c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad5b7fe{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f6f8989{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5795385a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d02070b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@139cf21c{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6784fe44{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@548e4dac{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@220f1cc0{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1693bc81{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63f6756a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b7f741f{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e9e0014{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.836 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43f6ed62{/static,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.836 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c16b187{/,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.839 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@383207b{/api,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.839 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71654399{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.840 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40f0c36{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.843 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11addc64{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:25.912 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:13:25.936 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:13:25.937 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:13:41.343 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:13:41.394 [http-nio-8081-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 10:13:41.410 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@437295c0{/SQL,null,AVAILABLE,@Spark}
2025-02-17 10:13:41.410 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b0db7b4{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:41.410 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d4938f8{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 10:13:41.411 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@916a729{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 10:13:41.412 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd309bf{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 10:14:00.625 [SparkUI-149] WARN  o.s.jetty.server.HttpChannel - /api/v1/applications
java.lang.NoClassDefFoundError: javax/ws/rs/ProcessingException
	at java.base/java.lang.ClassLoader.defineClass1(Native Method)
	at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
	at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
	at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
	at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)
	at java.base/java.lang.Class.getConstructor0(Class.java:3578)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2754)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createInstance(ContextHandler.java:2902)
	at org.sparkproject.jetty.servlet.ServletContextHandler$Context.createInstance(ServletContextHandler.java:1299)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createServlet(ContextHandler.java:2919)
	at org.sparkproject.jetty.servlet.ServletHolder.newInstance(ServletHolder.java:1202)
	at org.sparkproject.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:592)
	at org.sparkproject.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:486)
	at org.sparkproject.jetty.servlet.ServletHolder.prepare(ServletHolder.java:759)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:549)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.sparkproject.jetty.server.Server.handle(Server.java:516)
	at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.ClassNotFoundException: javax.ws.rs.ProcessingException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 47 common frames omitted
2025-02-17 10:14:02.237 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据量为:5006329
2025-02-17 10:14:02.237 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为:0
2025-02-17 10:14:02.238 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 数据清洗率为:100.00
2025-02-17 10:14:02.245 [http-nio-8081-exec-3] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve "((门店编号 IS NOT NULL) AND trim((NOT (门店编号 = ))))" due to data type mismatch: the left and right operands of the binary operator have incompatible types ("BOOLEAN" and "STRING").;
'Filter (isnotnull(门店编号#24L) AND trim(cast(NOT (门店编号#24L = cast( as bigint)) as string), None))
+- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:277)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:206)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:212)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:76)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4368)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1675)
	at sparkanalysis.service.impl.FileServiceImpl.cleanStoreData(FileServiceImpl.java:281)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:132)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:14:02.247 [http-nio-8081-exec-3] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve "((门店编号 IS NOT NULL) AND trim((NOT (门店编号 = ))))" due to data type mismatch: the left and right operands of the binary operator have incompatible types ("BOOLEAN" and "STRING").;
'Filter (isnotnull(门店编号#24L) AND trim(cast(NOT (门店编号#24L = cast( as bigint)) as string), None))
+- Relation [票号#21,省中心名称#22,市中心名称#23,门店编号#24L,销售终端编号#25L,销售日期#26,销售时间#27,体育项目#28,过关方式#29,游戏#30,票面金额#31,投注内容#32,倍数#33] csv

	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:140)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:18:26.381 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@28abed{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:20:46.401 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:20:46.404 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:20:46.404 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:20:46.405 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:20:46.405 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:20:46.405 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:20:46.415 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:20:46.897 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:20:47.488 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3807ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:20:47.560 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:20:47.574 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3893ms
2025-02-17 10:20:47.611 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@23a6dab{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:20:47.624 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52b2e132{/,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.756 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@52b2e132{/,null,STOPPED,@Spark}
2025-02-17 10:20:47.757 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183989fa{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@630debdf{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a55a8a8{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f6fc148{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19661cda{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b924c1e{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@462f7363{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25ee6b0a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66aa3298{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.762 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68a145d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.762 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7553021f{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.762 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27d8bcda{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e09ca38{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e031a85{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c97e23{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.764 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@661c8eeb{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.768 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34e16238{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d14860c{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5aed6a74{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15fd0559{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50061b47{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.772 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47378f45{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.777 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d36c11d{/static,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.778 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dc68ba{/,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.779 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f00d2b8{/api,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.780 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56b67eb1{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.780 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514af17e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.783 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3519144b{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:20:47.855 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:20:47.880 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:20:47.881 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:24:43.516 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@23a6dab{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:24:48.939 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:24:48.941 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:24:48.941 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:24:48.941 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:24:48.941 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:24:48.942 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:24:48.950 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:24:49.425 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:24:49.967 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3585ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:24:50.031 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:24:50.042 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3660ms
2025-02-17 10:24:50.074 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3ea55bc6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:24:50.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36c0cd4d{/,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.220 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@36c0cd4d{/,null,STOPPED,@Spark}
2025-02-17 10:24:50.221 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fabbf0c{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.221 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20295541{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.222 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40bc73fe{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.222 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef1f744{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.222 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae6bf8b{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.222 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ac8c5ff{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79e9f3e7{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b23f6f4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@506a9361{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42dad62e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a6664f{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33a775a9{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdf5ba8{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760122d1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad7b49{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19742f53{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7d20de{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.228 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6522244a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.228 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ca0da27{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.228 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610963d8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9d9282{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d0a33d8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5be95bbb{/static,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.237 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf2a44{/,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31209089{/api,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab1a44f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5eda8de1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.241 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e41da9a{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:50.310 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:24:50.335 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:24:50.336 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:24:54.664 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:24:54.724 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 10:24:54.738 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d12ac38{/SQL,null,AVAILABLE,@Spark}
2025-02-17 10:24:54.738 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23d5c576{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:54.740 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7331c04a{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 10:24:54.740 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3515a3f3{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 10:24:54.741 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26aebc14{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 10:25:00.786 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `storeCode` cannot be resolved. Did you mean one of the following? [`门店编号`].;
'Filter (isnull('storeCode) OR (trim('StoreCode, None) = ))
+- Relation [门店编号#0] csv

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:206)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:212)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:76)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4368)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1675)
	at sparkanalysis.service.impl.FileServiceImpl.cleanJingcaiStoreData(FileServiceImpl.java:282)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:25:00.790 [http-nio-8081-exec-2] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `storeCode` cannot be resolved. Did you mean one of the following? [`门店编号`].;
'Filter (isnull('storeCode) OR (trim('StoreCode, None) = ))
+- Relation [门店编号#0] csv

	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:144)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:26:28.931 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3ea55bc6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:26:34.687 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:26:34.689 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:26:34.689 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:26:34.689 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:26:34.689 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:26:34.690 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:26:34.699 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:26:35.190 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:26:35.731 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3604ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:26:35.793 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:26:35.807 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3680ms
2025-02-17 10:26:35.839 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@555135a5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:26:35.853 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ca9b0{/,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.984 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3ca9b0{/,null,STOPPED,@Spark}
2025-02-17 10:26:35.985 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fed4be8{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.985 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b82d3e7{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.986 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22e60419{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.986 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ace5221{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.986 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20fb0d1c{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.987 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55d955a5{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.987 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@148a2fc9{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.988 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53c0aaef{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.988 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fd55db3{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.989 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67af7943{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.989 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ee07831{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.989 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72656a7d{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58325fcb{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c50f90a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@476c7124{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d587f7{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f7c1b49{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d9802be{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2468624a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45409bed{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.993 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75da356f{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.993 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ba698d9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:35.999 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36679b11{/static,null,AVAILABLE,@Spark}
2025-02-17 10:26:36.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f6c04b2{/,null,AVAILABLE,@Spark}
2025-02-17 10:26:36.001 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72c97a16{/api,null,AVAILABLE,@Spark}
2025-02-17 10:26:36.002 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22548637{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:26:36.002 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68affe5b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:26:36.004 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f8b0eec{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:36.077 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:26:36.101 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:26:36.103 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:26:38.560 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:26:38.621 [http-nio-8081-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 10:26:38.635 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@196d3c7b{/SQL,null,AVAILABLE,@Spark}
2025-02-17 10:26:38.635 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5852f04c{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:38.636 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f4b863e{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 10:26:38.636 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24bebc4b{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 10:26:38.637 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dfca312{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 10:26:44.659 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:27:02.315 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 原始数据量为:5006329
2025-02-17 10:27:02.315 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为:5006329
2025-02-17 10:27:02.315 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 数据清洗率为:0.00
2025-02-17 10:27:02.324 [http-nio-8081-exec-1] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve "((门店编号 IS NOT NULL) AND trim((NOT (门店编号 = ))))" due to data type mismatch: the left and right operands of the binary operator have incompatible types ("BOOLEAN" and "STRING").;
'Filter (isnotnull(门店编号#0) AND trim(cast(NOT (门店编号#0 = cast( as int)) as string), None))
+- Relation [门店编号#0] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:277)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:206)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:212)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:76)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4368)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1675)
	at sparkanalysis.service.impl.FileServiceImpl.cleanJingcaiStoreData(FileServiceImpl.java:285)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:27:02.326 [http-nio-8081-exec-1] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve "((门店编号 IS NOT NULL) AND trim((NOT (门店编号 = ))))" due to data type mismatch: the left and right operands of the binary operator have incompatible types ("BOOLEAN" and "STRING").;
'Filter (isnotnull(门店编号#0) AND trim(cast(NOT (门店编号#0 = cast( as int)) as string), None))
+- Relation [门店编号#0] csv

	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:144)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:32:11.734 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@555135a5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:32:17.415 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:32:17.417 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:32:17.418 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:32:17.418 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:32:17.418 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:32:17.418 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:32:17.428 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:32:17.903 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:32:18.448 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3587ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:32:18.512 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:32:18.525 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3663ms
2025-02-17 10:32:18.556 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6a9ac3a7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:32:18.569 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30c002a4{/,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.697 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@30c002a4{/,null,STOPPED,@Spark}
2025-02-17 10:32:18.698 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ffc5191{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.698 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e92582a{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2661b8ec{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@362005a4{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.704 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40fa441d{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.704 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cb43878{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.706 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@723b0af4{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.706 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bea2e02{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.706 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7509e2ce{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.707 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d048d7b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.707 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1133d77b{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.707 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edae1e4{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.707 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58a8ee8c{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.709 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4040d0bb{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.709 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7318bb3c{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.709 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad5b7fe{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.710 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f6f8989{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.710 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5795385a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.710 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d02070b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.711 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@139cf21c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.711 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6784fe44{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.711 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@548e4dac{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@220f1cc0{/static,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f00a90c{/,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.719 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58289b52{/api,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.719 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ca148c6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.719 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d481154{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.724 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ce7d4e0{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:18.792 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:32:18.817 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:32:18.817 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:32:21.308 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:32:21.367 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 10:32:21.383 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34686b26{/SQL,null,AVAILABLE,@Spark}
2025-02-17 10:32:21.384 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42cfbcc4{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:21.384 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e2c20e6{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 10:32:21.385 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69bbe5af{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 10:32:21.386 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79751df3{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 10:32:27.868 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:32:45.522 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为:5006329
2025-02-17 10:32:45.523 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为:5006329
2025-02-17 10:32:45.523 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 数据清洗率为:0.00
2025-02-17 10:32:45.530 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve "((trim(门店编号) IS NOT NULL) AND trim((NOT (门店编号 = ))))" due to data type mismatch: the left and right operands of the binary operator have incompatible types ("BOOLEAN" and "STRING").;
'Filter (isnotnull(trim(cast(门店编号#0 as string), None)) AND trim(cast(NOT (门店编号#0 = cast( as int)) as string), None))
+- Relation [门店编号#0] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:277)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)
	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:206)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:212)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:76)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:4368)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1675)
	at sparkanalysis.service.impl.FileServiceImpl.cleanJingcaiStoreData(FileServiceImpl.java:285)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:32:45.533 [http-nio-8081-exec-2] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve "((trim(门店编号) IS NOT NULL) AND trim((NOT (门店编号 = ))))" due to data type mismatch: the left and right operands of the binary operator have incompatible types ("BOOLEAN" and "STRING").;
'Filter (isnotnull(trim(cast(门店编号#0 as string), None)) AND trim(cast(NOT (门店编号#0 = cast( as int)) as string), None))
+- Relation [门店编号#0] csv

	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:144)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:33:12.382 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6a9ac3a7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:33:58.903 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:33:58.906 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:33:58.906 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:33:58.906 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:33:58.906 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:33:58.907 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:33:58.915 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:33:59.410 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:33:59.958 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3702ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:34:00.026 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:34:00.038 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3782ms
2025-02-17 10:34:00.071 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@39e8b34a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:34:00.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d7f3790{/,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.213 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4d7f3790{/,null,STOPPED,@Spark}
2025-02-17 10:34:00.214 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ccc604b{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76a081c5{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5544db1a{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.216 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25795d43{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.216 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37abfa80{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.216 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18614c1b{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a58a886{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.218 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44e532f8{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.223 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f7ffb1{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.223 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a598e9a{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3972a67e{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28bb68e4{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18563be7{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25709312{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61548774{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fa64856{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4234b9f8{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c8562c7{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bfddb33{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ec53e35{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d12811d{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.228 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cc29002{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.233 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@522e261f{/static,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.234 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2695f3bc{/,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@543a7ebb{/api,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5823a4b2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e9e26aa{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.239 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5284d070{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:00.310 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:34:00.334 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:34:00.334 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:34:02.490 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:34:02.551 [http-nio-8081-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 10:34:02.567 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ffdad72{/SQL,null,AVAILABLE,@Spark}
2025-02-17 10:34:02.567 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b21ffca{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:02.567 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ea3ccb1{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 10:34:02.568 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7899ce59{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 10:34:02.569 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37c28c9d{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 10:34:08.672 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:34:25.312 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据量为:5006329
2025-02-17 10:34:25.312 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为:5006329
2025-02-17 10:34:25.312 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 数据清洗率为:0.00
2025-02-17 10:34:25.533 [Executor task launch worker for task 0.0 in stage 9.0 (TID 60)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:34:42.274 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为:{}0
2025-02-17 10:34:42.275 [http-nio-8081-exec-3] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
java.util.MissingFormatArgumentException: Format specifier '%.2f'
	at java.base/java.util.Formatter.format(Formatter.java:2688)
	at java.base/java.util.Formatter.format(Formatter.java:2625)
	at java.base/java.lang.String.format(String.java:4147)
	at sparkanalysis.service.impl.FileServiceImpl.cleanJingcaiStoreData(FileServiceImpl.java:288)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:34:42.278 [http-nio-8081-exec-3] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: Format specifier '%.2f'
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:144)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:35:44.235 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@39e8b34a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:35:50.909 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:35:50.913 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:35:50.913 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:35:50.914 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:35:50.914 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:35:50.914 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:35:50.922 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:35:51.429 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:35:51.951 [restartedMain] WARN  o.a.hadoop.util.ShutdownHookManager - Failed to add the ShutdownHook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:86)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:180)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.storage.DiskBlockManager.addShutdownHook(DiskBlockManager.scala:344)
	at org.apache.spark.storage.DiskBlockManager.<init>(DiskBlockManager.scala:79)
	at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:199)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:381)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:483)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 10:35:52.015 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4232ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:35:52.084 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:35:52.097 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4313ms
2025-02-17 10:35:52.130 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@28970370{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:35:52.143 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ea3e67b{/,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3ea3e67b{/,null,STOPPED,@Spark}
2025-02-17 10:35:52.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b7f6f55{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.306 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c5c0cd4{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@696ab96a{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@475fd2f1{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f97203a{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.311 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f2f425a{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.312 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4938283a{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.313 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9275cd1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.314 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e5abb4{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.314 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44db70ba{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.315 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@401f25c0{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.315 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63911432{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cfb9f7e{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca63c7f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52519c4{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@337a1119{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@340e0a{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9c6ce05{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1221eae9{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78eea3bd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78ea58f0{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.321 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d56be57{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e399f01{/static,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c32d905{/,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68e79345{/api,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3097a07{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.331 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35a4b09e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.335 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ae23087{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:35:52.426 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:35:52.453 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:35:52.453 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:35:52.853 [SpringApplicationShutdownHook] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@28970370{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:41:53.213 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:41:53.216 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:41:53.216 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:41:53.217 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:41:53.217 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:41:53.217 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:41:53.227 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:41:53.728 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:41:54.321 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3803ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:41:54.387 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:41:54.399 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3881ms
2025-02-17 10:41:54.433 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@880d18{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:41:54.446 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@efc7bd3{/,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.578 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@efc7bd3{/,null,STOPPED,@Spark}
2025-02-17 10:41:54.579 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@253f3c8c{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.580 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@462f7363{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.580 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66aa3298{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.581 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68a145d{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.581 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7553021f{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.581 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27d8bcda{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.582 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c97e23{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.582 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@661c8eeb{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34e16238{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d14860c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5aed6a74{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.584 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15fd0559{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.584 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50061b47{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.584 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47378f45{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.585 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d36c11d{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.585 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@205fba24{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.585 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f2cdae{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.590 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b046c0c{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.591 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78371ca1{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.591 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d32ff52{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.591 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d05b35d{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.592 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56b7443a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.598 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fb3714{/static,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.598 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56b67eb1{/,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514af17e{/api,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73a45ba4{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@385d30a0{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.603 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d6af8fa{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:41:54.686 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:41:54.711 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:41:54.711 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:42:00.147 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:42:00.208 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 10:42:00.223 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@152d9370{/SQL,null,AVAILABLE,@Spark}
2025-02-17 10:42:00.223 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d7d2a6a{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 10:42:00.224 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9b81304{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 10:42:00.224 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d2328ca{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 10:42:00.225 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33ea63cc{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 10:42:04.675 [Executor task launch worker for task 0.0 in stage 3.0 (TID 20)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:42:18.568 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为:5006329
2025-02-17 10:42:18.569 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为:0
2025-02-17 10:42:18.569 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 数据清洗率为:100.00
2025-02-17 10:42:18.780 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:42:36.243 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为:{}5006329
2025-02-17 10:42:36.244 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
java.util.MissingFormatArgumentException: Format specifier '%.2f'
	at java.base/java.util.Formatter.format(Formatter.java:2688)
	at java.base/java.util.Formatter.format(Formatter.java:2625)
	at java.base/java.lang.String.format(String.java:4147)
	at sparkanalysis.service.impl.FileServiceImpl.cleanJingcaiStoreData(FileServiceImpl.java:287)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:127)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:42:36.246 [http-nio-8081-exec-2] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: Format specifier '%.2f'
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:135)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:54:01.668 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@880d18{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:54:07.850 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:54:07.852 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 10:54:07.853 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 10:54:07.853 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 10:54:07.853 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 10:54:07.853 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 10:54:07.862 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 10:54:08.324 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 10:54:08.875 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3724ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 10:54:08.943 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 10:54:08.955 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3804ms
2025-02-17 10:54:08.986 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@23a6dab{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 10:54:09.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52b2e132{/,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.127 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@52b2e132{/,null,STOPPED,@Spark}
2025-02-17 10:54:09.127 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183989fa{/jobs,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@630debdf{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a55a8a8{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f6fc148{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19661cda{/stages,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b924c1e{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@462f7363{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25ee6b0a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66aa3298{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68a145d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7553021f{/storage,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27d8bcda{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.133 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e09ca38{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.133 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e031a85{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.133 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c97e23{/environment,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.134 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@661c8eeb{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.134 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34e16238{/executors,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d14860c{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.139 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5aed6a74{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.140 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15fd0559{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.140 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50061b47{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.140 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47378f45{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.146 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d36c11d{/static,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.146 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dc68ba{/,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.147 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f00d2b8{/api,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.148 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56b67eb1{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.148 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514af17e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.151 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3519144b{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:09.219 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 10:54:09.245 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 10:54:09.245 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 10:54:11.934 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:54:11.994 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 10:54:12.009 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e0f2820{/SQL,null,AVAILABLE,@Spark}
2025-02-17 10:54:12.010 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@147592b1{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:12.010 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474fb491{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 10:54:12.011 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73615507{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 10:54:12.012 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ac5d0b6{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 10:54:16.319 [Executor task launch worker for task 0.0 in stage 3.0 (TID 20)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:54:30.570 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为:5006329
2025-02-17 10:54:30.570 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为:0
2025-02-17 10:54:30.570 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 数据清洗率为:100.00
2025-02-17 10:54:30.790 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 10:54:47.619 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为:{}5006329
2025-02-17 10:54:47.619 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
java.util.MissingFormatArgumentException: Format specifier '%.2f'
	at java.base/java.util.Formatter.format(Formatter.java:2688)
	at java.base/java.util.Formatter.format(Formatter.java:2625)
	at java.base/java.lang.String.format(String.java:4147)
	at sparkanalysis.service.impl.FileServiceImpl.cleanJingcaiStoreData(FileServiceImpl.java:288)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:128)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 10:54:47.621 [http-nio-8081-exec-2] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: Format specifier '%.2f'
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 11:03:27.632 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@23a6dab{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:03:33.388 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:03:33.390 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:03:33.391 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:03:33.391 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:03:33.391 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:03:33.391 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:03:33.400 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:03:33.859 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:03:34.405 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3592ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:03:34.468 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:03:34.480 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3668ms
2025-02-17 11:03:34.515 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3c06fb1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:03:34.529 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c9d6caa{/,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.657 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@c9d6caa{/,null,STOPPED,@Spark}
2025-02-17 11:03:34.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@366c3e40{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.663 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f608054{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.663 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ff64c58{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.664 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c690e7{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.664 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64c9bb5e{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.665 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7555b48e{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.665 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5df60752{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.666 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c1af8a0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.666 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64d1410d{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.666 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55b7ca66{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32a6af2b{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@82c9b13{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.667 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5917a0a2{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a1ce56a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c9aaaaa{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27375fdf{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76e8879e{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f975ed2{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67ec1ef6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.671 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59e74651{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.671 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e4c0366{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.671 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e1c2fe2{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c11f5b2{/static,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4372313d{/,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4951998a{/api,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.679 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6acba890{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.679 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17881b5f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.683 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b3a0a40{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:34.754 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:03:34.777 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:03:34.777 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:03:37.530 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:03:37.590 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:03:37.605 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ae30e3d{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:03:37.605 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251503e7{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:37.606 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ae9c3fe{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:03:37.606 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a84bf57{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:03:37.607 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@511ea1{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:03:41.981 [Executor task launch worker for task 0.0 in stage 3.0 (TID 20)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:03:56.191 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为:5006329
2025-02-17 11:03:56.191 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为:0
2025-02-17 11:03:56.191 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 数据清洗率为:100.00
2025-02-17 11:03:56.406 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:04:13.510 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为:{}5006329
2025-02-17 11:04:13.511 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
java.util.IllegalFormatConversionException: f != java.lang.Long
	at java.base/java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4442)
	at java.base/java.util.Formatter$FormatSpecifier.printFloat(Formatter.java:2976)
	at java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2924)
	at java.base/java.util.Formatter.format(Formatter.java:2689)
	at java.base/java.util.Formatter.format(Formatter.java:2625)
	at java.base/java.lang.String.format(String.java:4147)
	at sparkanalysis.service.impl.FileServiceImpl.cleanJingcaiStoreData(FileServiceImpl.java:288)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:128)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 11:04:13.513 [http-nio-8081-exec-2] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: f != java.lang.Long
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 11:08:17.111 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3c06fb1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:08:22.863 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:08:22.865 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:08:22.865 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:08:22.865 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:08:22.865 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:08:22.866 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:08:22.874 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:08:23.344 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:08:23.904 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3629ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:08:23.968 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:08:23.981 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3706ms
2025-02-17 11:08:24.013 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3ea55bc6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:08:24.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36c0cd4d{/,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.161 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@36c0cd4d{/,null,STOPPED,@Spark}
2025-02-17 11:08:24.162 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fabbf0c{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.162 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20295541{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.163 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40bc73fe{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.163 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef1f744{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.164 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae6bf8b{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.164 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ac8c5ff{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.165 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79e9f3e7{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.165 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b23f6f4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.165 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@506a9361{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.166 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42dad62e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.166 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a6664f{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.166 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33a775a9{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.166 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdf5ba8{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760122d1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad7b49{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19742f53{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7d20de{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6522244a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ca0da27{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610963d8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9d9282{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d0a33d8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5be95bbb{/static,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf2a44{/,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31209089{/api,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab1a44f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5eda8de1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e41da9a{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:24.254 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:08:24.278 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:08:24.279 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:08:31.525 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:08:31.583 [http-nio-8081-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:08:31.598 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78105948{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:08:31.599 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f70b821{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:31.599 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d39c445{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:08:31.600 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4285634a{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:08:31.601 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48f60356{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:08:35.997 [Executor task launch worker for task 0.0 in stage 3.0 (TID 20)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:08:49.840 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 5006329
2025-02-17 11:08:49.840 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后的数据量为: 0
2025-02-17 11:08:49.840 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 数据清洗率为: 100.00
2025-02-17 11:08:50.058 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:08:59.689 [SparkUI-148] WARN  o.s.jetty.server.HttpChannel - /api/v1/applications
java.lang.NoClassDefFoundError: javax/ws/rs/ProcessingException
	at java.base/java.lang.ClassLoader.defineClass1(Native Method)
	at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
	at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
	at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
	at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)
	at java.base/java.lang.Class.getConstructor0(Class.java:3578)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2754)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createInstance(ContextHandler.java:2902)
	at org.sparkproject.jetty.servlet.ServletContextHandler$Context.createInstance(ServletContextHandler.java:1299)
	at org.sparkproject.jetty.server.handler.ContextHandler$StaticContext.createServlet(ContextHandler.java:2919)
	at org.sparkproject.jetty.servlet.ServletHolder.newInstance(ServletHolder.java:1202)
	at org.sparkproject.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:592)
	at org.sparkproject.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:486)
	at org.sparkproject.jetty.servlet.ServletHolder.prepare(ServletHolder.java:759)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:549)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.sparkproject.jetty.server.Server.handle(Server.java:516)
	at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.ClassNotFoundException: javax.ws.rs.ProcessingException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 47 common frames omitted
2025-02-17 11:09:07.254 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 5006329
2025-02-17 11:09:07.324 [Executor task launch worker for task 0.0 in stage 12.0 (TID 64)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:09:21.485 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 0
2025-02-17 11:09:21.579 [Executor task launch worker for task 0.0 in stage 15.0 (TID 84)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:09:36.780 [Executor task launch worker for task 0.0 in stage 17.0 (TID 103)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:36.782 [Executor task launch worker for task 0.0 in stage 17.0 (TID 103)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:36.795 [Executor task launch worker for task 0.0 in stage 17.0 (TID 103)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:09:37.980 [Executor task launch worker for task 2.0 in stage 17.0 (TID 105)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:37.980 [Executor task launch worker for task 1.0 in stage 17.0 (TID 104)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:37.980 [Executor task launch worker for task 2.0 in stage 17.0 (TID 105)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:37.981 [Executor task launch worker for task 1.0 in stage 17.0 (TID 104)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:37.981 [Executor task launch worker for task 2.0 in stage 17.0 (TID 105)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:09:37.982 [Executor task launch worker for task 1.0 in stage 17.0 (TID 104)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:09:37.990 [Executor task launch worker for task 3.0 in stage 17.0 (TID 106)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:37.991 [Executor task launch worker for task 3.0 in stage 17.0 (TID 106)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:09:37.991 [Executor task launch worker for task 3.0 in stage 17.0 (TID 106)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:28:23.146 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3ea55bc6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:28:28.986 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:28:28.988 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:28:28.989 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:28:28.989 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:28:28.989 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:28:28.990 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:28:28.999 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:28:29.471 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:28:30.024 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3598ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:28:30.088 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:28:30.099 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3674ms
2025-02-17 11:28:30.132 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5bb95f48{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:28:30.146 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@89134b{/,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.271 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@89134b{/,null,STOPPED,@Spark}
2025-02-17 11:28:30.273 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d792a26{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.275 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74e0e0ee{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.277 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68625b50{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.278 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6de00d7d{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d0458cd{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68237290{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f3e22ac{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.283 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7eac7277{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.284 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c20226{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.284 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@326c29bd{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.285 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47dcd289{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.285 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d7629b6{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.286 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b4c52e8{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f67b94a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@403be5af{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f1a6b{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1eb4b6dc{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76d5a3bf{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2557cc9e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@732ac1dd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.295 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d8a03e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.295 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@453b969f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.300 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1eaca5fe{/static,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11b62df0{/,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64cf2b86{/api,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@622569b9{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fa2f062{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39114c80{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:30.382 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:28:30.412 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:28:30.412 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:28:33.240 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:28:33.301 [http-nio-8081-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:28:33.316 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@421a075{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:28:33.317 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43ef86f9{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:33.317 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61deb213{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:28:33.318 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53b6d0de{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:28:33.319 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74e8ed00{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:28:37.703 [Executor task launch worker for task 0.0 in stage 3.0 (TID 20)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:28:52.029 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 5006329
2025-02-17 11:28:52.029 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 5006329
2025-02-17 11:28:52.111 [Executor task launch worker for task 0.0 in stage 6.0 (TID 40)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:29:06.497 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 0
2025-02-17 11:29:06.707 [Executor task launch worker for task 0.0 in stage 9.0 (TID 60)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.709 [Executor task launch worker for task 0.0 in stage 9.0 (TID 60)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.715 [Executor task launch worker for task 8.0 in stage 9.0 (TID 68)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.715 [Executor task launch worker for task 8.0 in stage 9.0 (TID 68)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.718 [Executor task launch worker for task 3.0 in stage 9.0 (TID 63)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.718 [Executor task launch worker for task 3.0 in stage 9.0 (TID 63)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.721 [Executor task launch worker for task 11.0 in stage 9.0 (TID 71)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.721 [Executor task launch worker for task 11.0 in stage 9.0 (TID 71)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.721 [Executor task launch worker for task 1.0 in stage 9.0 (TID 61)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.722 [Executor task launch worker for task 1.0 in stage 9.0 (TID 61)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.722 [Executor task launch worker for task 10.0 in stage 9.0 (TID 70)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.722 [Executor task launch worker for task 10.0 in stage 9.0 (TID 70)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.722 [Executor task launch worker for task 9.0 in stage 9.0 (TID 69)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.722 [Executor task launch worker for task 9.0 in stage 9.0 (TID 69)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.723 [Executor task launch worker for task 6.0 in stage 9.0 (TID 66)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.723 [Executor task launch worker for task 15.0 in stage 9.0 (TID 75)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.723 [Executor task launch worker for task 4.0 in stage 9.0 (TID 64)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.723 [Executor task launch worker for task 5.0 in stage 9.0 (TID 65)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.723 [Executor task launch worker for task 2.0 in stage 9.0 (TID 62)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 7.0 in stage 9.0 (TID 67)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 2.0 in stage 9.0 (TID 62)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 4.0 in stage 9.0 (TID 64)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 14.0 in stage 9.0 (TID 74)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 15.0 in stage 9.0 (TID 75)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 12.0 in stage 9.0 (TID 72)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 6.0 in stage 9.0 (TID 66)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 7.0 in stage 9.0 (TID 67)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 5.0 in stage 9.0 (TID 65)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 13.0 in stage 9.0 (TID 73)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 14.0 in stage 9.0 (TID 74)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 12.0 in stage 9.0 (TID 72)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.724 [Executor task launch worker for task 13.0 in stage 9.0 (TID 73)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:06.732 [Executor task launch worker for task 14.0 in stage 9.0 (TID 74)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 4.0 in stage 9.0 (TID 64)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 13.0 in stage 9.0 (TID 73)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 3.0 in stage 9.0 (TID 63)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 10.0 in stage 9.0 (TID 70)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 6.0 in stage 9.0 (TID 66)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 15.0 in stage 9.0 (TID 75)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 0.0 in stage 9.0 (TID 60)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 11.0 in stage 9.0 (TID 71)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 8.0 in stage 9.0 (TID 68)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 12.0 in stage 9.0 (TID 72)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 2.0 in stage 9.0 (TID 62)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 7.0 in stage 9.0 (TID 67)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 1.0 in stage 9.0 (TID 61)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 9.0 in stage 9.0 (TID 69)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.732 [Executor task launch worker for task 5.0 in stage 9.0 (TID 65)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:06.991 [Executor task launch worker for task 0.0 in stage 9.0 (TID 60)] WARN  o.a.s.s.c.csv.CSVHeaderChecker - Number of column in CSV header is not equal to number of fields in the schema:
 Header length: 13, schema size: 1
CSV file: file:///C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:29:18.904 [Executor task launch worker for task 17.0 in stage 9.0 (TID 77)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:18.904 [Executor task launch worker for task 16.0 in stage 9.0 (TID 76)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:18.905 [Executor task launch worker for task 17.0 in stage 9.0 (TID 77)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:18.905 [Executor task launch worker for task 16.0 in stage 9.0 (TID 76)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:18.906 [Executor task launch worker for task 17.0 in stage 9.0 (TID 77)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:18.906 [Executor task launch worker for task 16.0 in stage 9.0 (TID 76)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:29:18.907 [Executor task launch worker for task 18.0 in stage 9.0 (TID 78)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:18.908 [Executor task launch worker for task 18.0 in stage 9.0 (TID 78)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:29:18.910 [Executor task launch worker for task 18.0 in stage 9.0 (TID 78)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:32:56.795 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5bb95f48{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:33:02.697 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:33:02.699 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:33:02.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:33:02.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:33:02.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:33:02.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:33:02.709 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:33:03.180 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:33:03.729 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3663ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:33:03.793 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:33:03.804 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3738ms
2025-02-17 11:33:03.837 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1ddc6097{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:33:03.850 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d26b06f{/,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.988 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5d26b06f{/,null,STOPPED,@Spark}
2025-02-17 11:33:03.988 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c19512d{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.989 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4582fc97{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.989 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28ddb677{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bd94aa3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68f2c9a0{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1032bd98{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@440a5781{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c220ae7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.993 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e0a7b2f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.993 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61bf8177{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.994 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70089c89{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.994 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36641fae{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d3dcba7{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c841b46{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69124d52{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c80392{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe46273{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@103e2850{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.997 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76b3f93a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.997 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142d6823{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.997 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@500400c6{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:33:03.998 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fc47b28{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:04.003 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c850523{/static,null,AVAILABLE,@Spark}
2025-02-17 11:33:04.003 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f4c8c23{/,null,AVAILABLE,@Spark}
2025-02-17 11:33:04.004 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c46d787{/api,null,AVAILABLE,@Spark}
2025-02-17 11:33:04.005 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@186eb7b3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:33:04.005 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3069ff3a{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:33:04.009 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1364c358{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:33:04.081 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:33:04.105 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:33:04.106 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:34:54.313 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1ddc6097{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:34:59.870 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:34:59.872 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:34:59.873 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:34:59.873 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:34:59.873 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:34:59.873 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:34:59.881 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:35:00.350 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:35:00.907 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3625ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:35:00.970 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:35:00.982 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3700ms
2025-02-17 11:35:01.015 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@df938b1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:35:01.028 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a281084{/,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.159 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3a281084{/,null,STOPPED,@Spark}
2025-02-17 11:35:01.161 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397f1309{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.162 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1615ec32{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.166 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1faca46e{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bf72757{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a0f17b8{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e2b7b26{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67e8a0f0{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f4f2f79{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57c423bf{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b165f45{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@679cd739{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57af730d{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@590cdfbf{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cdbf4a5{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5673c7{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@399814dd{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70580be8{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e52f089{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d355008{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a739e5e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@207647ac{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b9a80d{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3056740a{/static,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.180 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17a439e1{/,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9c6ff57{/api,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@499fab41{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32282349{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.186 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@464cd9f2{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:01.254 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:35:01.278 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:35:01.278 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:35:11.727 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:35:11.787 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:35:11.802 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ce206e0{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:35:11.803 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10df7725{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:11.803 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e1d90f{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:35:11.803 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f83e48{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:35:11.804 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42a868a5{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:35:15.983 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 5006329
2025-02-17 11:35:28.602 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 0
2025-02-17 11:35:28.603 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 11:35:28.603 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 5006329
2025-02-17 11:35:33.024 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 5006329
2025-02-17 11:35:37.218 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 0
2025-02-17 11:35:37.501 [Executor task launch worker for task 0.0 in stage 18.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.503 [Executor task launch worker for task 0.0 in stage 18.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.529 [Executor task launch worker for task 0.0 in stage 18.0 (TID 120)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.542 [Executor task launch worker for task 9.0 in stage 18.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.542 [Executor task launch worker for task 14.0 in stage 18.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.542 [Executor task launch worker for task 12.0 in stage 18.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.543 [Executor task launch worker for task 9.0 in stage 18.0 (TID 129)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.543 [Executor task launch worker for task 14.0 in stage 18.0 (TID 134)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.543 [Executor task launch worker for task 12.0 in stage 18.0 (TID 132)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.543 [Executor task launch worker for task 2.0 in stage 18.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.544 [Executor task launch worker for task 5.0 in stage 18.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.544 [Executor task launch worker for task 2.0 in stage 18.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.544 [Executor task launch worker for task 5.0 in stage 18.0 (TID 125)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.544 [Executor task launch worker for task 14.0 in stage 18.0 (TID 134)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.544 [Executor task launch worker for task 9.0 in stage 18.0 (TID 129)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.544 [Executor task launch worker for task 12.0 in stage 18.0 (TID 132)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.545 [Executor task launch worker for task 10.0 in stage 18.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.545 [Executor task launch worker for task 1.0 in stage 18.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.545 [Executor task launch worker for task 10.0 in stage 18.0 (TID 130)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.545 [Executor task launch worker for task 1.0 in stage 18.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.546 [Executor task launch worker for task 5.0 in stage 18.0 (TID 125)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.546 [Executor task launch worker for task 2.0 in stage 18.0 (TID 122)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.546 [Executor task launch worker for task 3.0 in stage 18.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.546 [Executor task launch worker for task 7.0 in stage 18.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.546 [Executor task launch worker for task 3.0 in stage 18.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.546 [Executor task launch worker for task 8.0 in stage 18.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.546 [Executor task launch worker for task 7.0 in stage 18.0 (TID 127)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 8.0 in stage 18.0 (TID 128)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 11.0 in stage 18.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 13.0 in stage 18.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 11.0 in stage 18.0 (TID 131)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 15.0 in stage 18.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 6.0 in stage 18.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 4.0 in stage 18.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 13.0 in stage 18.0 (TID 133)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 15.0 in stage 18.0 (TID 135)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 6.0 in stage 18.0 (TID 126)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 4.0 in stage 18.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:37.547 [Executor task launch worker for task 1.0 in stage 18.0 (TID 121)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 10.0 in stage 18.0 (TID 130)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 7.0 in stage 18.0 (TID 127)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 3.0 in stage 18.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 8.0 in stage 18.0 (TID 128)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 11.0 in stage 18.0 (TID 131)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 15.0 in stage 18.0 (TID 135)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 6.0 in stage 18.0 (TID 126)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.548 [Executor task launch worker for task 13.0 in stage 18.0 (TID 133)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:37.549 [Executor task launch worker for task 4.0 in stage 18.0 (TID 124)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:46.955 [Executor task launch worker for task 16.0 in stage 18.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:46.956 [Executor task launch worker for task 17.0 in stage 18.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:46.956 [Executor task launch worker for task 16.0 in stage 18.0 (TID 136)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:46.956 [Executor task launch worker for task 17.0 in stage 18.0 (TID 137)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:46.957 [Executor task launch worker for task 16.0 in stage 18.0 (TID 136)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:46.957 [Executor task launch worker for task 17.0 in stage 18.0 (TID 137)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:35:46.962 [Executor task launch worker for task 18.0 in stage 18.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:46.962 [Executor task launch worker for task 18.0 in stage 18.0 (TID 138)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:35:46.963 [Executor task launch worker for task 18.0 in stage 18.0 (TID 138)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:38:40.206 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@df938b1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:42:43.696 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:42:43.699 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:42:43.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:42:43.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:42:43.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:42:43.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:42:43.710 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:42:44.225 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:42:44.805 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3915ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:42:44.872 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:42:44.885 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3996ms
2025-02-17 11:42:44.920 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@585a3c56{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:42:44.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@178209df{/,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.079 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@178209df{/,null,STOPPED,@Spark}
2025-02-17 11:42:45.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e937ab1{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26be49f7{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f4f2f79{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57c423bf{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b165f45{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@679cd739{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cdbf4a5{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5673c7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@399814dd{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70580be8{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e52f089{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d355008{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a739e5e{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.091 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@207647ac{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.091 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b9a80d{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.092 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3056740a{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.092 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3945879e{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.092 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6083ab3a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.093 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e137799{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.093 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53b9b486{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.093 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26bce53a{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.094 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66ff4954{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.100 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53428759{/static,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.101 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@363b2682{/,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.103 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@499fab41{/api,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.103 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47216247{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.103 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e0d0e3e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3753d669{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:45.181 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:42:45.207 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:42:45.207 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:42:48.154 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:42:48.223 [http-nio-8081-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:42:48.240 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e5ea72b{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:42:48.240 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cad5a9a{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:48.241 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e3c10df{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:42:48.241 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47385169{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:42:48.242 [http-nio-8081-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@372de04f{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:42:51.514 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 1048575
2025-02-17 11:42:57.991 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 空值数量: 1472
2025-02-17 11:42:57.991 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 11:42:57.991 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 1047103
2025-02-17 11:42:59.984 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 1047103
2025-02-17 11:43:01.674 [http-nio-8081-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 1472
2025-02-17 11:43:01.736 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00000-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.738 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00001-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.739 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00002-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.739 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00003-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.740 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00004-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.740 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00005-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.741 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00006-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.741 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00007-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.742 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00008-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.742 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00009-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.743 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00010-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.743 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00011-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.743 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00012-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.744 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00013-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.744 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00014-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.745 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00015-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.745 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00016-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.746 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00017-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.746 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00018-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:43:01.747 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\._SUCCESS.crc]: it still exists.
2025-02-17 11:43:01.747 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00000-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.748 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00001-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.748 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00002-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.749 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00003-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.749 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00004-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.750 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00005-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.750 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00006-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.751 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00007-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.751 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00008-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.751 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00009-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.753 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00010-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.753 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00011-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.753 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00012-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.754 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00013-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.754 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00014-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.755 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00015-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.755 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00016-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.755 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00017-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.757 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00018-544075bf-e8ed-4162-bead-f007834603ef-c000.snappy.parquet]: it still exists.
2025-02-17 11:43:01.757 [http-nio-8081-exec-3] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\_SUCCESS]: it still exists.
2025-02-17 11:43:01.817 [http-nio-8081-exec-3] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/processed prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:133)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 11:43:01.818 [http-nio-8081-exec-3] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/processed prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 11:43:25.305 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@585a3c56{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:43:54.597 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:43:54.600 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:43:54.601 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:43:54.601 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:43:54.601 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:43:54.601 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:43:54.611 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:43:55.124 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:43:55.720 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4915ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:43:55.788 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:43:55.801 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4996ms
2025-02-17 11:43:55.834 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@78725922{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:43:55.848 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75e0df01{/,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.985 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@75e0df01{/,null,STOPPED,@Spark}
2025-02-17 11:43:55.986 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e1fbae8{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7444c50b{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f33533b{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45f35e08{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6079356b{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cdae1a9{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b170a13{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@539749c3{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.994 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d206683{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.994 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f3fefff{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.994 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3adc0041{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.994 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5703ad33{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ab71074{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@484d2c62{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34c767c8{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c55b737{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a6574e7{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33fc1437{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.997 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa1d54b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.997 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34d971ea{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.997 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69393867{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:43:55.999 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d78b30e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:56.005 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3554776b{/static,null,AVAILABLE,@Spark}
2025-02-17 11:43:56.005 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6419406c{/,null,AVAILABLE,@Spark}
2025-02-17 11:43:56.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d4a740{/api,null,AVAILABLE,@Spark}
2025-02-17 11:43:56.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fc6882f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:43:56.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7456e992{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:43:56.011 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d486332{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:56.083 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:43:56.110 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:43:56.110 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:43:59.496 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:43:59.563 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:43:59.578 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1403da24{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:43:59.578 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bf3fda6{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:59.578 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d1738fb{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:43:59.578 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78511472{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:43:59.580 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47a9ed13{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:44:02.646 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 1048575
2025-02-17 11:44:07.593 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 1472
2025-02-17 11:44:07.593 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 11:44:07.593 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 1047103
2025-02-17 11:44:09.771 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 1047103
2025-02-17 11:44:11.325 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 1472
2025-02-17 11:44:11.547 [Executor task launch worker for task 0.0 in stage 18.0 (TID 30)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.549 [Executor task launch worker for task 0.0 in stage 18.0 (TID 30)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.565 [Executor task launch worker for task 0.0 in stage 18.0 (TID 30)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:44:11.569 [Executor task launch worker for task 2.0 in stage 18.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.569 [Executor task launch worker for task 3.0 in stage 18.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.569 [Executor task launch worker for task 1.0 in stage 18.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.570 [Executor task launch worker for task 1.0 in stage 18.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.570 [Executor task launch worker for task 2.0 in stage 18.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.570 [Executor task launch worker for task 3.0 in stage 18.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:44:11.571 [Executor task launch worker for task 2.0 in stage 18.0 (TID 32)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:44:11.571 [Executor task launch worker for task 1.0 in stage 18.0 (TID 31)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:44:11.571 [Executor task launch worker for task 3.0 in stage 18.0 (TID 33)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:46:33.306 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@78725922{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:46:39.326 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:46:39.329 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:46:39.329 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:46:39.329 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:46:39.329 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:46:39.330 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:46:39.339 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:46:39.807 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:46:40.357 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3590ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:46:40.422 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:46:40.433 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3666ms
2025-02-17 11:46:40.466 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1a385e03{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:46:40.480 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a0b4dc9{/,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.613 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2a0b4dc9{/,null,STOPPED,@Spark}
2025-02-17 11:46:40.614 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cf471b0{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.615 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5924e2e9{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.615 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b23f6f4{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.616 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@506a9361{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.616 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42dad62e{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.616 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a6664f{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.617 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760122d1{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.617 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad7b49{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.617 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19742f53{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7d20de{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6522244a{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ca0da27{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610963d8{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9d9282{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d0a33d8{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.619 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5be95bbb{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cf7c904{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36d76e68{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49992781{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@759e89e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d97725a{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.622 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cd5ec9d{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5034f914{/static,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39191df{/,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab1a44f{/api,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf83b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@519e8391{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.634 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c24c0{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:40.703 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:46:40.727 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:46:40.728 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:46:43.710 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:46:43.770 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:46:43.786 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4463eaee{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:46:43.786 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d814185{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:43.787 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c471ca1{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:46:43.787 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bc7191a{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:46:43.789 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a5f89cf{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:46:46.859 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 1048575
2025-02-17 11:46:51.682 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 1472
2025-02-17 11:46:51.682 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 11:46:51.682 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 1047103
2025-02-17 11:46:53.500 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 1047103
2025-02-17 11:46:55.145 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 1472
2025-02-17 11:46:55.332 [Executor task launch worker for task 0.0 in stage 18.0 (TID 30)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.333 [Executor task launch worker for task 0.0 in stage 18.0 (TID 30)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.350 [Executor task launch worker for task 2.0 in stage 18.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.350 [Executor task launch worker for task 3.0 in stage 18.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.350 [Executor task launch worker for task 1.0 in stage 18.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.350 [Executor task launch worker for task 0.0 in stage 18.0 (TID 30)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:46:55.350 [Executor task launch worker for task 3.0 in stage 18.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.350 [Executor task launch worker for task 2.0 in stage 18.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.350 [Executor task launch worker for task 1.0 in stage 18.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:46:55.351 [Executor task launch worker for task 1.0 in stage 18.0 (TID 31)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:46:55.351 [Executor task launch worker for task 3.0 in stage 18.0 (TID 33)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:46:55.351 [Executor task launch worker for task 2.0 in stage 18.0 (TID 32)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:52:56.301 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1a385e03{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:58:31.469 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:58:31.472 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:58:31.472 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:58:31.473 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:58:31.473 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:58:31.473 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:58:31.482 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:58:32.002 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:58:32.592 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3910ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:58:32.663 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:58:32.676 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3994ms
2025-02-17 11:58:32.710 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5605e073{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:58:32.724 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e5196b7{/,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.871 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6e5196b7{/,null,STOPPED,@Spark}
2025-02-17 11:58:32.872 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d59b96{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.873 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2202e6f{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.873 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@343f08ab{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.874 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4abd5e3c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.874 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4970baa4{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.874 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44255545{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b87d500{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a0601d6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.876 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9f01bf6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.876 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e73e4f0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.876 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c11448d{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.876 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632389eb{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.878 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e4f93b7{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.878 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@260416ba{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.878 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@686c05b5{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.879 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a2dcc45{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.879 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f7ea721{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@124332e2{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558ab7ae{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c33f4cb{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.881 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59d0bd04{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.882 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2daa21c8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.888 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12f893b8{/static,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.889 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6781723f{/,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.891 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7473763d{/api,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.891 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62ecb8e8{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.892 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19a88dcc{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.896 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7de764ad{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:32.970 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:58:32.997 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:58:32.998 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:58:36.033 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:58:36.098 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:58:36.114 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b41477{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:58:36.115 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20a0ef27{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:36.116 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ee18697{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:58:36.116 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@244acc46{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:58:36.117 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b6bed0b{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:58:40.429 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 1048575
2025-02-17 11:58:45.724 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 1472
2025-02-17 11:58:45.725 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 11:58:45.725 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 1047103
2025-02-17 11:58:47.658 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 1047103
2025-02-17 11:58:49.651 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 1472
2025-02-17 11:58:49.717 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00000-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:58:49.717 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00001-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:58:49.718 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00002-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:58:49.718 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\.part-00003-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet.crc]: it still exists.
2025-02-17 11:58:49.719 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\._SUCCESS.crc]: it still exists.
2025-02-17 11:58:49.720 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00000-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet]: it still exists.
2025-02-17 11:58:49.721 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00001-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet]: it still exists.
2025-02-17 11:58:49.721 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00002-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet]: it still exists.
2025-02-17 11:58:49.722 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\part-00003-83503141-03d5-4527-8477-00d3b19041b3-c000.snappy.parquet]: it still exists.
2025-02-17 11:58:49.722 [http-nio-8081-exec-2] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\实习demo\spark\Data\测试文件\processed\_SUCCESS]: it still exists.
2025-02-17 11:58:49.795 [http-nio-8081-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 '竞彩数据.csv' 时出错
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/processed prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:133)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 11:58:49.798 [http-nio-8081-exec-2] ERROR s.controller.FileController - 处理文件失败
java.lang.RuntimeException: 文件处理失败: Unable to clear output directory file:/C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/processed prior to writing to it.
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:136)
	at sparkanalysis.controller.FileController.processFile(FileController.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 11:59:08.408 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5605e073{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:59:37.689 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:59:37.692 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 11:59:37.692 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 11:59:37.692 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 11:59:37.692 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 11:59:37.692 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 11:59:37.702 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 11:59:38.224 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 11:59:38.837 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4219ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 11:59:38.902 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 11:59:38.915 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4296ms
2025-02-17 11:59:38.948 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@106fceaf{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 11:59:38.962 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41110850{/,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.104 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@41110850{/,null,STOPPED,@Spark}
2025-02-17 11:59:39.104 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@284176b3{/jobs,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.105 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28ea36e0{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.105 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ae4db41{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31ee3a43{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b1d8824{/stages,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e9c738c{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d6abb45{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68c6bb4c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.108 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b3db85b{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.108 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@423ddf31{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b319c36{/storage,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@800e5a1{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12477f5f{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7af8eb98{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1232a575{/environment,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bfe126e{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.112 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f3772{/executors,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.112 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@722c952a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.112 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@562f924{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e4e536e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ebe3d1{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48abd2db{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.120 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@118af275{/static,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.121 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7de78e32{/,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.122 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e1a8c32{/api,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47b7858d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71f08ed9{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.125 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34fd285{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:39.195 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 11:59:39.220 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 11:59:39.220 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 11:59:41.678 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 11:59:41.739 [http-nio-8081-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 11:59:41.755 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@724baa67{/SQL,null,AVAILABLE,@Spark}
2025-02-17 11:59:41.755 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@347c064b{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:41.756 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@331616bf{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 11:59:41.756 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37b7117c{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 11:59:41.756 [http-nio-8081-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@281c4d3{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 11:59:45.688 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 1048575
2025-02-17 11:59:50.651 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 空值数量: 1472
2025-02-17 11:59:50.651 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 11:59:50.651 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 1047103
2025-02-17 11:59:52.356 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 1047103
2025-02-17 11:59:54.235 [http-nio-8081-exec-1] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 1472
2025-02-17 11:59:54.411 [Executor task launch worker for task 0.0 in stage 19.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.413 [Executor task launch worker for task 0.0 in stage 19.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.429 [Executor task launch worker for task 3.0 in stage 19.0 (TID 34)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.429 [Executor task launch worker for task 1.0 in stage 19.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.429 [Executor task launch worker for task 2.0 in stage 19.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.429 [Executor task launch worker for task 1.0 in stage 19.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.429 [Executor task launch worker for task 2.0 in stage 19.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.429 [Executor task launch worker for task 3.0 in stage 19.0 (TID 34)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 11:59:54.430 [Executor task launch worker for task 0.0 in stage 19.0 (TID 31)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:59:54.431 [Executor task launch worker for task 3.0 in stage 19.0 (TID 34)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:59:54.431 [Executor task launch worker for task 1.0 in stage 19.0 (TID 32)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 11:59:54.431 [Executor task launch worker for task 2.0 in stage 19.0 (TID 33)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 12:00:13.393 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@106fceaf{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:04:56.371 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:04:56.373 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:04:56.374 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:04:56.374 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:04:56.374 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:04:56.374 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:04:56.383 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:04:56.908 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:04:57.556 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4159ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 14:04:57.642 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 14:04:57.659 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4262ms
2025-02-17 14:04:57.694 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4fd27626{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:04:57.708 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b54b0b1{/,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.851 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4b54b0b1{/,null,STOPPED,@Spark}
2025-02-17 14:04:57.852 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@174a95ba{/jobs,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.853 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fabbf0c{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.853 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@268afcb5{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.854 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40bc73fe{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.854 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef1f744{/stages,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.854 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae6bf8b{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5924e2e9{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79e9f3e7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b23f6f4{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@506a9361{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.857 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42dad62e{/storage,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.857 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a6664f{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33a775a9{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdf5ba8{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760122d1{/environment,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ad7b49{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19742f53{/executors,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.860 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7d20de{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.860 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6522244a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.860 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ca0da27{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.861 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610963d8{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.861 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9d9282{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.867 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d0a33d8{/static,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.868 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33f7ab2a{/,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.870 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf2a44{/api,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.871 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39191df{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.871 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab1a44f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f346687{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 14:04:57.951 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 14:04:57.984 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:04:57.984 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 14:05:03.843 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/实习demo/spark/Data/测试文件/竞彩数据.csv
2025-02-17 14:05:03.901 [http-nio-8081-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 14:05:03.917 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5eae0db5{/SQL,null,AVAILABLE,@Spark}
2025-02-17 14:05:03.917 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ff9cec5{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 14:05:03.918 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@387cc484{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 14:05:03.918 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f43c9fa{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 14:05:03.919 [http-nio-8081-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a07b118{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 14:05:08.232 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 1048575
2025-02-17 14:05:13.265 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 1472
2025-02-17 14:05:13.265 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 14:05:13.266 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 1047103
2025-02-17 14:05:14.997 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 1047103
2025-02-17 14:05:16.636 [http-nio-8081-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 1472
2025-02-17 14:05:16.823 [Executor task launch worker for task 0.0 in stage 19.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.825 [Executor task launch worker for task 0.0 in stage 19.0 (TID 31)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.840 [Executor task launch worker for task 3.0 in stage 19.0 (TID 34)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.840 [Executor task launch worker for task 1.0 in stage 19.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.840 [Executor task launch worker for task 2.0 in stage 19.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.840 [Executor task launch worker for task 3.0 in stage 19.0 (TID 34)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.840 [Executor task launch worker for task 2.0 in stage 19.0 (TID 33)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.840 [Executor task launch worker for task 1.0 in stage 19.0 (TID 32)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-17 14:05:16.844 [Executor task launch worker for task 1.0 in stage 19.0 (TID 32)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 14:05:16.844 [Executor task launch worker for task 3.0 in stage 19.0 (TID 34)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 14:05:16.844 [Executor task launch worker for task 2.0 in stage 19.0 (TID 33)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 14:05:16.844 [Executor task launch worker for task 0.0 in stage 19.0 (TID 31)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-17 14:07:17.413 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4fd27626{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:08:34.465 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:08:34.467 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:08:34.467 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:08:34.467 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:08:34.467 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:08:34.468 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:08:34.476 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:08:34.970 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:08:35.381 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:08:35.406 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:10:31.347 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:10:31.350 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:10:31.350 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:10:31.350 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:10:31.350 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:10:31.351 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:10:31.359 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:10:31.851 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:10:32.257 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:10:32.282 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:14:14.930 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:14:14.932 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:14:14.933 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:14:14.933 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:14:14.933 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:14:14.933 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:14:14.942 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:14:15.440 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:14:15.841 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:14:15.868 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:21:36.131 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:21:36.133 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:21:36.135 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:21:36.135 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:21:36.135 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:21:36.135 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:21:36.143 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:21:36.620 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:21:37.032 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:21:37.056 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:22:40.639 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:22:40.642 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:22:40.643 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:22:40.643 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:22:40.643 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:22:40.644 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:22:40.653 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:22:41.160 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:22:41.583 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask$$$capture(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:22:41.610 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask$$$capture(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:27:31.240 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:27:31.243 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:27:31.243 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:27:31.243 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:27:31.243 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:27:31.244 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:27:31.252 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:27:31.719 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:27:32.127 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:27:32.152 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
java.nio.channels.UnresolvedAddressException: null
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:149)
	at java.base/sun.nio.ch.Net.checkAddress(Net.java:157)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:330)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 14:37:48.778 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:37:48.780 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:37:48.781 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:37:48.781 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:37:48.781 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:37:48.781 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:37:48.790 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:37:49.274 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:37:49.821 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3726ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 14:37:49.884 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 14:37:49.897 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3803ms
2025-02-17 14:37:49.931 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1d94478a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:37:49.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d7dc5f{/,null,AVAILABLE,@Spark}
2025-02-17 14:37:49.962 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
org.apache.spark.SparkException: Could not parse Master URL: 'http://192.168.2.243:8080'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3199)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:582)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 14:37:49.972 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1d94478a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:37:49.998 [restartedMain] WARN  o.apache.spark.metrics.MetricsSystem - Stopping a MetricsSystem that is not running
2025-02-17 14:37:50.003 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
org.apache.spark.SparkException: Could not parse Master URL: 'http://192.168.2.243:8080'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3199)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:582)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 14:38:10.547 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:38:10.549 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:38:10.550 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:38:10.550 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:38:10.550 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:38:10.550 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:38:10.559 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:38:11.025 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:38:11.565 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3596ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 14:38:11.628 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 14:38:11.641 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3671ms
2025-02-17 14:38:11.672 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1b7f3821{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:38:11.685 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c150121{/,null,AVAILABLE,@Spark}
2025-02-17 14:38:11.702 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
org.apache.spark.SparkException: Could not parse Master URL: 'http://192.168.2.243:7070'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3199)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:582)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 14:38:11.713 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1b7f3821{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:38:11.739 [restartedMain] WARN  o.apache.spark.metrics.MetricsSystem - Stopping a MetricsSystem that is not running
2025-02-17 14:38:11.745 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
org.apache.spark.SparkException: Could not parse Master URL: 'http://192.168.2.243:7070'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3199)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:582)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:147)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 14:39:42.430 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:39:42.435 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:39:42.437 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:39:42.437 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:39:42.437 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:39:42.438 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:39:42.450 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:39:43.084 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:39:43.772 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4721ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 14:39:43.844 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 14:39:43.858 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4806ms
2025-02-17 14:39:43.893 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@53773f35{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:39:43.908 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1544ec2f{/,null,AVAILABLE,@Spark}
2025-02-17 14:39:43.929 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
org.apache.spark.SparkException: Could not parse Master URL: 'http://192.168.2.243:7070'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3199)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:582)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:146)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 14:39:43.940 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@53773f35{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 14:39:43.972 [restartedMain] WARN  o.apache.spark.metrics.MetricsSystem - Stopping a MetricsSystem that is not running
2025-02-17 14:39:43.978 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 创建SparkSession失败
org.apache.spark.SparkException: Could not parse Master URL: 'http://192.168.2.243:7070'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3199)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:582)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:146)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 14:52:49.977 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 14:52:49.979 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 14:52:49.980 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 14:52:49.980 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 14:52:49.980 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 14:52:49.980 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 14:52:49.990 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 14:52:50.285 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 14:52:50.308 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 14:52:50.465 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 14:52:51.054 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3755ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 14:52:51.118 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 14:52:51.130 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3832ms
2025-02-17 14:52:51.148 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7070. Attempting port 7071.
2025-02-17 14:52:51.165 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@31cc7e3d{HTTP/1.1, (http/1.1)}{0.0.0.0:7071}
2025-02-17 14:52:51.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f588a42{/,null,AVAILABLE,@Spark}
2025-02-17 14:59:24.960 [shutdown-hook-0] ERROR o.a.spark.util.ShutdownHookManager - Exception while deleting Spark temp dir: C:\Users\xubei\spark-temp\spark-0e490366-0e15-48be-bb84-f6bcfa02bc4d\userFiles-7cc2d16c-78c9-4ba6-b9a8-4ccf5db260af
java.nio.file.NoSuchFileException: C:\Users\xubei\spark-temp\spark-0e490366-0e15-48be-bb84-f6bcfa02bc4d\userFiles-7cc2d16c-78c9-4ba6-b9a8-4ccf5db260af
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:199)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1851)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:124)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 15:00:34.270 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:00:34.273 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:00:34.274 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:00:34.274 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:00:34.274 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:00:34.274 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:00:34.283 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:00:34.576 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:00:34.597 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:00:34.749 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:00:35.332 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3667ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 15:00:35.395 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 15:00:35.406 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3741ms
2025-02-17 15:00:35.424 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 15:00:35.441 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@75b28898{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 15:00:35.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bd655cf{/,null,AVAILABLE,@Spark}
2025-02-17 15:01:39.690 [shutdown-hook-0] ERROR o.a.spark.util.ShutdownHookManager - Exception while deleting Spark temp dir: C:\Users\xubei\spark-temp\spark-79ec094f-ba6a-495a-9432-82754c108d3e\userFiles-e466906a-0a14-48f1-a95f-d0b733097c0c
java.nio.file.NoSuchFileException: C:\Users\xubei\spark-temp\spark-79ec094f-ba6a-495a-9432-82754c108d3e\userFiles-e466906a-0a14-48f1-a95f-d0b733097c0c
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:199)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1851)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:124)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 15:02:19.408 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:02:19.410 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:02:19.411 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:02:19.411 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:02:19.411 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:02:19.411 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:02:19.420 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:02:19.715 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:02:19.737 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:02:19.887 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:09:04.842 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:09:04.845 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:09:04.845 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:09:04.845 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:09:04.845 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:09:04.846 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:09:04.855 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:09:05.211 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:09:05.239 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:09:05.421 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:19:56.127 [shutdown-hook-0] ERROR o.a.spark.util.ShutdownHookManager - Exception while deleting Spark temp dir: C:\Users\xubei\spark-temp\spark-d312aa70-c4af-4290-bc01-5e72bcbfdd39\userFiles-54272864-1891-47d3-aca0-9cc9b5152f19
java.nio.file.NoSuchFileException: C:\Users\xubei\spark-temp\spark-d312aa70-c4af-4290-bc01-5e72bcbfdd39\userFiles-54272864-1891-47d3-aca0-9cc9b5152f19
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:199)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1851)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:124)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 15:22:27.487 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:22:27.489 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:22:27.490 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:22:27.490 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:22:27.490 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:22:27.490 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:22:27.499 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:22:27.785 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:22:27.807 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:22:27.961 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:22:28.662 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3755ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 15:22:28.757 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:22:28.787 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:22:28.787 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:28:09.397 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:28:09.398 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:28:09.399 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:28:09.399 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:28:09.399 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:28:09.400 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:28:09.402 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:28:09.404 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:28:09.404 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:28:09.405 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:28:09.483 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:28:09.488 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:28:09.488 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:31:18.594 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:31:18.595 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:31:18.595 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:31:18.595 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:31:18.595 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:31:18.596 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:31:18.598 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:31:18.599 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:31:18.599 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:31:18.600 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:31:18.679 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:31:18.681 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:31:18.681 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:34:48.318 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:34:48.321 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:34:48.321 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:34:48.321 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:34:48.322 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:34:48.322 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:34:48.332 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:34:48.631 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:34:48.653 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:34:48.808 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:34:49.495 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3817ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 15:34:49.585 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:34:49.610 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:34:49.611 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:42:10.332 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:42:10.335 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:42:10.335 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:42:10.335 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:42:10.336 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:42:10.336 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:42:10.344 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:42:10.651 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:42:10.672 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:42:10.825 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:42:11.512 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3785ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 15:42:11.598 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:42:11.623 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:42:11.623 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:42:59.504 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:42:59.507 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:42:59.507 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:42:59.507 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:42:59.507 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:42:59.508 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:42:59.516 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:42:59.809 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:42:59.831 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:42:59.981 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:43:00.658 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3790ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 15:43:00.747 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:43:00.771 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:43:00.772 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:45:25.629 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:45:25.630 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:45:25.630 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:45:25.630 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:45:25.630 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:45:25.631 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:45:25.634 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:45:25.634 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:45:25.635 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:45:25.635 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:45:25.712 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:45:25.715 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:45:25.715 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:46:01.916 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:46:01.919 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:46:01.919 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:46:01.919 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:46:01.920 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:46:01.920 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:46:01.929 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:46:02.255 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:46:02.278 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:46:02.432 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:46:03.104 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3785ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 15:46:03.192 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:46:03.216 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:46:03.217 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:49:12.357 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:49:12.358 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:49:12.359 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:49:12.359 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:49:12.359 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:49:12.360 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:49:12.362 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:49:12.364 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:49:12.364 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:49:12.364 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:49:12.443 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:49:12.448 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:49:12.448 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:49:28.718 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:49:28.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:49:28.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:49:28.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:49:28.720 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:49:28.721 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:49:28.723 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:49:28.724 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:49:28.724 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:49:28.725 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:49:28.798 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:49:28.801 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/实习demo/spark/src/main/hadoop
2025-02-17 15:49:28.801 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 15:57:56.228 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 15:57:56.230 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 15:57:56.231 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 15:57:56.231 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 15:57:56.231 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 15:57:56.231 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 15:57:56.240 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 15:57:56.536 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:57:56.559 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 15:57:56.713 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 15:57:57.402 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4130ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 15:57:57.491 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 15:57:57.515 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 15:57:57.515 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:11:10.002 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:11:10.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:11:10.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:11:10.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:11:10.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:11:10.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:11:10.014 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:11:10.348 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:11:10.369 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:11:10.521 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:11:11.220 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3880ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:11:11.314 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:11:11.340 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:11:11.340 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:11:35.194 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/测试文件/竞彩数据.csv
2025-02-17 16:11:35.281 [http-nio-8080-exec-6] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 16:11:43.222 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 1048575
2025-02-17 16:11:43.222 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 287404
2025-02-17 16:11:43.222 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 22
2025-02-17 16:11:43.222 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 72.59%
2025-02-17 16:11:43.222 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 16:11:44.078 [http-nio-8080-exec-6] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 16:11:56.149 [http-nio-8080-exec-6] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 '竞彩数据.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 16:15:31.883 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:15:31.886 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:15:31.886 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:15:31.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:15:31.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:15:31.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:15:31.895 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:15:32.192 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:15:32.214 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:15:32.366 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:15:33.050 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3923ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:15:33.137 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:15:33.161 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:15:33.162 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:15:44.732 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/测试文件/Demo.csv
2025-02-17 16:15:44.793 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 16:15:48.347 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 1819
2025-02-17 16:15:48.348 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 235
2025-02-17 16:15:48.348 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 6
2025-02-17 16:15:48.348 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 87.08%
2025-02-17 16:15:48.348 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 16:15:49.162 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 16:15:59.589 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 16:18:14.925 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:18:14.927 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:18:14.928 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:18:14.928 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:18:14.928 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:18:14.928 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:18:14.937 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:18:15.253 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:18:15.276 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:18:15.431 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:18:16.100 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3811ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:18:16.187 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:18:16.212 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:18:16.212 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:18:21.848 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/测试文件/Demo.csv
2025-02-17 16:18:21.907 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 16:18:25.462 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 28
2025-02-17 16:18:25.462 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 2
2025-02-17 16:18:25.462 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 2
2025-02-17 16:18:25.462 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 92.86%
2025-02-17 16:18:25.462 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 16:18:26.304 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 16:18:36.651 [http-nio-8080-exec-1] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 16:20:29.447 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:20:29.450 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:20:29.451 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:20:29.451 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:20:29.451 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:20:29.451 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:20:29.459 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:20:29.772 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:20:29.794 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:20:29.945 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:20:30.619 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3823ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:20:30.704 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:20:30.727 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:20:30.728 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:24:36.483 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:24:36.486 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:24:36.487 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:24:36.487 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:24:36.487 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:24:36.487 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:24:36.497 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:24:36.838 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:24:36.862 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:24:37.015 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:24:37.727 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4024ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:24:37.821 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:24:37.845 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:24:37.847 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:26:58.478 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:26:58.481 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:26:58.482 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:26:58.482 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:26:58.482 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:26:58.482 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:26:58.490 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:26:58.790 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:26:58.812 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:26:58.964 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:26:59.516 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3667ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:26:59.592 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:26:59.608 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3759ms
2025-02-17 16:26:59.634 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:26:59.654 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@758aac31{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:26:59.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cbf860b{/,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.803 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@cbf860b{/,null,STOPPED,@Spark}
2025-02-17 16:26:59.804 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58d4c0a4{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.804 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a0b7d08{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c6c9bf9{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bc928bd{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cb026e2{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2701b1d3{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.808 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18159d52{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.808 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f8ad2cb{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.808 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@390e658c{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79e58b87{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@125a9091{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e8f9e5{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cefb81d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c509e0{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4953f6e9{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4603bdb9{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34ed5386{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@903ab7e{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d458c99{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a0b6822{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c061b93{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.814 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68090c34{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a78d453{/static,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b57e6d8{/,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.822 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@797dc338{/api,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@416c471a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@325113b1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55757298{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:26:59.895 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:26:59.920 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:26:59.920 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:27:45.265 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@758aac31{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:27:50.426 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:27:50.429 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:27:50.429 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:27:50.429 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:27:50.429 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:27:50.430 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:27:50.438 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:27:50.737 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:27:50.760 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:27:50.913 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:27:51.459 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3677ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:27:51.523 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:27:51.535 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3753ms
2025-02-17 16:27:51.554 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:27:51.571 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@fe1c18c{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:27:51.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5be59f{/,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2d5be59f{/,null,STOPPED,@Spark}
2025-02-17 16:27:51.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e0ccbc8{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.718 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b7ad788{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.718 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@938d636{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.719 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d57776{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.719 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70259ad8{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.719 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77351998{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.720 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5772ac5d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.720 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39114c80{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.721 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60ac175f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.721 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c5cde7c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.721 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c5c7458{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.723 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e3948b5{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.723 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39d924a2{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.723 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fe4aa8a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.724 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a11d078{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.724 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a209166{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.724 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cc49e12{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.725 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@435ff69{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.725 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7498624e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.725 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12076987{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.725 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b8ee7dc{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.726 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b1c1eee{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.732 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50048601{/static,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.732 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40dc93b8{/,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.733 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610336c7{/api,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.734 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4dee6583{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.734 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4172fcda{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.737 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cf2fb51{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:51.808 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:27:51.832 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:27:51.832 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:27:53.676 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/测试文件/Demo.csv
2025-02-17 16:27:53.741 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 16:27:54.015 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e1aef5c{/SQL,null,AVAILABLE,@Spark}
2025-02-17 16:27:54.016 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7655d25c{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:54.016 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77ccc0bc{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 16:27:54.017 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d112224{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 16:27:54.018 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f5a9249{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 16:27:57.285 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 28
2025-02-17 16:27:57.285 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 2
2025-02-17 16:27:57.285 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 2
2025-02-17 16:27:57.285 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 92.86%
2025-02-17 16:27:57.285 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 16:27:58.122 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 16:28:08.490 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 16:34:42.229 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@fe1c18c{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:34:48.322 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:34:48.325 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:34:48.331 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:34:48.331 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:34:48.331 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:34:48.332 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:34:48.340 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:34:48.657 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:34:48.680 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:34:48.831 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:34:49.464 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3774ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:34:49.528 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:34:49.541 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3851ms
2025-02-17 16:34:49.560 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:34:49.578 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@370a0a2c{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:34:49.592 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a52fbbf{/,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.728 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6a52fbbf{/,null,STOPPED,@Spark}
2025-02-17 16:34:49.729 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ade1c83{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.730 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25a868e9{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.730 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21be52b2{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.731 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a71a287{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.731 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126932eb{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.731 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@504813c7{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.732 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@132a50f4{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.733 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34d01532{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.733 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14de78cb{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.733 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b01bb18{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.734 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f3357c{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.738 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34fd285{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.739 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d56b180{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@169422c7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b31885b{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@573b35c1{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.741 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c980d25{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.741 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@779068bf{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.741 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41a1f00a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12579e62{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50853e7b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e150b58{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.748 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdbc4ed{/static,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.749 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cf00984{/,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@150905bc{/api,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d731cd{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.751 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e9641b4{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57429fd0{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:49.827 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:34:49.853 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:34:49.854 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:34:53.325 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/测试文件/Demo.csv
2025-02-17 16:34:53.388 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 16:34:53.677 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d711457{/SQL,null,AVAILABLE,@Spark}
2025-02-17 16:34:53.678 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fb1ef33{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:53.679 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d860909{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 16:34:53.679 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6af6240d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 16:34:53.680 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7655d25c{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 16:34:57.083 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 28
2025-02-17 16:34:57.083 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 2
2025-02-17 16:34:57.083 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 2
2025-02-17 16:34:57.083 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 92.86%
2025-02-17 16:34:57.083 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 16:34:57.932 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 16:35:09.067 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 16:35:33.907 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@370a0a2c{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:35:37.625 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:35:37.627 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:35:37.628 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:35:37.628 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:35:37.628 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:35:37.628 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:35:37.638 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:35:37.949 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:35:37.972 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:35:38.130 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:35:38.685 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3792ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:35:38.747 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:35:38.760 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3867ms
2025-02-17 16:35:38.780 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:35:38.796 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@78333821{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:35:38.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4881fb5b{/,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4881fb5b{/,null,STOPPED,@Spark}
2025-02-17 16:35:38.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a71a287{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126932eb{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.951 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@442e899c{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.951 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64c3d164{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.951 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@132a50f4{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34d01532{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f3357c{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34fd285{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d56b180{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@169422c7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b31885b{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.955 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@573b35c1{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.955 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c980d25{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.956 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@779068bf{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.956 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41a1f00a{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.956 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12579e62{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.957 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50853e7b{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.957 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e150b58{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.957 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdbc4ed{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.958 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63f9547d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.958 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66928614{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.959 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c50b277{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.965 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cf8d698{/static,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.965 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2314d55b{/,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.966 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f22b51{/api,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.967 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14074ce{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.967 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@666948c3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:35:38.971 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@643ac664{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:35:39.045 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:35:39.072 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:35:39.073 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:44:53.371 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@78333821{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:44:58.718 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:44:58.721 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:44:58.722 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:44:58.722 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:44:58.722 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:44:58.722 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:44:58.730 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:44:59.037 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:44:59.059 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:44:59.211 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:44:59.778 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3795ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:44:59.841 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:44:59.853 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3870ms
2025-02-17 16:44:59.871 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:44:59.888 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3d0caed1{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:44:59.901 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a465bca{/,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4a465bca{/,null,STOPPED,@Spark}
2025-02-17 16:45:00.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e1a8c32{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f87b62{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58e96604{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@590f950b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79948e9a{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa169a{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4191c3a9{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.050 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@671af039{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.050 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f8b0eec{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.051 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74853218{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.051 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23f84f7e{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.051 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ee2b57b{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.053 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55323f69{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.053 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f4c4e46{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.053 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f8c0a66{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.054 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75e07e5a{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.054 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf37b0{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.054 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b321ede{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.055 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33c3f512{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.055 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@608eb10d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.055 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7bfc2151{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.056 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@790bdf62{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.062 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dfc29b5{/static,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.063 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23154b68{/,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.064 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7bba16cf{/api,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.065 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51b2b695{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.065 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33479202{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.069 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50dea147{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:45:00.158 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:45:00.185 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:45:00.186 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:46:01.650 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3d0caed1{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:46:06.728 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:46:06.730 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:46:06.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:46:06.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:46:06.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:46:06.731 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:46:06.740 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:46:07.053 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:46:07.075 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:46:07.228 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:46:07.791 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3694ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:46:07.860 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:46:07.875 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3777ms
2025-02-17 16:46:07.896 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:46:07.914 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@77aff352{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:46:07.928 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39981052{/,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.075 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@39981052{/,null,STOPPED,@Spark}
2025-02-17 16:46:08.076 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@131d9e28{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.076 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@618b421a{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.077 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fe02edc{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69ca4402{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52a915b2{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@512143bd{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.079 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4568c71e{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.079 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2220ae4e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bf80de4{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@388d0137{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.081 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2047df7f{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.081 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bc23d8d{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.081 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35bd0d7b{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.082 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@448f85a1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.082 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e560a26{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57fb06a7{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@527b35b4{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b1e9792{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c9475cc{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ce5fb4b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dc732d6{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f67bd18{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.091 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c8b1f1{/static,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.091 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760ce99a{/,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.093 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38a2f222{/api,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.094 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@157bc487{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.094 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a76b60b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.097 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66a9fe6{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:46:08.169 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:46:08.201 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:46:08.201 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:47:07.997 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@77aff352{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:47:13.090 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:47:13.092 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:47:13.093 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:47:13.093 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:47:13.093 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:47:13.093 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:47:13.103 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:47:13.421 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:47:13.443 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:47:13.594 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:47:14.154 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3708ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:47:14.216 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:47:14.230 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3783ms
2025-02-17 16:47:14.248 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:47:14.265 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@79520b2f{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:47:14.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@708cab3d{/,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@708cab3d{/,null,STOPPED,@Spark}
2025-02-17 16:47:14.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17881b5f{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@279c7f02{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f65281d{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@256d0c27{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61494acc{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.418 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1483ac55{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.418 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@335b1fe5{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7de764ad{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f592f5{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@283e0b8d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.420 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f232074{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.420 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f5c5b69{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.420 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3129a53f{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.422 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3913008c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.422 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a1631d8{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.422 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43e2c9d3{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.422 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dbd2778{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.423 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f449f65{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.423 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fc8140d{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.423 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@555d53a6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.424 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72bfed28{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.424 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3958d8af{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.430 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e56bd64{/static,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.430 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@775ea700{/,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.431 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9454570{/api,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.432 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13ec7c84{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.432 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b19a248{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.435 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fc08035{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:47:14.507 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:47:14.532 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:47:14.532 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:53:52.230 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@79520b2f{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:53:58.127 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:53:58.130 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:53:58.131 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:53:58.131 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:53:58.131 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:53:58.131 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:53:58.141 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:53:58.467 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:53:58.491 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:53:58.652 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:53:59.235 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3857ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:53:59.302 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:53:59.316 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3937ms
2025-02-17 16:53:59.335 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:53:59.352 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@188dc4bf{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:53:59.365 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29e4494d{/,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.511 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@29e4494d{/,null,STOPPED,@Spark}
2025-02-17 16:53:59.512 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b3c08c1{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.512 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282895ba{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.513 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ef63fd9{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.514 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e962d3e{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.514 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ce84e97{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.514 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cc18700{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.515 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f884ae1{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.516 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@545cd486{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.516 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@654b099e{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.516 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29901080{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.516 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40ae2350{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.518 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fe5f73{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.518 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48a0a904{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.518 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@973fc09{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.518 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10291561{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.519 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e4ec8b4{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.519 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@85c315b{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.520 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37237d1a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.520 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bdef53b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.520 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38fff4cf{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.521 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@287f2af3{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.521 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f2eb21a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.527 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b713ce{/static,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.528 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41601e98{/,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.529 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@131d8cda{/api,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.530 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a1d9a2c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.530 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4690ce0b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.533 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45f8c05c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:53:59.609 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:53:59.635 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:53:59.636 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:54:53.457 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-17 16:54:53.532 [http-nio-8080-exec-4] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 16:54:53.813 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7186e5be{/SQL,null,AVAILABLE,@Spark}
2025-02-17 16:54:53.814 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13d356e3{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 16:54:53.814 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710d63e1{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 16:54:53.814 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72fce7ac{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 16:54:53.816 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a802ee9{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 16:54:57.207 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 9
2025-02-17 16:54:57.207 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1
2025-02-17 16:54:57.207 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 1
2025-02-17 16:54:57.207 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 88.89%
2025-02-17 16:54:57.207 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 16:54:58.076 [http-nio-8080-exec-4] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 16:55:08.344 [http-nio-8080-exec-4] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 16:57:00.669 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@188dc4bf{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:57:41.795 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:57:41.799 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 16:57:41.800 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 16:57:41.800 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 16:57:41.800 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 16:57:41.801 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 16:57:41.817 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 16:57:42.326 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:57:42.362 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 16:57:42.629 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 16:57:43.591 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @6493ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 16:57:43.699 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 16:57:43.720 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @6623ms
2025-02-17 16:57:43.749 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 16:57:43.779 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2fea1606{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 16:57:43.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c0aeb6c{/,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4c0aeb6c{/,null,STOPPED,@Spark}
2025-02-17 16:57:44.079 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23c30165{/jobs,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64939bbb{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.082 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe36274{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b37b34f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@536125c8{/stages,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2945f14b{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22fe4719{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f9afa43{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c4a9c61{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.091 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@342ccf72{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.092 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56487487{/storage,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.093 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b5c3bb4{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.094 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9d1fbd{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.095 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@401ac315{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.097 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18920f68{/environment,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.100 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ad0fcf4{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.101 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cd459c8{/executors,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.102 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39620775{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.103 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cb36816{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.104 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@227e1077{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cc82443{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2abc9de{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.116 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42c9751{/static,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.117 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7be55c41{/,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.120 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@698a29c7{/api,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.122 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24576b88{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2850a0d6{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@82c3e52{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:44.364 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 16:57:44.450 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 16:57:44.451 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 16:57:49.556 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理竞彩数据文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-17 16:57:49.766 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 16:57:50.402 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dc6a7c4{/SQL,null,AVAILABLE,@Spark}
2025-02-17 16:57:50.404 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d06d5c8{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:50.404 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74ea1cb7{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 16:57:50.405 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70da7677{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 16:57:50.407 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a714819{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 16:57:57.353 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据行数: 9
2025-02-17 16:57:57.353 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据行数: 1
2025-02-17 16:57:57.353 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果行数: 1
2025-02-17 16:57:57.353 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 数据过滤率: 88.89%
2025-02-17 16:57:57.353 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理后数据样本:
2025-02-17 16:57:58.679 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 分析结果样本:
2025-02-17 16:58:09.178 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理竞彩数据文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 17:03:20.508 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2fea1606{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 17:03:26.135 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:03:26.137 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:03:26.138 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:03:26.138 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:03:26.138 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:03:26.138 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:03:26.148 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:03:26.452 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:03:26.476 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:03:26.639 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:03:27.203 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3773ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:03:27.267 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:03:27.279 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3849ms
2025-02-17 17:03:27.297 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 7077. Attempting port 7078.
2025-02-17 17:03:27.313 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@8483784{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 17:03:27.325 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cf02fdc{/,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.456 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3cf02fdc{/,null,STOPPED,@Spark}
2025-02-17 17:03:27.456 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f61609c{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.457 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79091fa{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.457 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f70c0cf{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.457 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@244bd2c1{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.458 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456ff503{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.458 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d26a228{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.459 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b0256b3{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.459 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dc5866b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.460 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30b00bd0{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.460 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b2bc0e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.461 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2284e781{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.461 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a7769da{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.465 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79d2f0ee{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.467 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48fd80bb{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.468 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27e83aad{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.468 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@625f159a{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.468 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52b62ae4{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.469 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13ea8c01{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.469 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ec76edf{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.470 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d4671e6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.470 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9a23f5a{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.470 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@268e9992{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.476 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@123872a9{/static,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.476 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14770565{/,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.478 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@643d979e{/api,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.479 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a4b9a91{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.480 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1571d1dc{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.483 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7a6e24{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:27.552 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:03:27.576 [restartedMain] INFO  s.controller.HadoopConfig - 正在设置hadoop.home.dir为: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:03:27.577 [restartedMain] INFO  s.controller.HadoopConfig - 已找到winutils.exe，位置: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-17 17:03:35.809 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-17 17:03:35.868 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 17:03:36.123 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33d80f39{/SQL,null,AVAILABLE,@Spark}
2025-02-17 17:03:36.123 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cf5f57e{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:36.124 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ca78b59{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 17:03:36.124 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bc55427{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 17:03:36.125 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d6d9d58{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 17:03:39.407 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 9
2025-02-17 17:03:39.816 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 9
2025-02-17 17:03:39.816 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 17:03:39.816 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 0
2025-02-17 17:03:39.924 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 0
2025-02-17 17:03:40.019 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 9
2025-02-17 17:03:50.016 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 17:34:28.195 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@8483784{HTTP/1.1, (http/1.1)}{0.0.0.0:7078}
2025-02-17 17:34:33.989 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:34:33.992 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:34:33.999 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:34:33.999 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:34:33.999 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:34:34.000 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:34:34.008 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:34:34.349 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:34:34.372 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:34:34.528 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:34:35.095 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3854ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:34:35.162 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:34:35.175 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3933ms
2025-02-17 17:34:35.207 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4de19e6f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:34:35.221 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17e195d6{/,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.360 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@17e195d6{/,null,STOPPED,@Spark}
2025-02-17 17:34:35.361 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1741a82d{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.362 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ee90acf{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.362 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6eae021{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.362 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ad69d5d{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.363 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ea0f8f{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.363 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44c742d9{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.364 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cc53b64{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.364 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7863b975{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.365 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@704c340a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.365 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53968199{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.365 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ede3e29{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.366 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56e6d429{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.366 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b6a7e33{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.367 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22b17cb2{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.367 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd2f119{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.367 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5134bcf6{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.368 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32adb2c5{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.368 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@762efc09{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.369 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d5409f9{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.369 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3744dd2e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.369 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4df26834{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.370 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37a455b9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cc8ec1f{/static,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c45272d{/,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.377 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1247eee8{/api,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5907a769{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34384833{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60ce577{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:34:35.451 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:34:35.475 [restartedMain] ERROR s.controller.HadoopConfig - Hadoop环境初始化失败
java.lang.RuntimeException: 必要的Hadoop目录不存在: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\etc
	at sparkanalysis.controller.HadoopConfig.validateHadoopDirectoryStructure(HadoopConfig.java:54)
	at sparkanalysis.controller.HadoopConfig.init(HadoopConfig.java:29)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:401)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:219)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1767)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:601)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 17:34:35.485 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4de19e6f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:39:43.684 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:39:43.686 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:39:43.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:39:43.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:39:43.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:39:43.687 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:39:43.696 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:39:43.995 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:39:44.016 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:39:44.165 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:39:44.706 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3678ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:39:44.769 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:39:44.782 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3754ms
2025-02-17 17:39:44.815 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2d67ff16{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:39:44.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a4a6db8{/,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.965 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3a4a6db8{/,null,STOPPED,@Spark}
2025-02-17 17:39:44.966 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12ca90ec{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.967 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d3d773b{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.967 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b7c342{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.968 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b43b6dc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.968 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c4c00d6{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.968 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40bfc352{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.969 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65da2e0e{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.969 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d01d40{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.969 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5051c34f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.971 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74968440{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.971 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@378def88{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.971 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420cf497{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.971 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3306dfa1{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.972 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52975f51{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.972 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40362d2b{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.972 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa0cf7b{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.972 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e0ccbc8{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.973 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b7ad788{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.973 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b89dc5e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.973 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@938d636{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.975 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d57776{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.975 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70259ad8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.981 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77351998{/static,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.982 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b8ee7dc{/,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.983 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b1c1eee{/api,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.984 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@314ab809{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.984 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57bec4c7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:39:44.987 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f453f3e{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:39:45.057 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:39:45.083 [restartedMain] ERROR s.controller.HadoopConfig - Hadoop环境初始化失败
java.lang.RuntimeException: 必要的Hadoop目录不存在: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\lib
	at sparkanalysis.controller.HadoopConfig.validateHadoopDirectoryStructure(HadoopConfig.java:54)
	at sparkanalysis.controller.HadoopConfig.init(HadoopConfig.java:29)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:401)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:219)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1767)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:601)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 17:39:45.091 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2d67ff16{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:40:33.057 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:40:33.059 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:40:33.060 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:40:33.060 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:40:33.060 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:40:33.060 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:40:33.070 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:40:33.385 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:40:33.406 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:40:33.558 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:40:34.097 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3663ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:40:34.157 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:40:34.170 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3735ms
2025-02-17 17:40:34.206 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@581b7c77{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:40:34.220 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cba06ea{/,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.351 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6cba06ea{/,null,STOPPED,@Spark}
2025-02-17 17:40:34.352 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33f7ab2a{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.353 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf2a44{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.353 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75602afa{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ddc624{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@186eb3fa{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@266fbcb8{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab1a44f{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5eda8de1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.356 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ce86b81{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.356 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30909b42{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15e6fb41{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6613cabe{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47acb68f{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.358 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccf83b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.358 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@519e8391{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.358 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a321ab0{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.359 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e3d9bd0{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.359 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3519144b{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.360 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ce02f99{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.360 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dfa9ddc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.361 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40be752c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.361 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a7a10b4{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.367 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c5b1b55{/static,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.367 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4accf940{/,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.369 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f56861a{/api,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.369 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@192f28b8{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.369 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e00bfbf{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e735fd3{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:40:34.443 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:40:34.469 [restartedMain] ERROR s.controller.HadoopConfig - Hadoop环境初始化失败
java.lang.RuntimeException: 必要的Hadoop目录不存在: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\sbin
	at sparkanalysis.controller.HadoopConfig.validateHadoopDirectoryStructure(HadoopConfig.java:54)
	at sparkanalysis.controller.HadoopConfig.init(HadoopConfig.java:29)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:401)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:219)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1767)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:601)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-17 17:40:34.477 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@581b7c77{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:42:41.809 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:42:41.811 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:42:41.812 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:42:41.812 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:42:41.812 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:42:41.812 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:42:41.821 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:42:42.129 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:42:42.152 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:42:42.312 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:42:42.907 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3792ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:42:42.974 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:42:42.986 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3872ms
2025-02-17 17:42:43.019 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2f486020{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:42:43.033 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782aa31{/,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@782aa31{/,null,STOPPED,@Spark}
2025-02-17 17:42:43.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c376784{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75453814{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2290d8fa{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57ad4647{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4372313d{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4951998a{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46f19518{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41141ac8{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14339386{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3836c30d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6acba890{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17881b5f{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@279c7f02{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d02fe3c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.180 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f65281d{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.180 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@256d0c27{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.180 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61494acc{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1483ac55{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d00c9aa{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@391cf354{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@335b1fe5{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7de764ad{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.188 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f592f5{/static,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.188 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74f65965{/,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.190 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7edcb643{/api,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.190 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d3c26b2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.191 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fdfc041{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.194 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4196a542{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:43.271 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:42:43.299 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-17 17:42:56.620 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-17 17:42:56.689 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 17:42:56.961 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c4b9ec8{/SQL,null,AVAILABLE,@Spark}
2025-02-17 17:42:56.962 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77a09c77{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:56.963 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d6cca5c{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 17:42:56.963 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d8cf7a{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 17:42:56.964 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142bd0c9{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 17:43:00.446 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 9
2025-02-17 17:43:00.914 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 9
2025-02-17 17:43:00.914 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 17:43:00.914 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 0
2025-02-17 17:43:01.042 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 0
2025-02-17 17:43:01.161 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 9
2025-02-17 17:43:11.147 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 17:45:05.020 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2f486020{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:45:08.775 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:45:08.777 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:45:08.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:45:08.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:45:08.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:45:08.778 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:45:08.786 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:45:09.096 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:45:09.119 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:45:09.276 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:45:09.833 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3780ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:45:09.898 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:45:09.911 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3857ms
2025-02-17 17:45:09.943 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@c9d6caa{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:45:09.957 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@234c6bf3{/,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.105 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@234c6bf3{/,null,STOPPED,@Spark}
2025-02-17 17:45:10.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2290d8fa{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57ad4647{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4951998a{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77843c44{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.108 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d712941{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.108 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46f19518{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.108 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3836c30d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6acba890{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17881b5f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@279c7f02{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d02fe3c{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f65281d{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@256d0c27{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61494acc{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1483ac55{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.111 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d00c9aa{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@391cf354{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@335b1fe5{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7de764ad{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30f592f5{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@283e0b8d{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f232074{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.120 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f5c5b69{/static,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.121 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f86fd06{/,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.122 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13f6bc9d{/api,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77c7d77{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7641ecc7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.126 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@173996cd{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:10.198 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:45:10.224 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-17 17:45:22.156 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-17 17:45:22.217 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 17:45:22.493 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b3aea46{/SQL,null,AVAILABLE,@Spark}
2025-02-17 17:45:22.494 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@682c0661{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:22.494 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a175a8b{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 17:45:22.495 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16070cf1{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 17:45:22.496 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a16fb17{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 17:45:25.947 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 9
2025-02-17 17:45:26.360 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 9
2025-02-17 17:45:26.360 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 17:45:26.360 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 0
2025-02-17 17:45:26.473 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 0
2025-02-17 17:45:26.578 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 9
2025-02-17 17:45:36.684 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-17 17:55:00.125 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@c9d6caa{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:55:06.002 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:55:06.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:55:06.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:55:06.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:55:06.005 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:55:06.006 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:55:06.015 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:55:06.308 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:55:06.330 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:55:06.480 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:55:07.039 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3786ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:55:07.101 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:55:07.114 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3861ms
2025-02-17 17:55:07.147 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@668b3895{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:55:07.160 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72f13222{/,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@72f13222{/,null,STOPPED,@Spark}
2025-02-17 17:55:07.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd47e40{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d282af{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdae92e{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319df919{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.298 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47096ce8{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.298 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a1bcd59{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.299 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@618d56ac{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.299 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e6e6a91{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.299 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11d50ab6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.300 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51a49cdb{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@426af97e{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21a1d24b{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.306 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51c3d982{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c59f63b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cdab252{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f1a7ae{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f292a11{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@221ab84f{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50dc661f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33aa52a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bd65132{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c6ab6b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.315 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c6db337{/static,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e73087e{/,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d7c25af{/api,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@605024b2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e001f48{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.321 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@99311ad{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:07.393 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:55:07.417 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-17 17:55:11.453 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-17 17:55:11.531 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 17:55:11.803 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ff3403{/SQL,null,AVAILABLE,@Spark}
2025-02-17 17:55:11.803 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7da808ba{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:11.803 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41ff21b2{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 17:55:11.804 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cacd5bc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 17:55:11.805 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12eaf195{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 17:55:15.215 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 9
2025-02-17 17:55:15.679 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 空值数量: 9
2025-02-17 17:55:15.679 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 17:55:15.679 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 0
2025-02-17 17:55:15.797 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 0
2025-02-17 17:55:15.917 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 9
2025-02-17 17:55:16.020 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理文件 'Demo.csv' 时出错
org.apache.spark.sql.AnalysisException: Partition column `shengName` not found in schema struct<票号:bigint,省中心名称:string,市中心名称:string,门店编号:bigint,销售终端编号:bigint,销售日期:string,销售时间:string,体育项目:string,过关方式:string,游戏:string,票面金额:double,投注内容:string,倍数:int>.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.partitionColumnNotFoundInSchemaError(QueryCompilationErrors.scala:1684)
	at org.apache.spark.sql.execution.datasources.PartitioningUtils$.$anonfun$partitionColumnsSchema$3(PartitioningUtils.scala:582)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.PartitioningUtils$.$anonfun$partitionColumnsSchema$1(PartitioningUtils.scala:580)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.datasources.PartitioningUtils$.partitionColumnsSchema(PartitioningUtils.scala:579)
	at org.apache.spark.sql.execution.datasources.PartitioningUtils$.validatePartitionColumn(PartitioningUtils.scala:564)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:129)
	at sparkanalysis.controller.FileController.processFile(FileController.java:101)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-17 17:57:15.397 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@668b3895{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:57:21.255 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:57:21.258 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:57:21.258 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:57:21.258 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:57:21.258 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:57:21.258 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:57:21.267 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:57:21.558 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:57:21.582 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:57:21.751 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:57:22.336 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3879ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:57:22.401 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:57:22.414 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3957ms
2025-02-17 17:57:22.446 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4fb30c29{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:57:22.459 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1984f67a{/,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.595 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1984f67a{/,null,STOPPED,@Spark}
2025-02-17 17:57:22.596 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75ed5cd3{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.596 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75abd6c9{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f98de94{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.602 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b1dd99{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.603 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1916788c{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.603 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28acbec3{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.603 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@371f97fd{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.604 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450fa2d4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.604 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@540dd6db{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.604 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59f8d0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15c459e1{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25335518{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12848d6c{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.607 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5acb4ebf{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.607 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fd2f826{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.607 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10f8ccf6{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.608 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ef23103{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.608 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1eecd94a{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.609 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5888b11a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.609 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@418c3a25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.610 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14e2877f{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.610 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1976ebdb{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.616 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f9faa99{/static,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.617 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@efe3090{/,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42006702{/api,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@574237e6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.618 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@615609ea{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.621 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4843c26c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:22.695 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:57:22.721 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-17 17:57:37.523 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4fb30c29{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:57:43.032 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-17 17:57:43.034 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-17 17:57:43.035 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-17 17:57:43.035 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-17 17:57:43.035 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-17 17:57:43.035 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-17 17:57:43.044 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-17 17:57:43.350 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:57:43.374 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-17 17:57:43.527 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-17 17:57:44.087 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3684ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-17 17:57:44.151 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-17 17:57:44.164 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3761ms
2025-02-17 17:57:44.196 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@54a7774b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-17 17:57:44.209 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183400b2{/,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.341 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@183400b2{/,null,STOPPED,@Spark}
2025-02-17 17:57:44.342 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b4b3885{/jobs,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.342 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@496e99eb{/jobs/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.344 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@327ebec5{/jobs/job,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.344 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cc6e858{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.344 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@320a28ef{/stages,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.345 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e49cabf{/stages/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.345 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7473763d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.346 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25ad1a15{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.346 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18a97e17{/stages/pool,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.346 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1041ed48{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.347 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15fb1217{/storage,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.347 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e20940f{/storage/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.347 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@591550bd{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.348 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62ecb8e8{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.348 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19a88dcc{/environment,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.348 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1024dabe{/environment/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.349 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a8750d0{/executors,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.349 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc52008{/executors/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.350 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fa4e2f1{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.350 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@179aade2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.350 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59c28f7e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.351 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56b4f69a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.356 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3074100b{/static,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.356 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38dc690{/,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33114d9b{/api,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.358 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef3a181{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.359 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12c77524{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.363 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@703e65d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:44.434 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-17 17:57:44.461 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-17 17:57:47.249 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-17 17:57:47.309 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-17 17:57:47.572 [http-nio-8080-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66a12f80{/SQL,null,AVAILABLE,@Spark}
2025-02-17 17:57:47.573 [http-nio-8080-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6711ac32{/SQL/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:47.573 [http-nio-8080-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f7d6608{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-17 17:57:47.574 [http-nio-8080-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35b0c4e8{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-17 17:57:47.575 [http-nio-8080-exec-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b15e936{/static/sql,null,AVAILABLE,@Spark}
2025-02-17 17:57:50.944 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 9
2025-02-17 17:57:51.404 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 空值数量: 9
2025-02-17 17:57:51.404 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-17 17:57:51.404 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 0
2025-02-17 17:57:51.529 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 0
2025-02-17 17:57:51.654 [http-nio-8080-exec-1] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 9
2025-02-17 17:58:01.869 [http-nio-8080-exec-1] ERROR s.service.impl.FileServiceImpl - 处理文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-18 09:20:05.537 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 09:20:05.540 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 09:20:05.541 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 09:20:05.541 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-18 09:20:05.541 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 09:20:05.541 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 09:20:05.552 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-18 09:20:05.953 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 09:20:05.989 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 09:20:06.214 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 09:20:06.934 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4495ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 09:20:07.026 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 09:20:07.046 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4606ms
2025-02-18 09:20:07.090 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@42e06200{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 09:20:07.110 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@692c3df0{/,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@692c3df0{/,null,STOPPED,@Spark}
2025-02-18 09:20:07.286 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7332dd3e{/jobs,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@363b2682{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32282349{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.288 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2018cedd{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e86e7a1{/stages,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.289 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@716b22ee{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.290 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47216247{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.290 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e0d0e3e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.291 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b1ef0c9{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.291 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b10740a{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ce7d4e0{/storage,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e444515{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@133e5913{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f2d1644{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@497994c9{/environment,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11addc64{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49f41cc1{/executors,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.295 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b734548{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.295 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ea8c656{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c8b2707{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dc0b0e9{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e04d0af{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@416d186d{/static,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74467f68{/,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ba56a74{/api,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.306 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b90141f{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.306 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e299fa{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.312 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35a89c96{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 09:20:07.392 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 09:20:07.430 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 10:32:46.897 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-18 10:32:46.990 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 10:32:47.434 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12355584{/SQL,null,AVAILABLE,@Spark}
2025-02-18 10:32:47.435 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328b5064{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 10:32:47.435 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55eaf3c2{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 10:32:47.436 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@625a43c9{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 10:32:47.438 [http-nio-8080-exec-3] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7eaa1b08{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 10:32:52.090 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 9
2025-02-18 10:32:52.641 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 空值数量: 9
2025-02-18 10:32:52.641 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-18 10:32:52.641 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 0
2025-02-18 10:32:52.773 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 0
2025-02-18 10:32:52.893 [http-nio-8080-exec-3] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 9
2025-02-18 10:33:03.117 [http-nio-8080-exec-3] ERROR s.service.impl.FileServiceImpl - 处理文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-18 10:33:56.894 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@42e06200{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 10:33:57.662 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 10:33:57.663 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 10:33:57.664 [restartedMain] ERROR sparkanalysis.util.HadoopConfigTest - 未找到winutils.exe: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-18 10:33:57.664 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 初始化Spark配置时发生错误
java.lang.RuntimeException: Hadoop环境验证失败
	at sparkanalysis.config.SparkConfig.init(SparkConfig.java:51)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:401)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:219)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1767)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:601)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-18 10:34:00.105 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 10:34:00.107 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 10:34:00.109 [restartedMain] ERROR sparkanalysis.util.HadoopConfigTest - 未找到winutils.exe: C:\Users\xubei\Desktop\本地版本可连接\spark\src\main\hadoop\bin\winutils.exe
2025-02-18 10:34:00.109 [restartedMain] ERROR sparkanalysis.config.SparkConfig - 初始化Spark配置时发生错误
java.lang.RuntimeException: Hadoop环境验证失败
	at sparkanalysis.config.SparkConfig.init(SparkConfig.java:51)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMethod.invoke(InitDestroyAnnotationBeanPostProcessor.java:457)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:401)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:219)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1767)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:601)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-18 10:54:07.160 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 10:54:07.162 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 10:54:07.163 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 10:54:07.163 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\实习demo\spark\src\main\hadoop
2025-02-18 10:54:07.163 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 10:54:07.164 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 10:54:07.174 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-18 10:54:07.560 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 10:54:07.593 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 10:54:07.807 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 10:54:08.493 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4237ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 10:54:08.577 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 10:54:08.595 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4340ms
2025-02-18 10:54:08.638 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@efc7bd3{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 10:54:08.655 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bc9a193{/,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@bc9a193{/,null,STOPPED,@Spark}
2025-02-18 10:54:08.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21de9581{/jobs,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.822 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58c07c03{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.822 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514af17e{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b7c237d{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a2fac63{/stages,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a2d31e6{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d729b11{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73a45ba4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@385d30a0{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b1a1329{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f346687{/storage,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e41da9a{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@703c74dc{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d0f13f3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53376373{/environment,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f7f1a83{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@441fc956{/executors,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79f62487{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@376c24c0{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35f039cc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76b06d3d{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68bd8a2a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.837 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d432ba3{/static,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.838 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@645a5d27{/,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.840 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15ffd315{/api,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.841 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54ed16cd{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.841 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15af0103{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.845 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1df1f4b{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:08.925 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 10:54:08.961 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 10:54:15.676 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 处理文件的完整路径: C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-18 10:54:15.757 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 10:54:16.177 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47e71817{/SQL,null,AVAILABLE,@Spark}
2025-02-18 10:54:16.178 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60e742d0{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:16.178 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12be22c8{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 10:54:16.179 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7b6ccf{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 10:54:16.180 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9c2c002{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 10:54:21.135 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 原始数据量为: 9
2025-02-18 10:54:21.638 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空值数量: 9
2025-02-18 10:54:21.638 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 空字符串数量: 0
2025-02-18 10:54:21.638 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 格式错误数量: 0
2025-02-18 10:54:21.768 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗后的数据量为: 0
2025-02-18 10:54:21.878 [http-nio-8080-exec-2] INFO  s.service.impl.FileServiceImpl - 清洗掉的数据样本: 9
2025-02-18 10:54:32.023 [http-nio-8080-exec-2] ERROR s.service.impl.FileServiceImpl - 处理文件 'Demo.csv' 时出错
org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
2025-02-18 10:55:09.407 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@efc7bd3{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 16:58:25.198 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 16:58:25.202 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 16:58:25.203 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 16:58:25.203 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 16:58:25.203 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 16:58:25.203 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 16:58:25.213 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 16:58:25.594 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 16:58:25.626 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 16:58:25.840 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 16:58:26.497 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4233ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 16:58:26.580 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 16:58:26.594 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4329ms
2025-02-18 16:58:26.637 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@568d132c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 16:58:26.654 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@212dfd52{/,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@212dfd52{/,null,STOPPED,@Spark}
2025-02-18 16:58:26.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e26a90d{/jobs,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11aad117{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56ae93bc{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@519021{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d9347ed{/stages,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d186ae9{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70a273be{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@250d3bee{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19e9e024{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c4646e5{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a87f0ea{/storage,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@223c82eb{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52e1ef7{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53d715d4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bd8e1d7{/environment,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72804bcb{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@646e2082{/executors,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@197b8e31{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@517d9b54{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57527a9a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d3dcb9{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d251cb{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.836 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c4788c1{/static,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.836 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21c58142{/,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.838 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1812bac9{/api,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.838 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710371f5{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.839 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@498b6dad{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.844 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76216d3f{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 16:58:26.923 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 16:58:26.961 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 16:58:27.081 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@568d132c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:01:23.172 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:01:23.174 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:01:23.175 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:01:23.175 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:01:23.175 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:01:23.175 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:01:23.185 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:01:23.508 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:01:23.532 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:01:23.701 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:01:24.299 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3965ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 17:01:24.367 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:01:24.379 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4046ms
2025-02-18 17:01:24.413 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@45773710{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:01:24.427 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54ba1712{/,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.575 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@54ba1712{/,null,STOPPED,@Spark}
2025-02-18 17:01:24.576 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd9a3b0{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.576 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@597143e8{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.577 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48ee5295{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.577 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4676fb81{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.578 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8744a0{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.578 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3190d80e{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.579 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@573b2199{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.580 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f02fd82{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.580 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51196791{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.580 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41cd2e1e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.581 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@198c5a2{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.581 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@795469f9{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.581 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dc78396{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.582 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e1e7be4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.582 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ae1ad96{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d61473a{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b435094{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6243fedf{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.584 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7132d558{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.584 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@189cb1f9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.585 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58512f6c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.585 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@605a2384{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.591 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6bca2f{/static,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.591 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d5ca0b3{/,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.593 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bc706c0{/api,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.593 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a212a10{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.594 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fb21e4f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.598 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c48938c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:01:24.676 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:01:24.708 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:01:27.479 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@45773710{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:03:13.005 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:03:13.007 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:03:13.008 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:03:13.008 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:03:13.008 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:03:13.008 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:03:13.018 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:03:13.349 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:03:13.373 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:03:13.539 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:03:14.128 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4076ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 17:03:14.199 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:03:14.213 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4160ms
2025-02-18 17:03:14.247 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@280865d8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:03:14.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d5988d5{/,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.407 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6d5988d5{/,null,STOPPED,@Spark}
2025-02-18 17:03:14.408 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f767e86{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.408 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18e013e1{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.410 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e77d4bb{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.410 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7473a529{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.410 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57662759{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.411 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a4c999f{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.411 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bdebb7d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.412 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12705b4a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.412 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13075bf6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.412 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74e00bb3{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.413 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10232e87{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.413 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5532110b{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.413 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7542d885{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.414 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48660cc3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.414 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33b3ef09{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.414 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@961e557{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@412f7b29{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79b8f93c{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ec04ac4{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c6b8d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c404a13{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19355991{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.422 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22a15821{/static,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.422 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@317adfcf{/,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.424 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31f1fecf{/api,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.425 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@141b3af3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.425 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a8b71e0{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.429 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e669631{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:03:14.504 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:03:14.537 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:17:47.311 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: 没有匹配的处理器:Demo.csv
	at sparkanalysis.service.processor.factory.FileProcessorFactory.lambda$getProcessor$1(FileProcessorFactory.java:22)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at sparkanalysis.service.processor.factory.FileProcessorFactory.getProcessor(FileProcessorFactory.java:22)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:40)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:18:18.369 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: 没有匹配的处理器:Demo.csv
	at sparkanalysis.service.processor.factory.FileProcessorFactory.lambda$getProcessor$1(FileProcessorFactory.java:22)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at sparkanalysis.service.processor.factory.FileProcessorFactory.getProcessor(FileProcessorFactory.java:22)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:40)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:20:03.931 [ForkJoinPool.commonPool-worker-2] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: 没有匹配的处理器:Demo.csv
	at sparkanalysis.service.processor.factory.FileProcessorFactory.lambda$getProcessor$1(FileProcessorFactory.java:22)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at sparkanalysis.service.processor.factory.FileProcessorFactory.getProcessor(FileProcessorFactory.java:22)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:40)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:21:05.149 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@280865d8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:21:10.694 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:21:10.694 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:21:10.695 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:21:10.695 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:21:10.695 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:21:10.696 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:21:10.699 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:21:10.700 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:21:10.700 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:21:10.701 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:21:10.757 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:21:10.758 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1080705ms
2025-02-18 17:21:10.771 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1c179b6d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:21:10.772 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f3fbf68{/,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.815 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5f3fbf68{/,null,STOPPED,@Spark}
2025-02-18 17:21:10.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e5572ba{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35ba598d{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca8adb{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@698f8482{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42eac9b{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74ded5a7{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15109046{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ba247f8{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55d4bae4{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ca1ffa5{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24f9ea86{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@db2cf4f{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61324833{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.821 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ee79cde{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.822 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6649195f{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.822 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c08b5b{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.822 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@103cfb6a{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.822 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9af4f9c{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cec5abf{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.823 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68934ff2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1daba202{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d01bfd6{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f6f9423{/static,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cf4505d{/,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6465546a{/api,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c21e60e{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9e51a55{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.828 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@204206b{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:21:10.830 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:21:10.836 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:23:19.321 [Thread-26] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1c179b6d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:23:19.738 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:23:19.739 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:23:19.739 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:23:19.739 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:23:19.739 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:23:19.741 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:23:19.744 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:23:19.744 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:23:19.745 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:23:19.746 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:23:19.794 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:23:19.795 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1209742ms
2025-02-18 17:23:19.812 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@55576504{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:23:19.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74aec49e{/,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@74aec49e{/,null,STOPPED,@Spark}
2025-02-18 17:23:19.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b6edc0{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ed569f6{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.878 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b1d00d5{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.878 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@278c611f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.879 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f8e3289{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.879 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6025ce56{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76dcbc30{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32907c29{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f46af23{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.881 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@86deb29{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.881 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b34d089{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.881 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58d68925{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.882 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ed090ea{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.882 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@357b822b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.882 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1161b987{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.882 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64ca792a{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.884 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42932586{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.884 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@366e55c6{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.884 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2374e4fc{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.885 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3587f170{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.886 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bb10b80{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.886 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a88ed57{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.886 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14913b0b{/static,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.887 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d66066d{/,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.887 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199f52a8{/api,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.888 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@562e4b03{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.888 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2337735e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.889 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@424e5ab9{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:19.891 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:23:19.898 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:23:24.617 [Thread-44] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@55576504{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:23:25.009 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:23:25.011 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:23:25.012 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:23:25.012 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:23:25.012 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:23:25.012 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:23:25.015 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:23:25.016 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:23:25.017 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:23:25.017 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:23:25.062 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:23:25.063 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1215010ms
2025-02-18 17:23:25.075 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7787f196{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:23:25.076 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@192a082f{/,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.121 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@192a082f{/,null,STOPPED,@Spark}
2025-02-18 17:23:25.122 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b0cd88d{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.122 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c38ab66{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.122 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5babdbbf{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f2c5b8d{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69685f59{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37edd25f{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.123 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28c1d772{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.124 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b683eeb{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.124 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@389f3ff2{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.124 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53fb8b47{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.126 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1417f6b5{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.126 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21a28124{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.126 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e630edb{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.127 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@110baf8f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.127 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75c44248{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.127 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ef0b4b{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.127 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58ff7fcb{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49ab1e03{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10ded001{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37b57c81{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52178af{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.128 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4666ee0d{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64294cf9{/static,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1164da62{/,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48982037{/api,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@243bea40{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fe1b905{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10f0b837{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:23:25.132 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:23:25.138 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:24:02.113 [Thread-62] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7787f196{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:24:02.487 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:24:02.488 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:24:02.489 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:24:02.489 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:24:02.489 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:24:02.489 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:24:02.493 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:24:02.493 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:24:02.494 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:24:02.494 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:24:02.540 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:24:02.540 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1252488ms
2025-02-18 17:24:02.554 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2f2fd209{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:24:02.555 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d7846de{/,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.598 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5d7846de{/,null,STOPPED,@Spark}
2025-02-18 17:24:02.598 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e0d11a1{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b49fcbf{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30641c6b{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@348450cc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76bb76b2{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67773ff9{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49b1c8e0{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e740b3e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cc3449c{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2120e7cf{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f253090{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fdfb7c6{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7312a11d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11cb7c1c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f6e536e{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.602 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22619fa3{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.602 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@268a0117{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.602 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61f88395{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.602 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ac23e54{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.602 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4586398{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.603 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53ea6bd7{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.603 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78dcc8ef{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ebf4260{/static,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56e2bf0b{/,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7543f97f{/api,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b1a154c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.605 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@212c220b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.606 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38f4ea5{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:24:02.607 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:24:02.612 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:32:45.306 [Thread-80] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2f2fd209{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:32:45.699 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:32:45.699 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:32:45.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:32:45.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:32:45.700 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:32:45.701 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:32:45.703 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:32:45.704 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:32:45.704 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:32:45.705 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:32:45.751 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:32:45.752 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1775699ms
2025-02-18 17:32:45.764 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@56055be2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:32:45.765 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15260924{/,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@15260924{/,null,STOPPED,@Spark}
2025-02-18 17:32:45.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bcb8e37{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d349f40{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52c53e03{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b878bc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a590174{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fefee72{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14410be2{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530b64e0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11017879{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20339b01{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c9ff55a{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67d52f7{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f5cd752{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f2b427{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b7897d4{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@671629e7{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2307526a{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18751cb5{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28af3931{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.812 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4052590{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.812 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ff5e35{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.812 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@407a633a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f3edf20{/static,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@263f9535{/,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.813 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27c3186{/api,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.814 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d90d5f9{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.814 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53e3901e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.814 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d5b724d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:32:45.816 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:32:45.820 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 17:32:45.822 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:32:53.593 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@56055be2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:33:01.224 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:33:01.227 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:33:01.227 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:33:01.228 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:33:01.228 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:33:01.228 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:33:01.238 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:33:01.627 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:33:01.662 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:33:01.874 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:33:02.537 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4246ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 17:33:02.623 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:33:02.640 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4350ms
2025-02-18 17:33:02.686 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4d0c73ac{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:33:02.704 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f2d4a23{/,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.872 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5f2d4a23{/,null,STOPPED,@Spark}
2025-02-18 17:33:02.873 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b237e9{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.873 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16157657{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f24cf7d{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d42bf0c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.875 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23180c38{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.876 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9c34c39{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.876 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e4298a7{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.877 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2447098c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.877 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d035e7{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.878 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a3c0957{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.878 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29e5e9b9{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.879 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eb47640{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.879 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@718a33b4{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.879 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1da1c3c4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f463b85{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@192a6354{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.880 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56f6708{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.881 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7459c273{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.881 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e700b0a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.881 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1335bb43{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.882 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26d82a50{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.882 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cbaefca{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.890 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2193e64a{/static,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.890 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45113e0a{/,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.892 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6569b771{/api,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.892 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79554390{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.893 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@771f0553{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.897 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61736b8d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:02.974 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:33:03.011 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 17:33:03.014 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:33:15.039 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 GenericCsvProcessor 支持 处理文件 Demo.csv
2025-02-18 17:33:15.107 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 17:33:15.561 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@344ba0f1{/SQL,null,AVAILABLE,@Spark}
2025-02-18 17:33:15.562 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24f3df40{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:15.563 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75ba7a8c{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 17:33:15.563 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@625d6dbc{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 17:33:15.564 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4014eb03{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 17:33:16.357 [ForkJoinPool.commonPool-worker-1] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.processorImp.GenericCsvProcessor.process(GenericCsvProcessor.java:24)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:41)
	at sparkanalysis.controller.FileController.lambda$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:33:21.331 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:36:24.564 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4d0c73ac{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:37:03.000 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:37:03.002 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:37:03.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:37:03.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:37:03.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:37:03.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:37:03.014 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:37:03.362 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:37:03.390 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:37:03.582 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:37:04.349 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4627ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 17:37:04.434 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:37:04.449 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4727ms
2025-02-18 17:37:04.490 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@60f97d28{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:37:04.506 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33bf7a85{/,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.690 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@33bf7a85{/,null,STOPPED,@Spark}
2025-02-18 17:37:04.691 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14636e0b{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.692 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cdee327{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.692 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3408d865{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.693 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a3e6f25{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.694 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@512123ce{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.695 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20ad4d63{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.696 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21ce4cd6{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.696 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ffd131f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.697 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@762b7577{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.698 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dbe9ae4{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.698 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c8a13f{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.698 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7724b4c8{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@308aa5e3{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.700 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f71a64c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a063e33{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.702 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b2bf5fb{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.703 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ea33630{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.704 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7059752e{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.705 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2945ae42{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.706 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@287e4c0e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.707 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22eba777{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.707 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46fede6f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e3c2d10{/static,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.714 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8448e51{/,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.716 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@700b74bb{/api,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3da3fcd7{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65d661d5{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.721 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f4c40f5{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:04.810 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:37:04.846 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 17:37:04.850 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:37:07.144 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 GenericCsvProcessor 支持 处理文件 Demo.csv
2025-02-18 17:37:07.198 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 17:37:07.528 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff322f{/SQL,null,AVAILABLE,@Spark}
2025-02-18 17:37:07.528 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@219843e8{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:07.529 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71a437f{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 17:37:07.529 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f04875d{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 17:37:07.530 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ffd153e{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 17:37:08.114 [ForkJoinPool.commonPool-worker-1] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.processorImp.GenericCsvProcessor.process(GenericCsvProcessor.java:24)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:41)
	at sparkanalysis.controller.FileController.lambda$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:37:13.084 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:46:13.438 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@60f97d28{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:46:18.788 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:46:18.790 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:46:18.791 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:46:18.791 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:46:18.791 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:46:18.791 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:46:18.800 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:46:19.094 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:46:19.116 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:46:19.270 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:46:19.835 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3757ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 17:46:19.901 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:46:19.913 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3835ms
2025-02-18 17:46:19.945 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@9bd844f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:46:19.959 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f629e94{/,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.099 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7f629e94{/,null,STOPPED,@Spark}
2025-02-18 17:46:20.100 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49cf6c60{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.100 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@364b15c5{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.101 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@238a5a81{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.101 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67b9af96{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.102 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@570837ec{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.102 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@463664a2{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.103 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b798a5{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.103 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44c7dc57{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.104 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40cb9965{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.104 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b4f9f0f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.105 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13b5d0b3{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.105 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13526e01{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.105 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a364385{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f20b205{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.106 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@96b2c5c{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b270cb8{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51c11001{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.107 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a35f8a7{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.108 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@445fc937{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.108 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b9b4840{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@704dd38c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.109 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c09c59e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@758c2fc3{/static,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.115 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e24536f{/,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.116 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79177274{/api,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.117 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b30dcf{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.117 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@414c950d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.120 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fe1b73e{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:20.191 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:46:20.218 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 17:46:20.221 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:46:23.705 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 GenericCsvProcessor 支持 处理文件 Demo.csv
2025-02-18 17:46:23.758 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 17:46:24.038 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39cc1aac{/SQL,null,AVAILABLE,@Spark}
2025-02-18 17:46:24.038 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2585fafa{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:24.039 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79b7c2d5{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 17:46:24.039 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62ac99c{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 17:46:24.040 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67bc6adf{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 17:46:24.605 [ForkJoinPool.commonPool-worker-1] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.processorImp.GenericCsvProcessor.process(GenericCsvProcessor.java:24)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:41)
	at sparkanalysis.controller.FileController.lambda$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:46:29.576 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:48:48.817 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@9bd844f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:48:52.431 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:48:52.433 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:48:52.433 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:48:52.434 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:48:52.434 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:48:52.434 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:48:52.442 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:48:52.751 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:48:52.774 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:48:52.924 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:48:53.481 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3725ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 17:48:53.545 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:48:53.557 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3800ms
2025-02-18 17:48:53.588 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@14d5d4bc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:48:53.602 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a56ae94{/,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.734 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7a56ae94{/,null,STOPPED,@Spark}
2025-02-18 17:48:53.735 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51209951{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.735 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4aeafcef{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.736 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@156861c8{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.736 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f42f80e{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.737 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26881a3d{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.737 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44911c06{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.737 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73dc8677{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.739 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558a1e2b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.739 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53573710{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.739 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@290fc621{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e8ceb8{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c25c38f{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c379439{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.741 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@541a6d32{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.741 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f406ba{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.741 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8f6d9dc{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.741 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e038a15{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dadf378{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773479{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d23e4ee{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54231229{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7610d9c8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d1f58af{/static,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.751 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@555f7f43{/,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.752 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57daa6d6{/api,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.752 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c3f4cc6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.752 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72d8700f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.755 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57234b8e{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:53.826 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:48:53.852 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 17:48:53.854 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:48:56.306 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 GenericCsvProcessor 支持 处理文件 Demo.csv
2025-02-18 17:48:56.359 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 17:48:56.635 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2264791f{/SQL,null,AVAILABLE,@Spark}
2025-02-18 17:48:56.635 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@351c444a{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:56.637 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2516f276{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 17:48:56.637 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e71b318{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 17:48:56.638 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6be26dce{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 17:48:57.180 [ForkJoinPool.commonPool-worker-1] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.processorImp.GenericCsvProcessor.process(GenericCsvProcessor.java:24)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:41)
	at sparkanalysis.controller.FileController.lambda$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:49:02.149 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 17:49:34.173 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@14d5d4bc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:49:39.885 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:49:39.886 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:49:39.886 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:49:39.886 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:49:39.886 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:49:39.887 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:49:39.890 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:49:39.890 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:49:39.891 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:49:39.892 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:49:39.940 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:49:39.941 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @50185ms
2025-02-18 17:49:39.956 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@ed4529d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:49:39.957 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ef082cf{/,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.013 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7ef082cf{/,null,STOPPED,@Spark}
2025-02-18 17:49:40.014 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@244488ea{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.014 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5df60596{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.015 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ac6cdd6{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.015 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b668be8{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.015 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35a8d5c3{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.016 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63766b2a{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.016 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79dcf2a9{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.016 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51bf13a6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.017 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c452e0a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.017 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24b32c50{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.017 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f18e5{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.018 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74349bc5{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.018 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53f85cca{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.018 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dec4b04{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.019 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d2143a5{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.019 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f8fc065{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.019 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@363311ff{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@712627a{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a2b3189{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@404eb53d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cecef0f{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@458a80fb{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.022 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3053f261{/static,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.022 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1002f95f{/,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.023 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7359c624{/api,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.023 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ffa1df5{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.023 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ed4279d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.024 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a0d50a9{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:40.026 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:49:40.031 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 17:49:40.034 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:49:45.801 [Thread-60] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@ed4529d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:49:46.196 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 17:49:46.197 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 17:49:46.199 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 17:49:46.199 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 17:49:46.199 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 17:49:46.199 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 17:49:46.203 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 17:49:46.203 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:49:46.205 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 17:49:46.206 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 17:49:46.252 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 17:49:46.252 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @56496ms
2025-02-18 17:49:46.263 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4a2af917{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 17:49:46.264 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c019f37{/,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.306 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6c019f37{/,null,STOPPED,@Spark}
2025-02-18 17:49:46.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a8b16cc{/jobs,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2791d{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bcf58c2{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16986a8d{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5efb1a70{/stages,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8a672e{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.307 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ca26e30{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@274eacb0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54be49e7{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@356a71dc{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b4d17e6{/storage,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.308 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61755174{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7be1eddc{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e3bb85f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1381c2e6{/environment,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70eeef7d{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@584c8d98{/executors,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.309 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e8b4940{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1823a436{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@be8bfdf{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54610ba5{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cc3647f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.311 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@419963ba{/static,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.311 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4df39703{/,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.311 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11ae5824{/api,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.311 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55e27df3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.311 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c6fa022{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.311 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@322b32e8{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 17:49:46.314 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 17:49:46.318 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 17:49:46.320 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 17:59:27.813 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4a2af917{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:36:53.361 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:36:53.363 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:36:53.370 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:36:53.370 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:36:53.370 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:36:53.370 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:36:53.380 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:36:53.707 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:36:53.730 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:36:53.880 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:36:54.382 [restartedMain] WARN  o.a.hadoop.util.ShutdownHookManager - Failed to add the ShutdownHook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:216)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:86)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:180)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.storage.DiskBlockManager.addShutdownHook(DiskBlockManager.scala:344)
	at org.apache.spark.storage.DiskBlockManager.<init>(DiskBlockManager.scala:79)
	at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:199)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:381)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:483)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)
	at sparkanalysis.config.SparkConfig.sparkSession(SparkConfig.java:162)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.CGLIB$sparkSession$1(<generated>)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:331)
	at sparkanalysis.config.SparkConfig$$SpringCGLIB$$0.sparkSession(<generated>)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:140)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:651)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:489)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1166)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:769)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:752)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:145)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:493)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1441)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1348)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:911)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:789)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:241)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1356)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1193)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:563)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:523)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:325)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:323)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:973)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:946)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:616)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:753)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:455)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:323)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at sparkanalysis.SparkApplication.main(SparkApplication.java:9)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-02-18 22:36:54.444 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3825ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 22:36:54.508 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:36:54.520 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3902ms
2025-02-18 22:36:54.553 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7aa91576{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:36:54.566 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52fd569b{/,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.725 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@52fd569b{/,null,STOPPED,@Spark}
2025-02-18 22:36:54.727 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d0fcdfc{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.728 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9d216b0{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.729 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18696406{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.730 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61d7e2cf{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.730 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fcfd23a{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.731 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@295cc871{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.732 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fbe3dc{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.733 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48def9e6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.734 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71430172{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.734 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56959bb9{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.735 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7392c37e{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.736 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a5d469c{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.736 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dced004{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.736 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3709d187{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.737 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@370f27d7{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.737 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b63fc0{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.738 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f3ecd52{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.738 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@356b5c3b{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.739 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b49f33{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.739 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d6d71de{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c46bb9c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.740 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6be32f24{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.746 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@330b54bb{/static,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.748 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66124564{/,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232a1d43{/api,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.750 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@401eebb0{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.751 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13a952fa{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.755 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76b62101{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:36:54.839 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:36:54.877 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:36:54.882 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:36:55.284 [SpringApplicationShutdownHook] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7aa91576{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:37:18.317 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:37:18.320 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:37:18.320 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:37:18.320 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:37:18.322 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:37:18.322 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:37:18.336 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:37:18.746 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:37:18.783 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:37:18.976 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:37:19.663 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4842ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 22:37:19.739 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:37:19.753 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4931ms
2025-02-18 22:37:19.791 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@111117ba{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:37:19.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f7ee6f2{/,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.978 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4f7ee6f2{/,null,STOPPED,@Spark}
2025-02-18 22:37:19.979 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61cbc0a1{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.979 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cb7fd31{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.980 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d8cad3e{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.981 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a601e81{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.981 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d38d42{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.982 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a7d114f{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.983 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44af1d8c{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.983 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a26431b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.984 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33f5c0f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.984 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58630223{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.986 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d1143c1{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.986 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20e6e98c{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.987 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e337607{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.987 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3df30157{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.988 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68e3f3c7{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.988 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16923128{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.989 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55d030dc{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.989 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12ffae33{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30c5a2b9{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54fa38f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14ec4ff2{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76144673{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.998 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6765757{/static,null,AVAILABLE,@Spark}
2025-02-18 22:37:19.999 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72f9c01a{/,null,AVAILABLE,@Spark}
2025-02-18 22:37:20.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a6ab9b9{/api,null,AVAILABLE,@Spark}
2025-02-18 22:37:20.001 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c40eacd{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:37:20.002 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f2b025d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:37:20.006 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c8e8444{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:20.106 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:37:20.149 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:37:20.154 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:37:54.812 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 GenericCsvProcessor 支持 处理文件 Demo.csv
2025-02-18 22:37:54.886 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 22:37:55.288 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@abb3b42{/SQL,null,AVAILABLE,@Spark}
2025-02-18 22:37:55.289 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e1c55af{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:55.289 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@91376c2{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 22:37:55.290 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25377b3e{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 22:37:55.291 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a411c97{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 22:37:56.039 [ForkJoinPool.commonPool-worker-1] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.processorImp.GenericCsvProcessor.process(GenericCsvProcessor.java:24)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:41)
	at sparkanalysis.controller.FileController.lambda$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:38:02.513 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:39:46.882 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@111117ba{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:39:47.301 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:39:47.302 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:39:47.302 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:39:47.302 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:39:47.302 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:39:47.303 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:39:47.305 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:39:47.306 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:39:47.306 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:39:47.307 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:39:47.349 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:39:47.349 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @152528ms
2025-02-18 22:39:47.360 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2d333731{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:39:47.361 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76a74c0{/,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.414 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@76a74c0{/,null,STOPPED,@Spark}
2025-02-18 22:39:47.414 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@217edb91{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.414 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@509435cf{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.414 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35c52346{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e8be05b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26eab896{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c18c7a1{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.415 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e019952{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a4bbeef{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50c53853{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.416 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@654c3ac7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2989ad72{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@654df050{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d0397b{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1999f3b9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.417 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bba63f0{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cb2b7e5{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc1d889{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c92834b{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2eeeb77c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.420 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73f1d137{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.420 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e3e9b0{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.420 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28ce7c57{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.420 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@343653f{/static,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.421 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56cc9da4{/,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.421 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35bc81df{/api,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.421 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f687550{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.421 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16d62d5a{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.423 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f4ba487{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:39:47.424 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:39:47.428 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:39:47.432 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:40:00.665 [ForkJoinPool.commonPool-worker-2] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 GenericCsvProcessor 支持 处理文件 Demo.csv
2025-02-18 22:40:00.666 [ForkJoinPool.commonPool-worker-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 22:40:00.668 [ForkJoinPool.commonPool-worker-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4791dcba{/SQL,null,AVAILABLE,@Spark}
2025-02-18 22:40:00.668 [ForkJoinPool.commonPool-worker-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c85eeac{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:00.670 [ForkJoinPool.commonPool-worker-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71803d56{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 22:40:00.670 [ForkJoinPool.commonPool-worker-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43ca9978{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:00.671 [ForkJoinPool.commonPool-worker-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f66e4e{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 22:40:00.676 [ForkJoinPool.commonPool-worker-2] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.processorImp.GenericCsvProcessor.process(GenericCsvProcessor.java:24)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:41)
	at sparkanalysis.controller.FileController.lambda$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:40:00.679 [ForkJoinPool.commonPool-worker-2] ERROR s.service.impl.FileServiceImpl - 文件处理失败在processFile中:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:40:01.448 [Thread-24] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2d333731{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:40:01.907 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:40:01.908 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:40:01.909 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:40:01.909 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:40:01.909 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:40:01.910 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:40:01.917 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:40:01.918 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:40:01.920 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:40:01.921 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:40:02.017 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:40:02.018 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @167197ms
2025-02-18 22:40:02.033 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@31d4837b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:40:02.033 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ae648dd{/,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5ae648dd{/,null,STOPPED,@Spark}
2025-02-18 22:40:02.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39e8221e{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78969d21{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4183abda{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7652af14{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f448987{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ccbfeda{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d7d7235{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2709538d{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11f3d03b{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6285c11f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@557824a7{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7dd25d14{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1940cc05{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58501acb{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@647cbe4{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47db1c5c{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.088 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@263bb19d{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8f05dd6{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e9f380{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e5c6c29{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b67192{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.090 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f0203cf{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.090 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b52fa79{/static,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.090 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46f513d7{/,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.090 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2940cb86{/api,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.090 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63c37a70{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.092 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c2841ee{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.092 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c8ef585{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:40:02.094 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:40:02.100 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:40:02.102 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:43:24.030 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@31d4837b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:43:30.557 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:43:30.560 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:43:30.560 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:43:30.561 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:43:30.561 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:43:30.561 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:43:30.571 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:43:30.904 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:43:30.928 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:43:31.089 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:43:31.720 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4180ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 22:43:31.789 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:43:31.802 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4262ms
2025-02-18 22:43:31.837 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@46337d6b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:43:31.851 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62cccbc8{/,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.017 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@62cccbc8{/,null,STOPPED,@Spark}
2025-02-18 22:43:32.019 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69329930{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.019 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d019bd9{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26a73feb{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd9a3b0{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@597143e8{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.021 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6eb55604{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.021 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8744a0{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.022 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3190d80e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.022 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f296082{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.022 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24523a63{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.023 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@573b2199{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.023 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f02fd82{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.024 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51196791{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.025 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41cd2e1e{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.025 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@198c5a2{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.026 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@795469f9{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.026 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dc78396{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e1e7be4{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ae1ad96{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.028 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d61473a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.029 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b435094{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.029 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6243fedf{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.035 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7132d558{/static,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.036 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17a9a3cf{/,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.038 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6029c799{/api,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.039 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26050782{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.040 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@625b792d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.044 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19eb77e0{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:43:32.124 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:43:32.153 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:43:32.156 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:45:50.419 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 GenericCsvProcessor 支持 处理文件 Demo.csv
2025-02-18 22:45:50.472 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 22:45:50.741 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323924a6{/SQL,null,AVAILABLE,@Spark}
2025-02-18 22:45:50.742 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fbf8a47{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 22:45:50.742 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a061925{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 22:45:50.743 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20746ed1{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 22:45:50.744 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71d1904e{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 22:45:51.302 [ForkJoinPool.commonPool-worker-1] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.processorImp.GenericCsvProcessor.process(GenericCsvProcessor.java:24)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:41)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:45:56.264 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败在processFile中:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:50:03.331 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@46337d6b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:50:03.817 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:50:03.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:50:03.818 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:50:03.819 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:50:03.819 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:50:03.820 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:50:03.824 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:50:03.825 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:50:03.825 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:50:03.826 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:50:03.877 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:50:03.878 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @396338ms
2025-02-18 22:50:03.891 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1f4eb8cc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:50:03.892 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a3b4a15{/,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.935 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5a3b4a15{/,null,STOPPED,@Spark}
2025-02-18 22:50:03.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@274928d0{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e4377e{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.937 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4445862f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.938 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27da1b8f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.938 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ffcb3d2{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.939 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d5b653{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.939 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b686d7d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.940 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f533062{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.940 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64e16651{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.940 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@336dea88{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27a9de61{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66091c02{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@689ec517{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1828e4da{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d8a2235{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cc209de{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@241a30ea{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72e9dcea{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@765f222b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19e7b97a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bd2b425{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fe471b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53fbf71{/static,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63de03cf{/,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3204cd74{/api,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72ae2f7e{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2030c678{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24eadc38{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:50:03.948 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:50:03.954 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:50:03.958 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:51:57.319 [Thread-24] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1f4eb8cc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:51:57.705 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:51:57.706 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:51:57.706 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:51:57.706 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:51:57.706 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:51:57.706 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:51:57.710 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:51:57.711 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:51:57.711 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:51:57.712 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:51:57.753 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:51:57.754 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @510214ms
2025-02-18 22:51:57.766 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7b77faba{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:51:57.767 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e6f42b0{/,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.803 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1e6f42b0{/,null,STOPPED,@Spark}
2025-02-18 22:51:57.803 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c86b0d4{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.803 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ba9d49e{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@335a962f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4679816{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45935ea0{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a32403{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3624ae97{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@720f1ad8{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@640e40f0{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1173ab02{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@187aa000{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.806 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab8438a{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6208377e{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@529c4c4b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2122fcee{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c45a990{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.807 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cd2803f{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.808 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b35b135{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.808 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@473b6b65{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.808 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27bc6139{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.808 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4217fce2{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5136cd05{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cf08607{/static,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.809 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60bc5d0f{/,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7515e888{/api,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6efed038{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.810 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b8bc677{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.811 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@efb4ee{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:57.813 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:51:57.817 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:51:57.820 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:51:59.440 [Thread-42] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7b77faba{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:51:59.815 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:51:59.815 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:51:59.816 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:51:59.816 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:51:59.816 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:51:59.816 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:51:59.820 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:51:59.821 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:51:59.823 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:51:59.823 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:51:59.875 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:51:59.876 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @512336ms
2025-02-18 22:51:59.888 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@146cf7ec{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:51:59.889 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18a1c275{/,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.926 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@18a1c275{/,null,STOPPED,@Spark}
2025-02-18 22:51:59.927 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16fd680c{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.927 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46fbaaf{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.927 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e723ab9{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.928 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c3f387b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.928 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44bcf4d5{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.928 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@269b4584{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.928 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1befe510{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.928 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@165c3d38{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.928 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f0040cd{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.929 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d94ab03{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.929 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2446a9be{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.929 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@410da12b{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.929 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65fcd7e1{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.929 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ed77946{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.930 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d6d8c36{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.930 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16d2d2e8{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.930 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@118ac410{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.930 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@566601e5{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.930 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e83fbec{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.930 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@577ee{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.932 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@285b291a{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.932 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3428644f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.932 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b38728c{/static,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.933 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4dff476b{/,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.933 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bb6d823{/api,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.933 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24489d54{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4524e193{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5819577d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:51:59.936 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:51:59.941 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:51:59.944 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:52:17.283 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@146cf7ec{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:52:23.657 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:52:23.659 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:52:23.661 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:52:23.661 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:52:23.661 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:52:23.661 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:52:23.669 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:52:24.049 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:52:24.081 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:52:24.266 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:52:24.873 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3982ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 22:52:24.951 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:52:24.965 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4073ms
2025-02-18 22:52:25.005 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2397b5d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:52:25.020 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3982c0dd{/,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3982c0dd{/,null,STOPPED,@Spark}
2025-02-18 22:52:25.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42331396{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51fa9b7{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8082e0c{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a35435f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50d8d55b{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c39d189{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15baf652{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e748b48{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19929498{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78a4b018{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69555ec3{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4482d976{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@129d74fe{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c9406f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@411075d9{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a57a326{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e3b862d{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e491ca3{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c60732b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55348bcd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6041b776{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33ae1d6e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.184 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6afe818f{/static,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.185 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a97502c{/,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.187 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f2d00e2{/api,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.188 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68b36baa{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.189 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c47e5d5{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.193 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71bd8f24{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:25.266 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:52:25.296 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:52:25.299 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:52:28.911 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-18 22:52:28.913 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-18 22:52:28.980 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 22:52:29.373 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36ab7383{/SQL,null,AVAILABLE,@Spark}
2025-02-18 22:52:29.373 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e012e3f{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:29.374 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fe64843{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 22:52:29.374 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@178f14e0{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 22:52:29.375 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b762249{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 22:52:30.103 [ForkJoinPool.commonPool-worker-1] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv.
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)
	at sparkanalysis.service.processor.impl.CsvFileProcessor.process(CsvFileProcessor.java:22)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:56)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:36)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:52:35.069 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.IllegalArgumentException: Pathname /C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv from hdfs://192.168.2.243:30070/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:257)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-18 22:58:11.620 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2397b5d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:58:12.065 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:58:12.066 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:58:12.068 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:58:12.068 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:58:12.068 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:58:12.068 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:58:12.071 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:58:12.073 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:58:12.073 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:58:12.074 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:58:12.121 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:58:12.121 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @351229ms
2025-02-18 22:58:12.134 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1b7ebaff{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:58:12.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282891fe{/,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@282891fe{/,null,STOPPED,@Spark}
2025-02-18 22:58:12.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21803830{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f837369{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31d49366{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e6c7dcc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bfa7d13{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f365b16{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@134bab85{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56636c31{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77aeca79{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a5e8f13{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@189c8c1f{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a80ede6{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.178 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c78b42a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63d3e1ab{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bd8a2da{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2779d440{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fddb55e{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.179 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40734040{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b2c7dc6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b27466{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.181 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9a56352{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a3d48a0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26a6af89{/static,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.182 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55e79d29{/,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.183 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dd3060c{/api,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.183 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63c8c150{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.183 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c6b5977{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.183 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8b12e7{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:12.185 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:58:12.191 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:58:12.194 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 22:58:31.475 [Thread-24] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1b7ebaff{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:58:31.829 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 22:58:31.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 22:58:31.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 22:58:31.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 22:58:31.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 22:58:31.830 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 22:58:31.833 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 22:58:31.834 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:58:31.834 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 22:58:31.835 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 22:58:31.881 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 22:58:31.881 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @370989ms
2025-02-18 22:58:31.893 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@583f8ac2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 22:58:31.894 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46223313{/,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@46223313{/,null,STOPPED,@Spark}
2025-02-18 22:58:31.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34ef25d5{/jobs,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@112fc706{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.934 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29aa10d7{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a779190{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d2b132{/stages,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d60c4d1{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.936 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589f156b{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.937 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@164f004{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.937 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a5ca5e3{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.937 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@92c75c6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.937 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab86bac{/storage,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.938 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3dc452ce{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.938 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5426a34f{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.938 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78095b38{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.938 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56e3cb85{/environment,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.939 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22bab20a{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.939 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ce28545{/executors,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.939 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c6d398e{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.939 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e6fa0c4{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.939 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6717e96a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.940 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cb552fd{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.940 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef741b8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.940 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4adc30c2{/static,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@354f42b3{/,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67b7486b{/api,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d95c767{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.941 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74a1d274{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61e3969a{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 22:58:31.944 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 22:58:31.950 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 22:58:31.952 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:00:03.678 [Thread-42] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@583f8ac2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:00:04.020 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:00:04.021 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:00:04.021 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:00:04.021 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:00:04.021 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:00:04.022 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:00:04.024 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:00:04.024 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:00:04.025 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:00:04.025 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:00:04.064 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:00:04.065 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @463172ms
2025-02-18 23:00:04.075 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@38e67f4d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:00:04.075 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6858b6ba{/,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6858b6ba{/,null,STOPPED,@Spark}
2025-02-18 23:00:04.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@308697b1{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27433732{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.113 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7096dbf3{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@629153bc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f824886{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ea8ca6{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d5700c0{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.114 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@706d3620{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.115 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20b170bc{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.115 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@364051e1{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.115 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11181ab0{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.115 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2714ffd7{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.115 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7146b811{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.116 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1973d4e3{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.116 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a459477{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.116 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f0f27a1{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.116 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e430396{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.116 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31b6cdad{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.117 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31ac669b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.117 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17e566ac{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.117 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e936882{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.117 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@acbcb9f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.118 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@98fc32c{/static,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.118 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53ba0dbc{/,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.118 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33fb76ef{/api,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.118 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c803b35{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.119 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45df3909{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.119 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c6cab27{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:04.121 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:00:04.125 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:00:04.127 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:00:20.228 [Thread-60] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@38e67f4d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:00:20.566 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:00:20.566 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:00:20.567 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:00:20.567 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:00:20.567 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:00:20.567 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:00:20.569 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:00:20.570 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:00:20.570 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:00:20.571 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:00:20.609 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:00:20.610 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @479718ms
2025-02-18 23:00:20.622 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@28743090{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:00:20.623 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41bc03d4{/,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.657 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@41bc03d4{/,null,STOPPED,@Spark}
2025-02-18 23:00:20.658 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@653a053c{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.658 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19491ee7{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.658 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ad1758f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.658 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@101ef542{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.658 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1283b3c0{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.659 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f462a85{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.659 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d7fe6f7{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.659 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10840c4c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.659 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a1d5c28{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.659 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64f5f288{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.659 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26c01cf1{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2217e61a{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25cbb9ce{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@380c45ea{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13be855c{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ee544f7{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.660 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48858513{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.661 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b5c8544{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.661 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23710fc3{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.661 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66dde2f2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.661 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ffa8c2b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.661 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e045c2f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3589a49d{/static,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21adeb80{/,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38e50c37{/api,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d236a06{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.662 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d6afed3{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.663 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a45373d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:00:20.665 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:00:20.668 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:00:20.670 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:01:58.785 [Thread-78] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@28743090{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:01:59.110 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:01:59.111 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:01:59.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:01:59.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:01:59.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:01:59.112 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:01:59.115 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:01:59.115 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:01:59.116 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:01:59.116 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:01:59.154 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:01:59.155 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @578263ms
2025-02-18 23:01:59.166 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@439f2065{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:01:59.166 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e885cf5{/,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.204 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7e885cf5{/,null,STOPPED,@Spark}
2025-02-18 23:01:59.204 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b704551{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.204 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4475bfcb{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.204 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b3ef65{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.204 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@220c422{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27641266{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d76b4b1{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@457c9db2{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f25e838{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ebc814d{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59cb40c6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c743a8d{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e4b4cb9{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7694c4c9{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.205 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dc7205{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@322e5902{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23c0421b{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@627362f7{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@793536ed{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cbefadd{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27e8812d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33e56d3f{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75634bb9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.208 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@150e220c{/static,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.208 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66af3940{/,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.208 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ec3268b{/api,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.208 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d34b352{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.208 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b3af166{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.208 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@402c819{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:01:59.209 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:01:59.213 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:01:59.216 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:04:01.263 [Thread-96] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@439f2065{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:04:01.650 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:04:01.650 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:04:01.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:04:01.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:04:01.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:04:01.651 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:04:01.654 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:04:01.655 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:04:01.655 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:04:01.655 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:04:01.702 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:04:01.702 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @700810ms
2025-02-18 23:04:01.717 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@720e0dd7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:04:01.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ab0772b{/,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@ab0772b{/,null,STOPPED,@Spark}
2025-02-18 23:04:01.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28b9ca72{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60a73ea3{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72d2dae1{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@224c6194{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21d64ef2{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@230081f6{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4977742c{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45335dd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4427ebb{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62d9e696{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@364ec178{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2acd5fb0{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3dd349e6{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15e2eefc{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f537f5d{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38ca34ac{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@407d86cf{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e313a4b{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73a36b63{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b357944{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ab511d9{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69a7b8eb{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5518cd5{/static,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@780a24d5{/,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.763 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25f908ab{/api,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.764 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4134db11{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.764 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76a25615{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.764 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a0fcf08{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:04:01.765 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:04:01.771 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:04:01.773 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:05:58.622 [Thread-114] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@720e0dd7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:05:58.956 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:05:58.957 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:05:58.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:05:58.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:05:58.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:05:58.958 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:05:58.960 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:05:58.960 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:05:58.961 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:05:58.961 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:05:59.000 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:05:59.000 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @818109ms
2025-02-18 23:05:59.011 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5128b3d9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:05:59.011 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16aa0e18{/,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@16aa0e18{/,null,STOPPED,@Spark}
2025-02-18 23:05:59.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@942232c{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fe6200{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ee82cf{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.046 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@395c9fe3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39f4f2d6{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12eb9fe5{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@361f25d5{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1aa13612{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@650e8565{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8141f8{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.047 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@440d6b10{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4259ad5b{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77cb6393{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5358303a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@297722d0{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@444ba1c1{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3123d44f{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.048 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7abb6e7e{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64eeb49e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1acc23a3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4418dca5{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f964ad4{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.049 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6627af4b{/static,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.050 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a218b67{/,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.050 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@308a9b4d{/api,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.050 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ec732ce{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.050 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@651e9038{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.050 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b85e32b{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:05:59.052 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:05:59.056 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:05:59.058 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:07:33.948 [Thread-132] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5128b3d9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:07:34.285 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:07:34.285 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:07:34.287 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:07:34.287 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:07:34.287 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:07:34.287 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:07:34.289 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:07:34.290 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:07:34.290 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:07:34.290 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:07:34.333 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:07:34.334 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @913442ms
2025-02-18 23:07:34.345 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@30e880d8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:07:34.345 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50d6594f{/,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@50d6594f{/,null,STOPPED,@Spark}
2025-02-18 23:07:34.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18f28169{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bc0ceb{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11811558{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c7251f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438f9cdd{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@767aceca{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e8e18d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.383 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26390ed6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.383 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29ca8168{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.383 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6db68e87{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.383 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@977e7b1{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.383 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68bf7890{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.383 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75ef3e5b{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.383 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e66d28{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.384 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20202a33{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.384 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7760787c{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.384 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7403aaa7{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.384 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d72ebc7{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.384 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a4cfa3d{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.385 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710d5a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.385 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@103a67b4{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.385 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d24ad9c{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.385 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74792ca9{/static,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.385 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17b84ec6{/,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.386 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72ad4322{/api,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.386 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@249660a0{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.386 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f81209f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.386 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b3f4fd0{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:07:34.388 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:07:34.391 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:07:34.392 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:08:22.521 [Thread-150] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@30e880d8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:08:22.860 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:08:22.861 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:08:22.861 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:08:22.861 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:08:22.861 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:08:22.862 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:08:22.864 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:08:22.864 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:08:22.865 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:08:22.865 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:08:22.905 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:08:22.907 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @962014ms
2025-02-18 23:08:22.917 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@79ff00b8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:08:22.917 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c068d94{/,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4c068d94{/,null,STOPPED,@Spark}
2025-02-18 23:08:22.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cd55300{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fb038d5{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@354a402f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47e023df{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6944cead{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aea3ce{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49032de{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1e7d7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38f743d9{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66026741{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d87390d{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c0b25de{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35d4354e{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a26b970{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b20830{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48f09ff{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41dabaf0{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4898888e{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45fadbd7{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5867be83{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54cccab{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8e6d86e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21d3c0c7{/static,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.954 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75b56f8f{/,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.955 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43624a81{/api,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.955 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ec822bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.955 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79c6fde4{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.955 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45e695e1{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:08:22.956 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:08:22.961 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:08:22.962 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:09:40.281 [Thread-168] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@79ff00b8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:09:40.625 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:09:40.625 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:09:40.626 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:09:40.626 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:09:40.626 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:09:40.626 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:09:40.629 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:09:40.630 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:09:40.630 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:09:40.631 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:09:40.688 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:09:40.689 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1039796ms
2025-02-18 23:09:40.701 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6197e514{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:09:40.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43524a94{/,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@43524a94{/,null,STOPPED,@Spark}
2025-02-18 23:09:40.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60e980e9{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74207839{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.742 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@283f0c0f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7748f0fe{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a024c2e{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fdd8959{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@263ab1a0{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@684b1382{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a74603f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f7e1e42{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c405d08{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7292dc8d{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.743 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c58092d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f774619{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e0812f{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@266a96f2{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51daa433{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6244781a{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ae2853b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@768e7df6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ccd7e0e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64321614{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.744 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26842440{/static,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.745 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@640f2ef{/,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.745 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30758485{/api,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.745 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62d2a58c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.745 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35375cd8{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.745 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ed00eb7{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:40.748 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:09:40.752 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:09:40.754 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:09:54.820 [Thread-186] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6197e514{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:09:55.146 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:09:55.146 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:09:55.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:09:55.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:09:55.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:09:55.147 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:09:55.150 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:09:55.151 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:09:55.151 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:09:55.151 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:09:55.188 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:09:55.189 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1054297ms
2025-02-18 23:09:55.200 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@17d51c13{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:09:55.200 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62975edb{/,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.234 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@62975edb{/,null,STOPPED,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@434ab70a{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@357b995b{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18626646{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@392435c3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f578ba7{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21f147b6{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@724daaa9{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fd40a04{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11a47e6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.235 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13e1be9c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@338a52de{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@316ec805{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@508df4b6{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1433305c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18003152{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1144d8e7{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1517f427{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7bd53960{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b95ce7a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fe11414{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26322de2{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.236 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15caac55{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f03f6b3{/static,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f2a9e45{/,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bcae368{/api,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@248ab9c9{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64d38cb1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.238 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f6e8e65{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:09:55.240 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:09:55.244 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:09:55.245 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:10:52.841 [Thread-204] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@17d51c13{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:10:53.175 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:10:53.176 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:10:53.177 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:10:53.177 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:10:53.177 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:10:53.177 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:10:53.179 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:10:53.179 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:10:53.179 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:10:53.180 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:10:53.218 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:10:53.218 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1112326ms
2025-02-18 23:10:53.229 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4d51b25b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:10:53.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2907b224{/,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.261 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2907b224{/,null,STOPPED,@Spark}
2025-02-18 23:10:53.261 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b382a14{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.261 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bc6d40b{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.261 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ba4057{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.261 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62f3faa5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f33f5fe{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d015780{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34ea4de3{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16f0fbb1{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3233fbf7{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@522eb695{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ae4a4b2{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@591ee14c{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.262 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@798570d9{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32a9e847{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ac57acc{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30b78c81{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f7aa905{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23b10c0e{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52f20750{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@244c5612{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.263 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b4f5bce{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.264 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@252f5e5d{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.264 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3552013f{/static,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.264 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@224a910c{/,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.264 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61d1230{/api,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.264 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74886b7d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.264 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@911258e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.265 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33b79f8f{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:10:53.266 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:10:53.269 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:10:53.272 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:11:25.947 [Thread-222] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4d51b25b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:11:26.289 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:11:26.289 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:11:26.290 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:11:26.290 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:11:26.290 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:11:26.290 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:11:26.292 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:11:26.293 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:11:26.293 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:11:26.294 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:11:26.331 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:11:26.332 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1145439ms
2025-02-18 23:11:26.343 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7fa5462e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:11:26.343 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75a77190{/,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@75a77190{/,null,STOPPED,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66777d29{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183ad430{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@357c3d9e{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3eba34ec{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e092b87{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1eb7fd0e{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@684579be{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.379 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1576fb60{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.379 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30dab311{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.379 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67d3a7f7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.379 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c79ae44{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.379 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24df9afc{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a76026d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37eca715{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52d6e950{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9a6a468{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bdb3ddc{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fba1319{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@751e02fc{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.380 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23fd68c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f62f754{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fecf253{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d0001b7{/static,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18183234{/,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54a958bc{/api,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.381 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a556259{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3631eb44{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.382 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35b6a5f9{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:26.382 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:11:26.386 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:11:26.389 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:11:37.787 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7fa5462e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:11:43.859 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:11:43.862 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:11:43.862 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:11:43.863 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:11:43.863 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:11:43.863 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:11:43.871 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:11:44.235 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:11:44.268 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:11:44.451 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:11:45.071 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3979ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 23:11:45.148 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:11:45.161 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4070ms
2025-02-18 23:11:45.204 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@ea16541{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:11:45.218 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50a88064{/,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.368 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@50a88064{/,null,STOPPED,@Spark}
2025-02-18 23:11:45.369 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9c34c39{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.370 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43b7fc82{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.370 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e4298a7{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.370 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2447098c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.371 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d035e7{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.371 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a3c0957{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.371 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@718a33b4{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1da1c3c4{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f463b85{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@192a6354{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56f6708{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7459c273{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e700b0a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1335bb43{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26d82a50{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.376 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cbaefca{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.376 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2193e64a{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.376 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b35da2{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.377 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b91e790{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.377 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bab1a3f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.377 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d68ee46{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.378 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a058dad{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.385 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30c829d2{/static,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.385 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7452e6b8{/,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.388 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@503348b8{/api,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.388 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3403f5d0{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.389 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e8f394d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.392 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7413117d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:11:45.473 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:11:45.505 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:11:45.507 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:12:02.800 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-18 23:12:02.801 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-18 23:12:02.802 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 正在处理CSV文件: file:///C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-18 23:12:02.869 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 23:12:02.887 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2152504a{/SQL,null,AVAILABLE,@Spark}
2025-02-18 23:12:02.889 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6632f3b0{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 23:12:02.890 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6224a5da{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 23:12:02.890 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67581070{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 23:12:02.892 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bbbd98{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 23:12:06.589 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 保存结果到CSV文件: Data\demo\processed
2025-02-18 23:12:06.836 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 文件处理成功:Demo.csv
2025-02-18 23:21:00.481 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@ea16541{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:21:06.000 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:21:06.003 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:21:06.003 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:21:06.003 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:21:06.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:21:06.004 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:21:06.012 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:21:06.314 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:21:06.337 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:21:06.503 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:21:07.058 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3768ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 23:21:07.121 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:21:07.133 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3843ms
2025-02-18 23:21:07.164 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7b72788e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:21:07.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f39ce11{/,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.310 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1f39ce11{/,null,STOPPED,@Spark}
2025-02-18 23:21:07.312 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f007326{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.312 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@166bb5b3{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.313 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@257a22eb{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.314 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e4e912{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.314 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23ff2a24{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.315 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a4055bd{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.315 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f83153a{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d3c187f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.316 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64daa669{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39520ab0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@217b2b04{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c1c9697{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d19fe22{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50569376{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7199efb3{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d480a2e{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ff5e0c8{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@540c955f{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.321 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ee30474{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.321 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@303ab0ba{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.321 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7aa6354b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.321 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c6f856d{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69663d53{/static,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ddddca5{/,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@760495bc{/api,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.331 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a57e23{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.331 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c121641{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.335 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@202e17c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:07.408 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:21:07.439 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:21:07.442 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:21:10.361 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-18 23:21:10.362 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-18 23:21:10.362 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 正在处理CSV文件: file:///C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-18 23:21:10.415 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 23:21:10.431 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2dff55{/SQL,null,AVAILABLE,@Spark}
2025-02-18 23:21:10.433 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef66428{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:10.433 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@758b4473{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 23:21:10.433 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44684011{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:10.434 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17085e00{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 23:21:13.699 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 保存结果到CSV文件: Data\demo\processed
2025-02-18 23:21:13.937 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 文件处理成功:Demo.csv
2025-02-18 23:21:47.057 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7b72788e{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:21:50.900 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:21:50.903 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:21:50.904 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:21:50.904 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:21:50.904 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:21:50.904 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:21:50.914 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:21:51.208 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:21:51.231 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:21:51.382 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:21:51.939 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3784ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 23:21:52.010 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:21:52.024 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3870ms
2025-02-18 23:21:52.057 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@27701083{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:21:52.071 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d696e0e{/,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.208 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3d696e0e{/,null,STOPPED,@Spark}
2025-02-18 23:21:52.209 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2447098c{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.209 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d035e7{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.210 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29e5e9b9{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.210 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eb47640{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.210 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@718a33b4{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.211 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1da1c3c4{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.211 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56f6708{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.211 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7459c273{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.212 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e700b0a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.212 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1335bb43{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.213 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26d82a50{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.213 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cbaefca{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.214 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2193e64a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.214 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b35da2{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.214 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b91e790{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bab1a3f{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d68ee46{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.215 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a058dad{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30c829d2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f218bed{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2303e1ef{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.217 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@113c26e2{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.223 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@592a7bce{/static,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.223 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@364ce73c{/,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b114a2d{/api,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4de466b0{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a96459b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.228 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74ff42a9{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:21:52.298 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:21:52.342 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:21:52.346 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:37:20.629 [http-nio-8080-exec-4] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 23:37:20.645 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@266155a0{/SQL,null,AVAILABLE,@Spark}
2025-02-18 23:37:20.646 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5257c388{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 23:37:20.646 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41f25096{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 23:37:20.647 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2530205a{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 23:37:20.648 [http-nio-8080-exec-4] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@576a794e{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 23:37:24.067 [http-nio-8080-exec-4] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-18 23:38:42.988 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@27701083{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:38:49.161 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-18 23:38:49.163 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-18 23:38:49.165 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-18 23:38:49.165 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-18 23:38:49.165 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-18 23:38:49.165 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-18 23:38:49.174 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-18 23:38:49.475 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:38:49.501 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-18 23:38:49.668 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-18 23:38:50.229 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3870ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-18 23:38:50.292 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-18 23:38:50.303 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3946ms
2025-02-18 23:38:50.336 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@38f66d98{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-18 23:38:50.349 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f9bf2{/,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4f9bf2{/,null,STOPPED,@Spark}
2025-02-18 23:38:50.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@491ebf0e{/jobs,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.485 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2f3e51{/jobs/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.485 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69bab39c{/jobs/job,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.485 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c685c5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.487 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf2ded5{/stages,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.487 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d0ead2{/stages/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.488 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33a523cd{/stages/stage,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.488 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53938dd8{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.488 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b07dd4c{/stages/pool,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.488 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7adc3a16{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.489 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fdd485d{/storage,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.489 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3584b5aa{/storage/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.489 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62564d31{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.490 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45fca5fe{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.490 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11aa347e{/environment,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.490 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f7a4ba3{/environment/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.491 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60ac2d0e{/executors,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.491 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cb9db3a{/executors/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.491 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52b3279c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.492 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ca18c3c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.492 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315037ac{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.492 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3074b24b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.498 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5961da64{/static,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.499 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e9ed83e{/,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.501 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54771f0{/api,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.502 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5724faf3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.502 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@96eccc8{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.505 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cddce95{/metrics/json,null,AVAILABLE,@Spark}
2025-02-18 23:38:50.574 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-18 23:38:50.600 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-18 23:38:50.602 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-18 23:39:00.131 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-18 23:39:00.148 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b4b81d2{/SQL,null,AVAILABLE,@Spark}
2025-02-18 23:39:00.149 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1730167a{/SQL/json,null,AVAILABLE,@Spark}
2025-02-18 23:39:00.149 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51faa849{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-18 23:39:00.149 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@293881b8{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-18 23:39:00.150 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37dc311e{/static/sql,null,AVAILABLE,@Spark}
2025-02-18 23:39:03.746 [http-nio-8080-exec-2] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 00:03:54.644 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@38f66d98{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:03:55.113 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:03:55.114 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:03:55.115 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:03:55.115 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:03:55.115 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:03:55.116 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:03:55.121 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:03:55.121 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:03:55.122 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:03:55.122 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:03:55.170 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:03:55.171 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1508813ms
2025-02-19 00:03:55.184 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4699bd72{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:03:55.185 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@414ac10c{/,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.222 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@414ac10c{/,null,STOPPED,@Spark}
2025-02-19 00:03:55.223 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15c4dcec{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.223 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65cd7281{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.223 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d2088b3{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ada9eba{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5df6996c{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@152864fe{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.224 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55fe1232{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4faad1cc{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6842c3f6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69a3aa0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.225 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74823dda{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17eeec1{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c49461{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@723235b9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.226 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b18ca67{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a12d319{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@371c86f0{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@374bf5a6{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.227 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f53165a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@671e27b7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7557425d{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd46300{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.229 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a4aad11{/static,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.230 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@615f1ef4{/,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.230 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79a7e944{/api,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.230 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a8d2ecd{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.231 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68f3ef61{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.232 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@421c3229{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:03:55.232 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:03:55.237 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:03:55.241 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:04:02.015 [Thread-23] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4699bd72{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:04:02.386 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:04:02.386 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:04:02.387 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:04:02.387 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:04:02.387 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:04:02.387 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:04:02.390 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:04:02.390 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:04:02.391 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:04:02.391 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:04:02.429 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:04:02.430 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1516072ms
2025-02-19 00:04:02.440 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5284dd76{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:04:02.441 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51c60294{/,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@51c60294{/,null,STOPPED,@Spark}
2025-02-19 00:04:02.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41e9f250{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28f54183{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.477 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59f96847{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.478 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dedb8c4{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.478 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b204e10{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.478 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14ed2ab9{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.478 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4998d042{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.479 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1460e090{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.479 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@110ae0ef{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.479 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4623b9a4{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.479 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67dd2b42{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.480 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c73ee45{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.480 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56280433{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.480 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26c3464c{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.480 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65322143{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.481 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@206ce401{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.481 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a47ed5c{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.481 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18ff21b0{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.481 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c928af3{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.482 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@165b75a8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.482 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@224cbc26{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.482 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@594b5b14{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.482 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fe55dc7{/static,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c45a51b{/,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2adc5cb6{/api,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1467ce29{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2490d28a{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.484 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6baa0079{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:02.485 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:04:02.490 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:04:02.492 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:04:40.311 [Thread-41] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5284dd76{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:04:40.675 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:04:40.676 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:04:40.677 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:04:40.677 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:04:40.677 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:04:40.677 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:04:40.679 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:04:40.681 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:04:40.681 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:04:40.682 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:04:40.718 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:04:40.720 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1554361ms
2025-02-19 00:04:40.731 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2d2a7921{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:04:40.731 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29330ff4{/,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.768 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@29330ff4{/,null,STOPPED,@Spark}
2025-02-19 00:04:40.768 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12d54948{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.768 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47d36a17{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.768 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76004598{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42f38672{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@462c64c0{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6045f69c{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51d6f1ef{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@796dd799{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@279dd693{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f11964b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f4a9f1{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@266f2a97{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c353106{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c4c5a40{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.770 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1223659c{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6856cbcf{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6819434c{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7057c795{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.771 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ddd359c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.772 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2695ccc3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.772 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@df33d88{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.772 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b7fa353{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.772 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@622837dc{/static,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.773 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33d90cc8{/,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.773 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c7c0f9f{/api,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.773 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6522c747{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.774 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d08ad6b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.774 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e087a93{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:04:40.775 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:04:40.779 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:04:40.782 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:07:37.811 [Thread-59] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2d2a7921{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:07:38.160 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:07:38.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:07:38.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:07:38.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:07:38.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:07:38.161 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:07:38.163 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:07:38.164 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:07:38.164 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:07:38.165 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:07:38.201 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:07:38.201 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1731844ms
2025-02-19 00:07:38.213 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@a45d329{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:07:38.213 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@584a7939{/,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@584a7939{/,null,STOPPED,@Spark}
2025-02-19 00:07:38.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2da0f3{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.247 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b36248e{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.247 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d1bfe5{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eda5378{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e20c0a5{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56927ee4{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@709f79db{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.249 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422d7400{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.249 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64b48c29{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.249 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@208a18a2{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.249 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ce068ef{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.250 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72887104{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.250 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a464b8b{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.250 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10ccfba7{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.250 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2af4b071{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.250 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11d53696{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.250 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29114538{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.250 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68c387e5{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.251 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@252ac5c4{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.251 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@526c91cb{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.251 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c6215fa{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.251 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ae04681{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.252 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@560d8b33{/static,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.252 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@413a34c6{/,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.252 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7554fae3{/api,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.252 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13eab300{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.252 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61cbb616{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.253 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c8f3dce{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:38.254 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:07:38.258 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:07:38.259 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:07:48.200 [Thread-77] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@a45d329{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:07:48.712 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:07:48.714 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:07:48.714 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:07:48.715 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:07:48.715 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:07:48.715 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:07:48.719 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:07:48.719 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:07:48.719 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:07:48.721 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:07:48.769 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:07:48.769 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1742411ms
2025-02-19 00:07:48.781 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1b49b5b0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:07:48.782 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1100665c{/,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.824 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1100665c{/,null,STOPPED,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c777e85{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66b599aa{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1930e22{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22411c2f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632c1a01{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c220108{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a043a96{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6463836{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.825 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7accdb7b{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65a5ce87{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@645ec973{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5376ac7c{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17e2ed47{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d419f65{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25ca8dcf{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.826 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5967275f{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1be46986{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42303d3c{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19ddab13{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15daa73f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ada5462{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@392273f6{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4045783a{/static,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.827 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1da1fc37{/,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c80e58a{/api,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4999af7{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16df6b67{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1b79be{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:07:48.830 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:07:48.837 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:07:48.839 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:08:29.884 [Thread-95] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1b49b5b0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:08:30.240 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:08:30.240 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:08:30.241 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:08:30.241 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:08:30.241 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:08:30.241 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:08:30.243 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:08:30.243 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:08:30.244 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:08:30.244 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:08:30.281 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:08:30.281 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1783923ms
2025-02-19 00:08:30.292 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@415067e9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:08:30.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2cabdeea{/,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2cabdeea{/,null,STOPPED,@Spark}
2025-02-19 00:08:30.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f18803d{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7823f3fa{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@146269dc{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32ca8587{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ad5bbf0{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@119e1d7f{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4be8da5a{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f22f935{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41033a80{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@498028c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3055efdb{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15cc0ad9{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32617f93{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@318c0b89{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ecdccba{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@129c489{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bf3bf66{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46611382{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6352a287{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38caeb0b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a6cb4d0{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b0c92cd{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.329 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b908e5f{/static,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d920ada{/,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d4467ed{/api,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74fef727{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7690d1d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.330 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a70adac{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:30.331 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:08:30.335 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:08:30.337 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:08:41.260 [Thread-113] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@415067e9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:08:41.592 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:08:41.593 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:08:41.594 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:08:41.594 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:08:41.594 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:08:41.594 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:08:41.595 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:08:41.596 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:08:41.596 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:08:41.597 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:08:41.631 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:08:41.632 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1795274ms
2025-02-19 00:08:41.641 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@24938a40{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:08:41.641 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e057dec{/,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7e057dec{/,null,STOPPED,@Spark}
2025-02-19 00:08:41.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74beeecd{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2df89296{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@449f8624{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@268f24f8{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f2888a1{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cd45b0a{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34c970be{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51588ba6{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d645969{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@538f7120{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@329efa64{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22a88d6f{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@583cdb62{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@379bca1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@250d783{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4deb012d{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9c641d{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6552d410{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20db3f3b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.678 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9afec20{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.679 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7947eabf{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.679 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f0839e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.679 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20a257dc{/static,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.679 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17ca8a3d{/,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.679 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@719bb355{/api,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.680 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@653c1339{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.680 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4488a5da{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.680 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9142fb8{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:08:41.681 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:08:41.685 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:08:41.687 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:09:39.875 [Thread-131] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@24938a40{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:09:40.214 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:09:40.215 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:09:40.215 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:09:40.215 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:09:40.215 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:09:40.215 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:09:40.219 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:09:40.220 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:09:40.220 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:09:40.220 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:09:40.254 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:09:40.256 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1853898ms
2025-02-19 00:09:40.267 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@35566268{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:09:40.268 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d06ba97{/,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.300 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6d06ba97{/,null,STOPPED,@Spark}
2025-02-19 00:09:40.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d91852d{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11cdb18f{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4515ccaa{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50aa22c2{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6240b8{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ba12c6e{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f4f4b4d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232d11e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c2d265a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a4a3b91{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e54529b{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e2dcc8e{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c03ff83{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e70fbf9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7052b282{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f43a88d{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cec7ad3{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51170544{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fb2fdd2{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43b27cb2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32173fbf{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.304 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b6374c9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62993dd3{/static,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f02e468{/,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d630a10{/api,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1aabefa3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.305 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5678670f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.306 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7bc4c79d{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:09:40.307 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:09:40.311 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:09:40.313 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:11:13.478 [Thread-149] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@35566268{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:11:13.820 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:11:13.821 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:11:13.821 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:11:13.821 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:11:13.821 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:11:13.821 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:11:13.824 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:11:13.824 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:11:13.824 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:11:13.825 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:11:13.859 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:11:13.860 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @1947501ms
2025-02-19 00:11:13.870 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4e47b718{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:11:13.870 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16bbb201{/,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.902 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@16bbb201{/,null,STOPPED,@Spark}
2025-02-19 00:11:13.902 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79aa65fd{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.902 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4afc9cf2{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.902 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ebd09c{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.902 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ac2dcb5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78f47440{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e7c7ae5{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18f0c8aa{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e51b366{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c3c8bd7{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@660eb590{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21d91fb4{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18833a69{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.903 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b58420{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d2d1eb4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b6a382{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70838024{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e74bde1{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1012d5e1{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@656aca62{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78712ae8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32378af1{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fae8cda{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.906 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bdc9c8d{/static,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.906 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@403a0c71{/,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.906 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3dbba834{/api,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.906 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bbda4d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.906 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9521464{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.907 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24248e4{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:11:13.908 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:11:13.911 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:11:13.914 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4e47b718{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:12:34.711 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:12:34.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:12:34.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:12:34.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:12:34.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:12:34.712 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:12:34.714 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:12:34.714 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:12:34.716 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:12:34.716 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:12:34.751 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:12:34.752 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @2028393ms
2025-02-19 00:12:34.762 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4a263f01{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:12:34.762 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41d8179f{/,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.800 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@41d8179f{/,null,STOPPED,@Spark}
2025-02-19 00:12:34.800 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ad7ba87{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.800 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@516e59d7{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.800 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d193f1a{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.800 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66322bd6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.800 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c7519a{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.800 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c31ce9e{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.801 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405e99fa{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.801 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@689c4eaa{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.801 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71dea5cd{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.801 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78a7cd4b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.801 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6298cbae{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.801 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77a1f43d{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.801 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52fe570e{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e9ea1f0{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61569783{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5941caeb{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a7cedb5{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24dcf859{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a7a3608{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.802 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5130eeca{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.803 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ec340de{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.803 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34557ff6{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.803 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b44672{/static,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.804 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33d4ea90{/,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.804 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c5706b1{/api,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7679cfa3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56e50edf{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.805 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72b11bd2{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:12:34.807 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:12:34.813 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:12:34.815 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4a263f01{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:16:59.960 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 00:16:59.963 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - 正在验证Hadoop环境...
2025-02-19 00:16:59.964 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 00:16:59.964 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 00:16:59.964 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - PATH是否包含hadoop bin目录: 是
2025-02-19 00:16:59.964 [restartedMain] INFO  sparkanalysis.util.HadoopConfigTest - Hadoop环境验证成功
2025-02-19 00:16:59.976 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: SparkAnalysis
2025-02-19 00:17:00.305 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:17:00.329 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 00:17:00.491 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 00:17:01.085 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3838ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 00:17:01.151 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 00:17:01.163 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3916ms
2025-02-19 00:17:01.197 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5e4ea582{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 00:17:01.210 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@145abfbb{/,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.348 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@145abfbb{/,null,STOPPED,@Spark}
2025-02-19 00:17:01.349 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1adcc5e5{/jobs,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.349 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16ce833f{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.350 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31262bee{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.350 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35ee9c3e{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.350 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7aa517f8{/stages,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.350 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e1336f4{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.351 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c5c6b2{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.351 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f89c0f9{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.352 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20e5e4a8{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.352 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4103a7ea{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.353 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5376a4a1{/storage,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.353 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ed3f6d7{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.353 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9460a5{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20240a1f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69bc6da8{/environment,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@599557a5{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46bdb52f{/executors,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.356 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6203a097{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37a56160{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e78d298{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bc1f59a{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.358 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@741bf723{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.363 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e0cce36{/static,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.364 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1537257f{/,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.366 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b5ab608{/api,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.366 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e017491{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.366 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@636e5a09{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.370 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16eb655e{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 00:17:01.442 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 00:17:01.472 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 00:17:01.475 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证完成
2025-02-19 00:17:14.374 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5e4ea582{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:18:59.378 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:18:59.379 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:18:59.381 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:18:59.381 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:18:59.381 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:18:59.382 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:18:59.382 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:18:59.382 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:18:59.382 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:18:59.383 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:18:59.383 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:18:59.383 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:18:59.383 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:18:59.392 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:18:59.777 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:18:59.811 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:19:00.023 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:19:00.666 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4346ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 11:19:00.753 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:19:00.770 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4449ms
2025-02-19 11:19:00.813 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@59c315d7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:19:00.829 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@678c1b82{/,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@678c1b82{/,null,STOPPED,@Spark}
2025-02-19 11:19:00.991 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b602e1e{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ccc00a4{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.992 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cac827f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.993 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1020df2f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.993 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@734a3eb0{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.994 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51ba12f0{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20637da3{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.995 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44bbcbc0{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61abecad{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@456bed1a{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.996 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51ce011d{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.997 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32fd23ac{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.998 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e0f4977{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.998 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d9ceb3a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.998 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@453333c2{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.999 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4397ed09{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.999 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32cbc689{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:19:00.999 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77f8a692{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27c0ea12{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41fbee9a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@365cb485{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73c133e4{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5413080e{/static,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.008 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35193ee0{/,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.009 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@775aa075{/api,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.010 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11366d99{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.010 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1620769a{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.015 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ef307c0{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:19:01.096 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:19:01.129 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:22:58.321 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@59c315d7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:23:26.357 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:23:26.358 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:23:26.359 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:23:26.359 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:23:26.359 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:23:26.360 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:23:26.360 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:23:26.360 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:23:26.361 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:23:26.361 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:23:26.361 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:23:26.361 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:23:26.361 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:23:26.370 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:23:26.700 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:23:26.724 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:23:26.885 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:23:27.475 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4020ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 11:23:27.544 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:23:27.558 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4103ms
2025-02-19 11:23:27.594 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@71dd96cd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:23:27.607 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69b043e9{/,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.751 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@69b043e9{/,null,STOPPED,@Spark}
2025-02-19 11:23:27.752 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dbe9ae4{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.753 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c8a13f{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.753 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@308aa5e3{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f71a64c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a063e33{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.754 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b2bf5fb{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.755 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2945ae42{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.755 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@287e4c0e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.756 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22eba777{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.756 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46fede6f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.757 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e3c2d10{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.757 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@258ef4dd{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.757 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@247a0d5f{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79d13b8b{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@198c1641{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.758 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a78fe05{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@498f0f94{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52f4c4da{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.759 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12c1a9b4{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@659ae20b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.760 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4639e556{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.761 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4faf62c{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.766 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71e5ea5{/static,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.767 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57d848fa{/,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7163a55c{/api,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34c7a5d3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.769 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7690b95b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.773 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4edd921{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:23:27.846 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:23:27.876 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:24:26.046 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 11:24:26.070 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b67197b{/SQL,null,AVAILABLE,@Spark}
2025-02-19 11:24:26.071 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bae5e52{/SQL/json,null,AVAILABLE,@Spark}
2025-02-19 11:24:26.071 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@371a1e73{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-19 11:24:26.071 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@384c7c69{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-19 11:24:26.073 [http-nio-8080-exec-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4956294a{/static/sql,null,AVAILABLE,@Spark}
2025-02-19 11:24:30.658 [http-nio-8080-exec-2] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 11:30:01.590 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-19 11:30:01.591 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-19 11:30:01.592 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 正在处理CSV文件: file:///C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-19 11:30:01.736 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 保存结果到CSV文件: Data\demo\processed
2025-02-19 11:30:01.785 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\.part-00000-e1ff2502-4c80-4dab-b0c5-ed590cce2626-c000.csv.crc]: it still exists.
2025-02-19 11:30:01.786 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\._SUCCESS.crc]: it still exists.
2025-02-19 11:30:01.787 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\part-00000-e1ff2502-4c80-4dab-b0c5-ed590cce2626-c000.csv]: it still exists.
2025-02-19 11:30:01.787 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\_SUCCESS]: it still exists.
2025-02-19 11:30:01.788 [ForkJoinPool.commonPool-worker-1] ERROR s.s.processor.impl.CsvFileProcessor - 保存CSV文件失败: Data\demo\processed
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/processed prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
	at sparkanalysis.service.processor.impl.CsvFileProcessor.save(CsvFileProcessor.java:58)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:67)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:50)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-19 11:30:01.791 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.RuntimeException: 保存CSV文件失败: Data\demo\processed
	at sparkanalysis.service.processor.impl.CsvFileProcessor.save(CsvFileProcessor.java:61)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:67)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:50)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Caused by: org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/processed prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
	at sparkanalysis.service.processor.impl.CsvFileProcessor.save(CsvFileProcessor.java:58)
	... 9 common frames omitted
2025-02-19 11:30:15.088 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@71dd96cd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:34:09.764 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:34:09.767 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:34:09.768 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:34:09.768 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:34:09.768 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:34:09.768 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:34:09.768 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:34:09.769 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:34:09.769 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:34:09.769 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:34:09.769 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:34:09.770 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:34:09.770 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:34:09.779 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:34:10.115 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:34:10.140 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:34:10.303 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:34:10.881 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4000ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 11:34:10.951 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:34:10.965 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4085ms
2025-02-19 11:34:11.000 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@736a9fe0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:34:11.014 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60697ca1{/,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.166 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@60697ca1{/,null,STOPPED,@Spark}
2025-02-19 11:34:11.167 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f3a729e{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@363b7d4a{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f2629b2{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.168 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62dfd4fb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.169 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68e82f13{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.170 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@96d1e71{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46d3cd97{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f356289{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.171 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d123a81{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d2b7888{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.172 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b0802bb{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37a50691{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cc60040{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.173 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@443b36fe{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@595ce3e9{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.174 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@369e0c06{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c0b9347{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b77f531{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.175 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5abb7612{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a5fc7d3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.176 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@138ae3c8{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.177 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72751b8b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.183 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2454a1be{/static,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.183 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79231a33{/,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.185 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dc5c26a{/api,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.185 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32e91b02{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.185 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@509743f1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.189 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b11ed08{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:11.269 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:34:11.301 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:34:15.452 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-19 11:34:15.453 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-19 11:34:15.453 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 正在处理CSV文件: file:///C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-19 11:34:15.506 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 11:34:15.524 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@651838fe{/SQL,null,AVAILABLE,@Spark}
2025-02-19 11:34:15.524 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78099307{/SQL/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:15.524 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31ce135b{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-19 11:34:15.525 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@231ff54a{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-19 11:34:15.526 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23347d46{/static/sql,null,AVAILABLE,@Spark}
2025-02-19 11:34:18.938 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 保存结果到CSV文件: Data\demo\processed
2025-02-19 11:34:18.993 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\.part-00000-e1ff2502-4c80-4dab-b0c5-ed590cce2626-c000.csv.crc]: it still exists.
2025-02-19 11:34:18.993 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\._SUCCESS.crc]: it still exists.
2025-02-19 11:34:18.994 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\part-00000-e1ff2502-4c80-4dab-b0c5-ed590cce2626-c000.csv]: it still exists.
2025-02-19 11:34:18.995 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.hadoop.fs.FileUtil - Failed to delete file or dir [C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\processed\_SUCCESS]: it still exists.
2025-02-19 11:34:18.996 [ForkJoinPool.commonPool-worker-1] ERROR s.s.processor.impl.CsvFileProcessor - 保存CSV文件失败: Data\demo\processed
org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/processed prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
	at sparkanalysis.service.processor.impl.CsvFileProcessor.save(CsvFileProcessor.java:58)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:67)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:50)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2025-02-19 11:34:18.998 [ForkJoinPool.commonPool-worker-1] ERROR s.service.impl.FileServiceImpl - 文件处理失败:Demo.csv
java.lang.RuntimeException: 保存CSV文件失败: Data\demo\processed
	at sparkanalysis.service.processor.impl.CsvFileProcessor.save(CsvFileProcessor.java:61)
	at sparkanalysis.service.impl.FileServiceImpl.processFile(FileServiceImpl.java:67)
	at sparkanalysis.controller.FileController.lambda$uploadFile$0(FileController.java:50)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1796)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Caused by: org.apache.spark.SparkException: Unable to clear output directory file:/C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/processed prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
	at sparkanalysis.service.processor.impl.CsvFileProcessor.save(CsvFileProcessor.java:58)
	... 9 common frames omitted
2025-02-19 11:34:56.264 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@736a9fe0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:35:20.259 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:35:20.261 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:35:20.261 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:35:20.261 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:35:20.262 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:35:20.262 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:35:20.262 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:35:20.263 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:35:20.263 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:35:20.264 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:35:20.264 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:35:20.264 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:35:20.264 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:35:20.272 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:35:20.576 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:35:20.599 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:35:20.760 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:35:21.330 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4239ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 11:35:21.393 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:35:21.406 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4315ms
2025-02-19 11:35:21.438 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6d5f7843{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:35:21.451 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a1708f2{/,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.583 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7a1708f2{/,null,STOPPED,@Spark}
2025-02-19 11:35:21.584 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4135f9d0{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.585 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36dab37c{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.585 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bfce2d2{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.586 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@693b4eb1{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.586 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ab319cb{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.587 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@406f457b{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.588 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cea9abe{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.588 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@504a59a7{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.593 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61c16f22{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.595 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dbb3e73{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.596 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fb99dcb{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.597 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72887130{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.598 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ea9e07d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.598 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d764ed5{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a88802d{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ca4a4e9{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.599 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b5e5a{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50d60e40{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.600 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50404ba{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f458cf0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f679e8c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.601 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a3bdc2e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.606 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@689c9f4c{/static,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.607 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62797cb4{/,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.608 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33fd3e3d{/api,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.610 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72c4a31c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.610 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b923e39{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.614 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b434c61{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:21.689 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:35:21.717 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:35:24.779 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-19 11:35:24.781 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-19 11:35:24.781 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 正在处理CSV文件: file:///C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-19 11:35:24.862 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 11:35:24.888 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58b4385c{/SQL,null,AVAILABLE,@Spark}
2025-02-19 11:35:24.889 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f5788f9{/SQL/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:24.891 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@520f51dd{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-19 11:35:24.891 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22632d34{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-19 11:35:24.894 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4abf26e3{/static/sql,null,AVAILABLE,@Spark}
2025-02-19 11:35:28.450 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 保存结果到CSV文件: Data\demo\processed
2025-02-19 11:35:28.680 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 文件处理成功:Demo.csv
2025-02-19 11:37:58.669 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6d5f7843{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:38:12.982 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:38:12.983 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:38:12.983 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:38:12.983 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:38:12.983 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:38:12.983 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:38:12.983 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:38:12.983 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:38:12.983 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:38:12.984 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:38:12.984 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:38:12.984 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:38:12.984 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:38:12.987 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:38:12.987 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:38:12.988 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:38:12.988 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:38:13.028 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:38:13.028 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @175938ms
2025-02-19 11:38:13.039 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3ad622bc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:38:13.040 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aa82014{/,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.075 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2aa82014{/,null,STOPPED,@Spark}
2025-02-19 11:38:13.076 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20a8e8ef{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.076 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f7917df{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.076 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@728ce62a{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.077 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1577cb71{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.077 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31352e16{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.077 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27b0325a{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.077 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72358c89{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.077 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62d7262f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@144ab696{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@140f3083{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36c267dc{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45c4be{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ef7d522{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.078 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66391bbb{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@84f0658{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54b1c7f7{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@204c6cbe{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f38e33d{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.081 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42a46874{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f04603b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@295041b7{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.083 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@784e3160{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28ad8b32{/static,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@796d7843{/,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6724b825{/api,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8dac65c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@187d66d7{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fa546ac{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:13.087 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:38:13.093 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:38:42.559 [Thread-23] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3ad622bc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:38:42.931 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:38:42.931 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:38:42.932 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:38:42.932 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:38:42.932 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:38:42.932 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:38:42.932 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:38:42.932 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:38:42.932 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:38:42.933 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:38:42.933 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:38:42.933 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:38:42.933 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:38:42.937 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:38:42.937 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:38:42.938 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:38:42.938 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:38:42.977 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:38:42.977 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @205887ms
2025-02-19 11:38:42.989 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@176262f8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:38:42.990 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f3cf58f{/,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.024 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5f3cf58f{/,null,STOPPED,@Spark}
2025-02-19 11:38:43.024 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5b087c{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.024 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40a6c4f8{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.025 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@156ff058{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.025 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cb5615d{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.025 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a63a07{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.025 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cdeafb5{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.025 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@188a138b{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.026 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6eb98619{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.026 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@524bd7e1{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.026 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ecfa8fc{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.026 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4df5aed{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.026 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78ae4c5d{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@354a99ff{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28a269c5{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6706e446{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e1c036e{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.027 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@181cb396{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.028 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ec4a36e{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.028 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@690b9ba6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.028 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c475b91{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.028 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d2363d3{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.029 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@161e02ea{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.030 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3883643b{/static,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.030 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748050ce{/,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.030 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c7cc2f7{/api,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.030 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5130ac8a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.031 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cf2a061{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.031 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e5965f3{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:38:43.032 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:38:43.037 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:39:26.255 [Thread-43] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@176262f8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:39:26.599 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:39:26.599 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:39:26.599 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:39:26.600 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:39:26.600 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:39:26.600 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:39:26.600 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:39:26.600 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:39:26.600 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:39:26.601 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:39:26.601 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:39:26.601 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:39:26.601 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:39:26.604 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:39:26.604 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:39:26.605 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:39:26.605 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:39:26.644 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:39:26.645 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @249555ms
2025-02-19 11:39:26.657 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@a5505f2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:39:26.658 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d89addd{/,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.698 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6d89addd{/,null,STOPPED,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fac8bab{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@294ae021{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67ea4c9b{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bbd9f0b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21f278e0{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@729fadec{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f8d7130{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.699 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c3ff76a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.700 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f13e54{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.700 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7212deca{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.700 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14f0cb2{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.700 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b6d4a2c{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.700 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@32f82a7a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55e2f824{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31af347a{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78bf2c5d{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49180c86{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@451d567{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b3f8841{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.701 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@acbc940{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.703 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29fa897c{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.703 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@233ca1cc{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.703 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63c66e8b{/static,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.703 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dde6984{/,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.704 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d70956{/api,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.704 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d7778c1{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.705 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40a20bbe{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.705 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c4980ea{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:26.706 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:39:26.712 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:39:27.659 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@a5505f2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:39:33.323 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:39:33.325 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:39:33.326 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:39:33.326 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:39:33.326 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:39:33.326 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:39:33.326 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:39:33.327 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:39:33.327 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:39:33.327 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:39:33.327 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:39:33.327 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:39:33.327 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:39:33.336 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:39:33.637 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:39:33.660 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:39:33.821 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:39:34.399 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3909ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 11:39:34.466 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:39:34.480 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3989ms
2025-02-19 11:39:34.514 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@2db85bfa{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:39:34.527 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@136d32b8{/,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.668 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@136d32b8{/,null,STOPPED,@Spark}
2025-02-19 11:39:34.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70b843c3{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.669 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c3b1b07{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24ed9e30{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31441e08{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.670 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b889a22{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.672 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44afeecc{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.672 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b4c2809{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.672 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721419cd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.673 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7681f1d6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.673 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@692cc0d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.673 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f0256f3{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.674 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18949e6b{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.674 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53abf8e8{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.674 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51765f60{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.675 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f17f047{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.675 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ecd2775{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64865522{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7161aef0{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.676 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6be6356b{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@672254e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7aa8968b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.677 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5732a932{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.683 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34e629b1{/static,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.684 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1859f083{/,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.686 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59719918{/api,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.686 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56603bd0{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.686 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1c566{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.690 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1ad8{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:39:34.763 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:39:34.793 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:43:49.711 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-19 11:43:49.712 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-19 11:43:49.713 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 正在处理CSV文件: file:///C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-19 11:43:49.766 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 11:43:49.789 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a4fd644{/SQL,null,AVAILABLE,@Spark}
2025-02-19 11:43:49.790 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f638116{/SQL/json,null,AVAILABLE,@Spark}
2025-02-19 11:43:49.790 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b9f2962{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-19 11:43:49.791 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bc8b9ce{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-19 11:43:49.792 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@361b6f4d{/static/sql,null,AVAILABLE,@Spark}
2025-02-19 11:43:52.952 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 保存结果到CSV文件: Data\demo\processed
2025-02-19 11:43:53.196 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 文件处理成功:Demo.csv
2025-02-19 11:47:40.987 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@2db85bfa{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:47:41.423 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:47:41.424 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:47:41.424 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:47:41.424 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:47:41.424 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:47:41.424 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:47:41.425 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:47:41.425 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:47:41.425 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:47:41.426 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:47:41.426 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:47:41.426 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:47:41.426 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:47:41.430 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:47:41.431 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:47:41.432 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:47:41.432 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:47:41.474 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:47:41.474 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @490985ms
2025-02-19 11:47:41.486 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@59f6e25b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:47:41.487 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a2792ea{/,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.523 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3a2792ea{/,null,STOPPED,@Spark}
2025-02-19 11:47:41.524 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b7a9834{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.524 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@35af1e3f{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.524 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@427e75d4{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.524 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@783d57c7{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.524 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33977769{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.525 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63a7c18f{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.525 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a556197{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.525 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@474d6e96{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.525 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f09c0ea{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.525 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2193cb2{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.526 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@379859fe{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.526 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@341afb4b{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.526 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@698dd81d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.526 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@183495a2{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.527 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a1a38a1{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.527 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a55b94{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.527 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@562fd0a6{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.527 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42c10ef5{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.528 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4eb85f70{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.528 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a340d64{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.528 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50b8e3e3{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.528 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a4cf561{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.529 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aca00b9{/static,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.529 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b9ccc0a{/,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.529 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ec4ff23{/api,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.529 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18cda665{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.530 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79d4b630{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.530 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38b1515f{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:41.532 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:47:41.538 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:47:45.447 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@59f6e25b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:47:50.784 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:47:50.786 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:47:50.787 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:47:50.787 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:47:50.787 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:47:50.787 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:47:50.787 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:47:50.788 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:47:50.788 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:47:50.789 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:47:50.789 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:47:50.789 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:47:50.789 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:47:50.797 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:47:51.098 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:47:51.121 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:47:51.276 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:47:51.826 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3697ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 11:47:51.890 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:47:51.903 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @3773ms
2025-02-19 11:47:51.935 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1c480735{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:47:51.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2352c8a7{/,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.079 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2352c8a7{/,null,STOPPED,@Spark}
2025-02-19 11:47:52.079 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36dab37c{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.079 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bee71ca{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@693b4eb1{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ab319cb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.080 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@406f457b{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.081 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38a26d5d{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.081 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@504a59a7{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.082 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61c16f22{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.082 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dbb3e73{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fb99dcb{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72887130{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.084 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ea9e07d{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d764ed5{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a88802d{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ca4a4e9{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.085 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b5e5a{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50d60e40{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50404ba{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f458cf0{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f679e8c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.087 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a3bdc2e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.089 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@689c9f4c{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.093 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22e92ccb{/static,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.094 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33fd3e3d{/,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.095 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a1c00e7{/api,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.096 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b923e39{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.096 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a89b81e{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.099 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@715bf64e{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:47:52.169 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:47:52.197 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:58:54.764 [Thread-20] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1c480735{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:58:55.256 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:58:55.257 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:58:55.257 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:58:55.257 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:58:55.258 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:58:55.258 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:58:55.258 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:58:55.258 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:58:55.258 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:58:55.259 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:58:55.259 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:58:55.259 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:58:55.259 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:58:55.262 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:58:55.263 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:58:55.264 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:58:55.265 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:58:55.318 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:58:55.319 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @667190ms
2025-02-19 11:58:55.332 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@175aed33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:58:55.333 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47f78015{/,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.386 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@47f78015{/,null,STOPPED,@Spark}
2025-02-19 11:58:55.386 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22a3033{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.387 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66166b19{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.387 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7de4d612{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.388 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ebec48b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.388 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@646a4055{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.388 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26e06d9c{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.389 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f4f6f81{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.389 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@222bab01{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.390 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e52591{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.390 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5378df3d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.390 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d1a35b5{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.391 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44a63439{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.391 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56818753{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.391 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66c4b605{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.391 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3321ce53{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.392 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@517c042c{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.392 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6012a13{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.393 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a8c2a95{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.393 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37bc1d72{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.393 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6781e9a9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.393 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f821b68{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.393 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e6c54f3{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.394 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f22ebf4{/static,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.394 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4533c36{/,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bebc51a{/api,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b7d08a2{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58288d8d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3929d5a6{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:58:55.398 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:58:55.406 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 11:59:52.824 [Thread-22] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@175aed33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:59:53.189 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:59:53.189 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:59:53.189 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:59:53.189 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:59:53.189 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:59:53.190 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 11:59:53.190 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 11:59:53.190 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 11:59:53.190 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 11:59:53.191 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 11:59:53.191 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 11:59:53.191 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 11:59:53.191 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 11:59:53.193 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 11:59:53.194 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:59:53.194 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 11:59:53.195 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 11:59:53.237 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 11:59:53.238 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @725109ms
2025-02-19 11:59:53.251 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6e760a26{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 11:59:53.251 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8d7d94{/,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.291 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4a8d7d94{/,null,STOPPED,@Spark}
2025-02-19 11:59:53.291 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a68dc90{/jobs,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c9718f9{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57a4aec8{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@399da79c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.292 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@334aeea1{/stages,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.294 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43d24ffb{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30a1a169{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b0eec10{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71f1aab6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41f4a7a7{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4634c5f2{/storage,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.296 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a243df0{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a900a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d1695cf{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6791bfa7{/environment,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.297 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ffd0313{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.298 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422ba63e{/executors,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.298 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21767933{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.298 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c5cc015{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.298 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28380a6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.300 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d084249{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.300 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65116803{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ed81092{/static,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e2ea5df{/,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.301 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@490f7e03{/api,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74dcc909{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.302 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6863e73a{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.303 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@99e812b{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 11:59:53.304 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 11:59:53.310 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 12:01:15.382 [Thread-40] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6e760a26{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 12:01:15.724 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 12:01:15.724 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 12:01:15.724 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 12:01:15.724 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 12:01:15.725 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 12:01:15.725 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 12:01:15.725 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 12:01:15.725 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 12:01:15.725 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 12:01:15.726 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 12:01:15.726 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 12:01:15.726 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 12:01:15.726 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 12:01:15.728 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 12:01:15.728 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 12:01:15.729 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 12:01:15.729 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 12:01:15.766 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 12:01:15.767 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @807638ms
2025-02-19 12:01:15.777 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@73f44759{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 12:01:15.778 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a450c0b{/,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.814 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7a450c0b{/,null,STOPPED,@Spark}
2025-02-19 12:01:15.815 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b7da3da{/jobs,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.815 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1222ef4b{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.815 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72770ef6{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.815 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66922e98{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.815 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38580005{/stages,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.816 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52f389fc{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.816 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4da63fe8{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.816 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@322e9251{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c198ab{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11bb835f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44cadd0a{/storage,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57c6fdd1{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.817 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cee81bc{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e27ee4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20b7ed0d{/environment,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e740cf{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.818 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3606dbba{/executors,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18e2cc52{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@542ad92{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7382d936{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d87dbb3{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@556ce00f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@359234b5{/static,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@218aa036{/,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7af1da25{/api,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7224e2bd{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c458887{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.820 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c77b443{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:15.821 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 12:01:15.826 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 12:01:26.719 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 开始处理文件: C:\Users\xubei\Desktop\本地版本可连接\spark\Data\demo\Demo.csv
2025-02-19 12:01:26.720 [ForkJoinPool.commonPool-worker-1] DEBUG s.s.p.factory.FileProcessorFactory - 处理器 CsvFileProcessor 支持 处理文件 Demo.csv
2025-02-19 12:01:26.720 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 正在处理CSV文件: file:///C:/Users/xubei/Desktop/本地版本可连接/spark/Data/demo/Demo.csv
2025-02-19 12:01:26.770 [ForkJoinPool.commonPool-worker-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 12:01:26.784 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@276c4f4f{/SQL,null,AVAILABLE,@Spark}
2025-02-19 12:01:26.784 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dd179bf{/SQL/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:26.785 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d915b0c{/SQL/execution,null,AVAILABLE,@Spark}
2025-02-19 12:01:26.785 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@142c3db4{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-02-19 12:01:26.786 [ForkJoinPool.commonPool-worker-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d52bac7{/static/sql,null,AVAILABLE,@Spark}
2025-02-19 12:01:29.975 [ForkJoinPool.commonPool-worker-1] INFO  s.s.processor.impl.CsvFileProcessor - 保存结果到CSV文件: Data\demo\processed
2025-02-19 12:01:30.223 [ForkJoinPool.commonPool-worker-1] INFO  s.service.impl.FileServiceImpl - 文件处理成功:Demo.csv
2025-02-19 13:28:46.734 [Thread-58] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@73f44759{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:28:47.152 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:28:47.153 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:28:47.153 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:28:47.153 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:28:47.153 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:28:47.154 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:28:47.154 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:28:47.154 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:28:47.154 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:28:47.154 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:28:47.154 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:28:47.154 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:28:47.154 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:28:47.157 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:28:47.157 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:28:47.158 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:28:47.158 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:28:47.194 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:28:47.195 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @6059067ms
2025-02-19 13:28:47.206 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7a262927{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:28:47.207 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cc7a90{/,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.242 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@cc7a90{/,null,STOPPED,@Spark}
2025-02-19 13:28:47.242 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41dbcbeb{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.242 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e9d5ff7{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.242 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1931eb7f{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.243 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e3fb20e{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.243 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4efc0385{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.243 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64daa57f{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.244 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@951aca0{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.244 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3da43446{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.244 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4880b08c{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.244 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3026f7dd{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.245 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@301a1328{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.245 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cf201ac{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.245 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fe760df{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.245 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@446c876{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.245 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ac73b22{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aba8597{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6584f7b6{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cea984f{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4613772c{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d2f897a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.246 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44be1784{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.247 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ab03061{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad79347{/static,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1228a4a2{/,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1798dd6e{/api,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46ec3cc{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.248 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ab848{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.249 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10301f1c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:28:47.250 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:28:47.256 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:30:06.466 [Thread-77] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7a262927{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:30:06.834 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:30:06.835 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:30:06.835 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:30:06.835 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:30:06.835 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:30:06.835 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:30:06.835 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:30:06.835 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:30:06.835 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:30:06.836 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:30:06.836 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:30:06.836 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:30:06.836 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:30:06.839 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:30:06.839 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:30:06.840 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:30:06.840 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:30:06.891 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:30:06.892 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @6138762ms
2025-02-19 13:30:06.903 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@18b106db{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:30:06.904 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9e95ea6{/,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@9e95ea6{/,null,STOPPED,@Spark}
2025-02-19 13:30:06.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41ff509f{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23141781{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6626f155{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14526eae{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@296b672d{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.942 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7735c1ce{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49267f5c{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fbd5e26{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fbf67fa{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@725575f6{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.943 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ae0d243{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56a0b810{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3345435c{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73d032dd{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a9cc44d{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7387edf0{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.944 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dab2965{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ba8fb94{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47105485{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f377297{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c387b7b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1900eb5{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4aa43037{/static,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65e37e29{/,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@48e1d191{/api,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@336fd5ef{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55ccc237{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75a3e902{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:30:06.949 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:30:06.953 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:40:22.847 [Thread-95] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@18b106db{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:40:23.230 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:40:23.232 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:40:23.232 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:40:23.235 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:40:23.235 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:40:23.236 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:40:23.236 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:40:23.275 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:40:23.276 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @6755146ms
2025-02-19 13:40:23.287 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3312efeb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:40:23.287 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ab95442{/,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.323 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7ab95442{/,null,STOPPED,@Spark}
2025-02-19 13:40:23.323 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b50b001{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.323 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27cc5198{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.325 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c449687{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.325 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16c30d6{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.325 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a03ecd1{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.325 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4797f1b5{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.325 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2beac56a{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.325 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41dbddad{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bf2a2ea{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61b9d4b3{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e4b53de{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d57dfdd{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b320bea{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e317156{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71932182{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.326 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36d4ec53{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cb77ecf{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a4847b2{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f0ba725{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2059c703{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@555cbd29{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20efd590{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.327 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77b216e3{/static,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64969ff3{/,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ec85ce6{/api,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3af5607{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17a9429c{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.328 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dbf361{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:40:23.330 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:40:23.336 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:40:23.340 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3312efeb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:43:55.216 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:43:55.217 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:43:55.217 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:43:55.217 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:43:55.217 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:43:55.217 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:43:55.217 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:43:55.217 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:43:55.217 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:43:55.218 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:43:55.218 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:43:55.218 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:43:55.218 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:43:55.221 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:43:55.222 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:43:55.222 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:43:55.223 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:43:55.261 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:43:55.263 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @6967133ms
2025-02-19 13:43:55.273 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@3800ead6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:43:55.273 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20ec54a5{/,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@20ec54a5{/,null,STOPPED,@Spark}
2025-02-19 13:43:55.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@468d41e3{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37e8b6a1{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3817f4cd{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e6ac050{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34d78dfd{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.317 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3971780c{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f2853bf{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3680cb76{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38550dfd{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17b5c801{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a45ae13{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e4ec7c{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31faccfc{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.318 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@690e7de0{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e34abc1{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22ed541a{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10827eb2{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56632b92{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33b7c082{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.319 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@550648b2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@722f7c0b{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@392f1dd7{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@470ab467{/static,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4685d58{/,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a87dbf6{/api,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37886994{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@343dc5db{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.322 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3024b4f3{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:43:55.323 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:43:55.327 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:45:02.937 [Thread-113] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@3800ead6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:45:03.286 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:45:03.286 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:45:03.286 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:45:03.286 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:45:03.286 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:45:03.287 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:45:03.287 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:45:03.287 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:45:03.287 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:45:03.287 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:45:03.287 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:45:03.287 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:45:03.287 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:45:03.290 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:45:03.290 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:45:03.290 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:45:03.291 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:45:03.325 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:45:03.325 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @7035196ms
2025-02-19 13:45:03.336 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@10d153fd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:45:03.336 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@289967e8{/,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.371 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@289967e8{/,null,STOPPED,@Spark}
2025-02-19 13:45:03.371 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ba79310{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.371 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68b6122e{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d6f8684{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e57fad0{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68c3db6b{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4aa21d0f{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cbe2b78{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a5ed45a{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78970496{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.372 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@230776ba{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e530eb2{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15c5fc88{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d89df9f{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c64bf9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66f98c2{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.373 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6044addf{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5113d83e{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16fe5ac1{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75ef19c6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b91f75d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b46a807{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c76b8ab{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.374 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@196276a{/static,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c4ee704{/,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2667bcc1{/api,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6866b5cd{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34238056{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.375 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f9c178a{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:45:03.376 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:45:03.380 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:47:18.413 [Thread-148] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@10d153fd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:47:18.761 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:47:18.761 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:47:18.761 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:47:18.762 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:47:18.762 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:47:18.762 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:47:18.762 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:47:18.762 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:47:18.762 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:47:18.763 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:47:18.763 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:47:18.763 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:47:18.763 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:47:18.765 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:47:18.765 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:47:18.765 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:47:18.766 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:47:18.807 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:47:18.807 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @7170678ms
2025-02-19 13:47:18.817 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@457cd2e8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:47:18.819 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18f051b2{/,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@18f051b2{/,null,STOPPED,@Spark}
2025-02-19 13:47:18.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69470349{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@501043{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a97e934{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7045b34f{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41b3f138{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.855 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57d50481{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e38bf1f{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5359e0f9{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5b9bcb{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251d878c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@224dd741{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d0f23cf{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ecb936b{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71fb5510{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64a3208b{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.856 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ae95848{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c366b11{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43a63ba3{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68f96cdd{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61026861{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72781c71{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.858 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a9e9998{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9ea394b{/static,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6202b679{/,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1815bf59{/api,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69d47477{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76280b6f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.859 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c5334fb{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:47:18.860 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:47:18.865 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:48:27.518 [Thread-166] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@457cd2e8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:48:27.861 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:48:27.861 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:48:27.862 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:48:27.862 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:48:27.862 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:48:27.862 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:48:27.862 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:48:27.862 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:48:27.862 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:48:27.863 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:48:27.863 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:48:27.863 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:48:27.863 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:48:27.864 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:48:27.866 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:48:27.866 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:48:27.866 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:48:27.900 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:48:27.900 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @7239771ms
2025-02-19 13:48:27.911 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6d0861ad{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:48:27.912 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4dcc20d8{/,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4dcc20d8{/,null,STOPPED,@Spark}
2025-02-19 13:48:27.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a2a8c78{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.945 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42648602{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44cf5dc{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f17008a{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19a52215{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@695a6a81{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@573b409{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5042654c{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4917087f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5037a31a{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.946 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11138511{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@959f6b5{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55646d6d{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2adebf35{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16bcf5ed{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57eaa98{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b8a6f8c{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25b7ea19{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40895fef{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.947 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62a3e218{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d467e5e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@91e8996{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61733fcc{/static,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68498de1{/,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77d5b58a{/api,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7daf30f1{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@422c4c92{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@508ab478{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:48:27.950 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:48:27.955 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:49:53.036 [Thread-184] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6d0861ad{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:49:53.369 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:49:53.370 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:49:53.370 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:49:53.370 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:49:53.370 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:49:53.370 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:49:53.370 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:49:53.370 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:49:53.370 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:49:53.371 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:49:53.371 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:49:53.371 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:49:53.371 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:49:53.374 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:49:53.374 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:49:53.374 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:49:53.375 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:49:53.408 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:49:53.409 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @7325279ms
2025-02-19 13:49:53.419 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5affc7d8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:49:53.419 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bd6dd46{/,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3bd6dd46{/,null,STOPPED,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@757f21cd{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@399130fe{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@594c51cd{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@662dca1{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@636b09ff{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@654c4abf{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11c6edeb{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.453 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b15ac31{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e4d55e6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@cc5efab{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@52aef419{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56d2d3ff{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@767e0048{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d93b8b8{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71060efa{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5adbb829{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.454 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d61cd95{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c3a1ce1{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@687b5770{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f3325f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70a19d90{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1ac22{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20ee4d7b{/static,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.455 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c8d50d8{/,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.456 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a92463c{/api,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.456 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f040952{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.456 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79212907{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.456 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d31e39f{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:49:53.457 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:49:53.461 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:52:48.973 [Thread-202] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5affc7d8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:52:49.310 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:52:49.311 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:52:49.311 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:52:49.311 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:52:49.311 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:52:49.311 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:52:49.311 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:52:49.311 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:52:49.311 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:52:49.312 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:52:49.312 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:52:49.312 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:52:49.312 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:52:49.314 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:52:49.314 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:52:49.315 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:52:49.315 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:52:49.350 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:52:49.350 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @7501221ms
2025-02-19 13:52:49.361 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@5effc159{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:52:49.361 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36bb914a{/,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.393 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@36bb914a{/,null,STOPPED,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@146390ef{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c465758{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fe46601{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bcf18d5{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@336ce784{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1eb80822{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37253a1d{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@418ff5dd{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31a20681{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.395 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5072a63d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17cf4248{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@501f0cd6{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4dc7c985{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38ca25e1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@534ba7d1{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7587db4f{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@453c399b{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a65c4fc{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.396 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ed64e95{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.397 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62ca533e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.397 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@662bf5df{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.397 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31ea21e0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.397 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f53bfd2{/static,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.397 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@409ac007{/,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.398 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1deb9397{/api,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.398 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cb97d19{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.398 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf84dd{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.398 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b980688{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:52:49.399 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:52:49.404 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 13:57:47.502 [Thread-220] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@5effc159{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:57:47.860 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:57:47.860 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:57:47.860 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:57:47.860 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:57:47.860 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:57:47.861 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 13:57:47.861 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 13:57:47.861 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 13:57:47.861 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 13:57:47.861 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 13:57:47.861 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 13:57:47.861 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 13:57:47.861 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 13:57:47.863 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 13:57:47.865 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:57:47.865 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 13:57:47.865 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 13:57:47.905 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 13:57:47.905 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @7799776ms
2025-02-19 13:57:47.915 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6b5293b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 13:57:47.916 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f733a31{/,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3f733a31{/,null,STOPPED,@Spark}
2025-02-19 13:57:47.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49e6a2db{/jobs,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46b1b9a1{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.948 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c32e58b{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34e0eb47{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@412d9bbb{/stages,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.949 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e531dbe{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30b479e0{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7424c52d{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65b7ed10{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b535df3{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@580a5a86{/storage,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77277cc4{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c4351c{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a3cfcf8{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.950 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@556ad150{/environment,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@751d7a53{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39a65baa{/executors,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@131ccf7a{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dd3e2fa{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a75b3d8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@120e9b30{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.952 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74d692b4{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@519304f7{/static,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29a1c8c{/,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a15cdcf{/api,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5401a83c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d7f34be{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.953 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18f61690{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 13:57:47.954 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 13:57:47.959 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 14:04:41.802 [Thread-238] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6b5293b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:04:42.259 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:04:42.260 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:04:42.260 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:04:42.260 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:04:42.260 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:04:42.260 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:04:42.260 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:04:42.260 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:04:42.260 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:04:42.261 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:04:42.261 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:04:42.261 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:04:42.261 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:04:42.263 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:04:42.263 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:04:42.264 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:04:42.264 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:04:42.308 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:04:42.309 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @8214180ms
2025-02-19 14:04:42.320 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@49256cb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:04:42.320 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1056d9a4{/,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1056d9a4{/,null,STOPPED,@Spark}
2025-02-19 14:04:42.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ce155b{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49918c52{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1caf8cab{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79cd5699{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@baa5d21{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.354 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2726f052{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ae82e47{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c088e06{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ab032a2{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7511f003{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62944747{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c023805{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57628601{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7651546d{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40973105{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6310a92f{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.355 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2eabfd7f{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24c25e1a{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d0afcc4{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@772adbe9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c57eb0f{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fe570ce{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@140e617a{/static,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16552f86{/,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@417d8e3d{/api,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1128055a{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33e949fb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.357 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14102744{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:04:42.358 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:04:42.364 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 14:14:15.726 [Thread-256] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@49256cb{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:14:21.562 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:14:21.564 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:14:21.564 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:14:21.564 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:14:21.564 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:14:21.565 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:14:21.565 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:14:21.565 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:14:21.565 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:14:21.566 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:14:21.566 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:14:21.566 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:14:21.566 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:14:21.572 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:14:21.572 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:14:21.573 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:14:21.575 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:14:21.636 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:14:21.637 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @8793507ms
2025-02-19 14:14:21.652 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@17952b2c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:14:21.652 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7946460a{/,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.710 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7946460a{/,null,STOPPED,@Spark}
2025-02-19 14:14:21.711 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15bc70a0{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.711 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3278ce2e{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.711 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44d81b91{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.711 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6db0630c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.712 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@610f9ed6{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.712 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@784eb463{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.712 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7893e9e4{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.712 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23403225{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.712 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18c5fc8f{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@178a253f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@285853ef{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14fb1e7a{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1658843f{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b75ac5f{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e47b245{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.713 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14eb33ca{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.715 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e4ce7d2{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.715 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420b4518{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.715 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33cf0501{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.715 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e407a97{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.715 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f8d7f42{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.715 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a617baa{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.716 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76e57892{/static,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.716 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16eba48{/,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.717 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6dc2ec93{/api,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.718 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@111e890b{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.718 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e79b1f1{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.719 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41eb563c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:21.721 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:14:21.731 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 14:14:57.685 [Thread-278] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@17952b2c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:14:58.028 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:14:58.029 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:14:58.029 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:14:58.029 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:14:58.029 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:14:58.029 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:14:58.029 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:14:58.030 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:14:58.030 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:14:58.031 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:14:58.031 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:14:58.031 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:14:58.031 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:14:58.033 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:14:58.033 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:14:58.033 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:14:58.035 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:14:58.072 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:14:58.073 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @8829943ms
2025-02-19 14:14:58.086 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@4e5fb859{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:14:58.086 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e81dc4f{/,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4e81dc4f{/,null,STOPPED,@Spark}
2025-02-19 14:14:58.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33260d5a{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.129 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f7c4f2f{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b28a20d{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2aff1435{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55cb915e{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79d8db0d{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@da4f45{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.130 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@355c928e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d548805{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fbe80ab{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b9e15b2{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63225c04{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@575c49cb{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fd733ab{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@514c0c92{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d8e38f0{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30ded519{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.131 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7760ef10{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@ba4d4ec{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68f334f9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d935e14{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1eeb1937{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46a7df44{/static,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a7113ab{/,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.132 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9a87733{/api,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.133 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7421abdf{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.133 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41ff2b8f{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.133 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ce92255{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:14:58.134 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:14:58.138 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 14:14:58.146 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@4e5fb859{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:04.975 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:15:04.976 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:04.976 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:04.979 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:15:04.979 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:04.980 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:04.980 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:15:05.015 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:15:05.017 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @8836887ms
2025-02-19 14:15:05.028 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@66a88718{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:05.028 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5630095a{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.059 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5630095a{/,null,STOPPED,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a23b98d{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74463842{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@703903a7{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18871d67{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5eac6f97{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@169baa6d{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e45f3c2{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c375dfb{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59c65e66{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.060 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a0419c0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a778e44{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8a7826{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@674415db{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b42465a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5064c7be{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d628b4e{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@634edffc{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75d3910a{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@781018e7{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f9701dc{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e198cb6{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.061 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63464989{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.062 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64b6d7c0{/static,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.062 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9bc12{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.062 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2055b009{/api,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.062 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20060622{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.062 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3832e5bc{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.062 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d60911c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:05.063 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:15:05.068 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 14:15:05.073 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@66a88718{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:26.192 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:26.193 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:26.193 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:26.193 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:26.193 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:26.193 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:15:26.193 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:15:26.194 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:15:26.194 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:26.194 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:26.194 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:26.194 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:26.194 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:26.196 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:15:26.197 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:26.197 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:26.198 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:15:26.233 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:15:26.233 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @8858104ms
2025-02-19 14:15:26.244 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@56d9f1b1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:26.244 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1192f9f9{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.278 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1192f9f9{/,null,STOPPED,@Spark}
2025-02-19 14:15:26.278 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@653606d2{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.278 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@229ffc85{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b8f3e95{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b2339d1{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@253c4c4{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a419c25{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@675d9a00{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f21643b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f6aca3a{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7947caa3{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4262f743{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13ae4d04{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.279 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b36bc64{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12dd15b9{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2effff6f{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72ac71a7{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@660ca898{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@494aea8a{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34ca154d{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f7c0c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c561ab2{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.280 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6754c022{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.281 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23656ad9{/static,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.281 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@722ae033{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4677e21e{/api,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@eaedef9{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55574351{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.282 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a47bc1f{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:26.283 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:15:26.287 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 14:15:36.183 [Thread-313] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@56d9f1b1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:36.537 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:36.537 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:36.537 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:36.537 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:36.537 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:36.538 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:15:36.538 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:15:36.538 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:15:36.538 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:36.538 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:36.538 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:36.538 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:36.538 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:36.541 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:15:36.542 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:36.542 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:36.542 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:15:36.582 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:15:36.583 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @8868454ms
2025-02-19 14:15:36.593 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@20cdc3e4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:36.593 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a99f668{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.628 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6a99f668{/,null,STOPPED,@Spark}
2025-02-19 14:15:36.628 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23313384{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.628 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f4647ed{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a4df44c{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@233fde5b{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34bb89de{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb8205c{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dcbffae{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76dc301b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7265ffa5{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b641900{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c3e84e7{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76e41fbd{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1cc7ca4{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.629 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77c8706e{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@323fba9d{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b8a1f64{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22077a08{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cc2af69{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6afca0a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fd95352{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@710ff758{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a268af8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.630 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49c74d9e{/static,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28eba8b6{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ad6d980{/api,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42107142{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31dfb8af{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.631 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cbc1929{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:36.632 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:15:36.637 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 14:15:50.693 [Thread-377] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@20cdc3e4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:51.049 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:51.050 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:51.050 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:51.050 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:51.050 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:51.050 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:15:51.050 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:15:51.051 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:15:51.051 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:15:51.051 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:15:51.051 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:15:51.051 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:15:51.051 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:15:51.053 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:15:51.054 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:51.054 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:15:51.054 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:15:51.089 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:15:51.090 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @8882961ms
2025-02-19 14:15:51.101 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@7873aa9d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:15:51.101 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1dc01d0d{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1dc01d0d{/,null,STOPPED,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3abb231{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28ab8e79{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72a9350b{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69966c50{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17a0b9f6{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f0bdfc{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.135 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fed9684{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45f11e28{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c3f37b6{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5498d66d{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50d01a{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2750dcdc{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a06fd2a{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fab3ca{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34c0f926{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.137 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b8f314{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5608049b{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b01ba13{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ebc782e{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5330e524{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12114b2e{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b136c2b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@257bae30{/static,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.138 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f344968{/,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.139 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45a44df{/api,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.139 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39e86c7d{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.139 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c831c0c{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.139 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4136e05c{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:15:51.140 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:15:51.145 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@7873aa9d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:24:41.480 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:24:41.482 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:24:41.483 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:24:41.483 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:24:41.483 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:24:41.484 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop组件验证完成
2025-02-19 14:24:41.484 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境初始化成功
2025-02-19 14:24:41.484 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 14:24:41.485 [restartedMain] INFO  s.controller.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 14:24:41.485 [restartedMain] INFO  s.controller.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 14:24:41.485 [restartedMain] INFO  s.controller.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 14:24:41.485 [restartedMain] INFO  s.controller.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 14:24:41.485 [restartedMain] INFO  s.controller.HadoopConfig - Hadoop环境验证成功
2025-02-19 14:24:41.495 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在创建新的SparkSession，应用名称: spark-analysis
2025-02-19 14:24:41.842 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:24:41.875 [restartedMain] WARN  org.apache.spark.SparkConf - The configuration key 'spark.yarn.access.namenodes' has been deprecated as of Spark 2.2 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.
2025-02-19 14:24:42.075 [restartedMain] WARN  org.apache.spark.SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
2025-02-19 14:24:42.696 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4015ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 14:24:42.777 [restartedMain] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 17.0.12+8-LTS-286
2025-02-19 14:24:42.792 [restartedMain] INFO  o.sparkproject.jetty.server.Server - Started @4111ms
2025-02-19 14:24:42.830 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@6e0bf92{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 14:24:42.848 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a8e62fe{/,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5a8e62fe{/,null,STOPPED,@Spark}
2025-02-19 14:24:43.000 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b4c2809{/jobs,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.001 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721419cd{/jobs/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.002 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@692cc0d{/jobs/job,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.002 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f0256f3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.003 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18949e6b{/stages,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.003 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53abf8e8{/stages/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.003 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ecd2775{/stages/stage,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.004 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64865522{/stages/stage/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.004 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7161aef0{/stages/pool,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.004 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6be6356b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.005 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@672254e1{/storage,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.005 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7aa8968b{/storage/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.005 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5732a932{/storage/rdd,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@34e629b1{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@478a451b{/environment,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.007 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f98410a{/environment/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.008 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67bc542f{/executors,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.008 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12ddecc5{/executors/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.008 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ec8593a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.009 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f036a62{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.009 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b6c28e7{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.009 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@759e2651{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.015 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6571485b{/static,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.016 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1c566{/,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.018 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78e4b7{/api,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.018 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cea54e{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.019 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d41f3df{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.023 [restartedMain] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6102aeb2{/metrics/json,null,AVAILABLE,@Spark}
2025-02-19 14:24:43.099 [restartedMain] INFO  sparkanalysis.config.SparkConfig - SparkSession创建成功
2025-02-19 14:24:43.137 [restartedMain] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@6e0bf92{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-02-19 15:20:50.492 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 15:20:50.494 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 15:20:50.495 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 15:20:50.495 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 15:20:50.495 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境验证成功
2025-02-19 15:20:50.496 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop组件验证完成
2025-02-19 15:20:50.496 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境初始化成功
2025-02-19 15:20:50.497 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 15:20:50.497 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 15:20:50.497 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 15:20:50.497 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 15:20:50.497 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 15:20:50.497 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境验证成功
2025-02-19 15:20:51.372 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.
2025-02-19 15:20:51.385 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.
2025-02-19 15:20:51.398 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.
2025-02-19 15:20:51.410 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7080. Attempting port 7081.
2025-02-19 15:20:51.422 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7081. Attempting port 7082.
2025-02-19 15:20:51.434 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7082. Attempting port 7083.
2025-02-19 15:20:51.448 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7083. Attempting port 7084.
2025-02-19 15:20:51.460 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7084. Attempting port 7085.
2025-02-19 15:20:51.471 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7085. Attempting port 7086.
2025-02-19 15:20:51.483 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7086. Attempting port 7087.
2025-02-19 15:20:51.494 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7087. Attempting port 7088.
2025-02-19 15:20:51.508 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7088. Attempting port 7089.
2025-02-19 15:20:51.519 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7089. Attempting port 7090.
2025-02-19 15:20:51.532 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7090. Attempting port 7091.
2025-02-19 15:20:51.544 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7091. Attempting port 7092.
2025-02-19 15:20:51.556 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7092. Attempting port 7093.
2025-02-19 15:20:51.571 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.net.BindException: Cannot assign requested address: bind: Service 'sparkDriver' failed after 16 retries (starting from 7077)! Consider explicitly setting the appropriate port for the service 'sparkDriver' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
	at java.base/sun.nio.ch.Net.bind0(Native Method)
	at java.base/sun.nio.ch.Net.bind(Net.java:555)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:337)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 15:22:31.570 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 15:22:31.571 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 15:22:31.572 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 15:22:31.572 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 15:22:31.572 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境验证成功
2025-02-19 15:22:31.573 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop组件验证完成
2025-02-19 15:22:31.573 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境初始化成功
2025-02-19 15:22:31.574 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 15:22:31.574 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 15:22:31.574 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 15:22:31.574 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 15:22:31.575 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 15:22:31.575 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境验证成功
2025-02-19 15:22:32.431 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.
2025-02-19 15:22:32.444 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.
2025-02-19 15:22:32.456 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.
2025-02-19 15:22:32.468 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7080. Attempting port 7081.
2025-02-19 15:22:32.481 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7081. Attempting port 7082.
2025-02-19 15:22:32.493 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7082. Attempting port 7083.
2025-02-19 15:22:32.504 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7083. Attempting port 7084.
2025-02-19 15:22:32.516 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7084. Attempting port 7085.
2025-02-19 15:22:32.529 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7085. Attempting port 7086.
2025-02-19 15:22:32.540 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7086. Attempting port 7087.
2025-02-19 15:22:32.552 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7087. Attempting port 7088.
2025-02-19 15:22:32.564 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7088. Attempting port 7089.
2025-02-19 15:22:32.578 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7089. Attempting port 7090.
2025-02-19 15:22:32.591 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7090. Attempting port 7091.
2025-02-19 15:22:32.603 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7091. Attempting port 7092.
2025-02-19 15:22:32.615 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7092. Attempting port 7093.
2025-02-19 15:22:32.629 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.net.BindException: Cannot assign requested address: bind: Service 'sparkDriver' failed after 16 retries (starting from 7077)! Consider explicitly setting the appropriate port for the service 'sparkDriver' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
	at java.base/sun.nio.ch.Net.bind0(Native Method)
	at java.base/sun.nio.ch.Net.bind(Net.java:555)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:337)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 15:23:54.804 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 15:23:54.806 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 15:23:54.807 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 15:23:54.807 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 15:23:54.807 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境验证成功
2025-02-19 15:23:54.808 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop组件验证完成
2025-02-19 15:23:54.808 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境初始化成功
2025-02-19 15:23:54.809 [restartedMain] INFO  sparkanalysis.config.SparkConfig - 正在设置Hadoop主目录: C:/Users/xubei/Desktop/本地版本可连接/spark/src/main/hadoop
2025-02-19 15:23:54.809 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - 正在验证Hadoop环境...
2025-02-19 15:23:54.809 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - JAVA_HOME路径: D:\jdk17\jdk-17.0.12
2025-02-19 15:23:54.809 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - HADOOP_HOME路径: C:\Users\xubei\Desktop\hadoop-3.3.6
2025-02-19 15:23:54.809 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - PATH是否包含hadoop bin目录: 是
2025-02-19 15:23:54.809 [restartedMain] INFO  sparkanalysis.config.HadoopConfig - Hadoop环境验证成功
2025-02-19 15:23:55.674 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.
2025-02-19 15:23:55.687 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.
2025-02-19 15:23:55.699 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.
2025-02-19 15:23:55.711 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7080. Attempting port 7081.
2025-02-19 15:23:55.723 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7081. Attempting port 7082.
2025-02-19 15:23:55.735 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7082. Attempting port 7083.
2025-02-19 15:23:55.747 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7083. Attempting port 7084.
2025-02-19 15:23:55.758 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7084. Attempting port 7085.
2025-02-19 15:23:55.771 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7085. Attempting port 7086.
2025-02-19 15:23:55.782 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7086. Attempting port 7087.
2025-02-19 15:23:55.794 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7087. Attempting port 7088.
2025-02-19 15:23:55.806 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7088. Attempting port 7089.
2025-02-19 15:23:55.818 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7089. Attempting port 7090.
2025-02-19 15:23:55.831 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7090. Attempting port 7091.
2025-02-19 15:23:55.843 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7091. Attempting port 7092.
2025-02-19 15:23:55.855 [restartedMain] WARN  org.apache.spark.util.Utils - Service 'sparkDriver' could not bind on port 7092. Attempting port 7093.
2025-02-19 15:23:55.868 [restartedMain] ERROR org.apache.spark.SparkContext - Error initializing SparkContext.
java.net.BindException: Cannot assign requested address: bind: Service 'sparkDriver' failed after 16 retries (starting from 7077)! Consider explicitly setting the appropriate port for the service 'sparkDriver' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
	at java.base/sun.nio.ch.Net.bind0(Native Method)
	at java.base/sun.nio.ch.Net.bind(Net.java:555)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:337)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 15:37:48.571 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:39:36.744 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:39:37.872 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3743ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 15:40:22.616 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:40:23.950 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3894ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 15:40:43.094 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:40:44.279 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3720ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 15:41:35.714 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:41:37.114 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3941ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 15:42:13.219 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:42:14.360 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3635ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 15:42:14.466 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 15:43:52.792 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:43:52.892 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 15:44:32.412 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:44:32.504 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 15:45:14.116 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:45:14.198 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 15:45:24.545 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:45:24.657 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 15:46:42.893 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:46:44.056 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3763ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 15:46:44.157 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 15:56:56.176 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 15:56:57.328 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3866ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 15:56:57.431 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:14:25.918 [http-nio-8080-exec-9] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:14:30.396 [http-nio-8080-exec-9] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 16:16:17.539 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:16:17.634 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:16:45.771 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:16:45.865 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:16:56.600 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:16:57.890 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3964ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:16:57.996 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:17:10.951 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:17:14.616 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 9
2025-02-19 16:17:14.969 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 0
2025-02-19 16:17:15.046 [http-nio-8080-exec-2] ERROR s.s.c.impl.CommonDataCleanerImpl - 数据清洗失败
java.lang.RuntimeException: 清洗后数据为空，请检查清洗规则
	at sparkanalysis.service.common.impl.CommonDataCleanerImpl.cleanData(CommonDataCleanerImpl.java:52)
	at sparkanalysis.interfaces.rest.controller.clean.DataCleanController.cleanCommonData(DataCleanController.java:41)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:18:20.475 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:18:21.667 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3850ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:18:21.767 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:19:07.730 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:19:13.496 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 16:19:17.949 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 16:19:21.781 [http-nio-8080-exec-1] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 16:19:30.671 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739953170667
2025-02-19 16:19:32.713 [dispatcher-event-loop-15] WARN  o.a.spark.scheduler.TaskSetManager - Stage 18 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:19:37.161 [dispatcher-event-loop-13] WARN  o.a.spark.scheduler.TaskSetManager - Stage 19 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:19:45.751 [dispatcher-event-loop-5] WARN  o.a.spark.scheduler.TaskSetManager - Stage 20 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:19:53.898 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 准备保存数据，行数: 655568
2025-02-19 16:19:54.267 [dispatcher-event-loop-11] WARN  o.a.spark.scheduler.TaskSetManager - Stage 23 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:20:06.491 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据已保存到: cleaned/common
2025-02-19 16:21:34.217 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 16:21:37.792 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 16:21:47.081 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739953307081
2025-02-19 16:21:48.334 [dispatcher-event-loop-4] WARN  o.a.spark.scheduler.TaskSetManager - Stage 42 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:21:51.810 [dispatcher-event-loop-10] WARN  o.a.spark.scheduler.TaskSetManager - Stage 43 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:21:52.734 [Executor task launch worker for task 0.0 in stage 43.0 (TID 244)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 43.0 (TID 244)
java.lang.OutOfMemoryError: Java heap space
	at java.base/java.util.Arrays.copyOf(Arrays.java:3537)
	at java.base/java.lang.String.<init>(String.java:575)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3470/0x00000241c80b6788.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3464/0x00000241c80b5318.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3420/0x00000241c80a24f8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:21:52.753 [Executor task launch worker for task 0.0 in stage 43.0 (TID 244)] ERROR o.a.s.u.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 43.0 (TID 244),5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.base/java.util.Arrays.copyOf(Arrays.java:3537)
	at java.base/java.lang.String.<init>(String.java:575)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3470/0x00000241c80b6788.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3464/0x00000241c80b5318.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3420/0x00000241c80a24f8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:21:52.756 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 43.0 (TID 244) (198.18.0.1 executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.base/java.util.Arrays.copyOf(Arrays.java:3537)
	at java.base/java.lang.String.<init>(String.java:575)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3470/0x00000241c80b6788.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3464/0x00000241c80b5318.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3420/0x00000241c80a24f8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-19 16:21:52.758 [task-result-getter-0] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 43.0 failed 1 times; aborting job
2025-02-19 16:21:52.765 [http-nio-8080-exec-2] ERROR s.s.c.impl.CommonDataCleanerImpl - 数据保存失败
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 244) (198.18.0.1 executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.base/java.util.Arrays.copyOf(Arrays.java:3537)
	at java.base/java.lang.String.<init>(String.java:575)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3470/0x00000241c80b6788.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3464/0x00000241c80b5318.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3420/0x00000241c80a24f8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.Dataset.$anonfun$isEmpty$1(Dataset.scala:654)
	at org.apache.spark.sql.Dataset.$anonfun$isEmpty$1$adapted(Dataset.scala:653)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.isEmpty(Dataset.scala:653)
	at sparkanalysis.service.common.impl.CommonDataCleanerImpl.saveCleanedData(CommonDataCleanerImpl.java:100)
	at sparkanalysis.interfaces.rest.controller.clean.DataCleanController.cleanCommonData(DataCleanController.java:44)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.base/java.util.Arrays.copyOf(Arrays.java:3537)
	at java.base/java.lang.String.<init>(String.java:575)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3470/0x00000241c80b6788.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3464/0x00000241c80b5318.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3420/0x00000241c80a24f8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-19 16:26:44.362 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:26:45.751 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4511ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:26:45.863 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:26:52.687 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:26:53.858 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3785ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:26:53.961 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:26:58.350 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:26:58.453 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:27:00.416 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:27:00.500 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:27:12.973 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:27:13.066 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:27:34.786 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:27:36.688 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:27:45.894 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:27:49.828 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:27:52.208 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:28:04.586 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:28:06.467 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:28:06.550 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:28:10.530 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:28:10.608 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:29:30.043 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:29:31.238 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3912ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:29:31.351 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:31:36.446 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:31:42.323 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 16:31:45.861 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 16:31:48.875 [http-nio-8080-exec-1] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 16:31:55.243 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739953915242
2025-02-19 16:31:56.506 [dispatcher-event-loop-2] WARN  o.a.spark.scheduler.TaskSetManager - Stage 18 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:32:00.182 [dispatcher-event-loop-4] WARN  o.a.spark.scheduler.TaskSetManager - Stage 19 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:32:08.576 [dispatcher-event-loop-9] WARN  o.a.spark.scheduler.TaskSetManager - Stage 20 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:32:16.685 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 准备保存数据，行数: 655568
2025-02-19 16:32:17.114 [dispatcher-event-loop-14] WARN  o.a.spark.scheduler.TaskSetManager - Stage 23 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:32:17.225 [Executor task launch worker for task 0.0 in stage 23.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:32:17.227 [Executor task launch worker for task 0.0 in stage 23.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:32:17.247 [Executor task launch worker for task 0.0 in stage 23.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:32:30.072 [http-nio-8080-exec-1] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据已保存到Parquet文件: cleaned/common
2025-02-19 16:33:09.378 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:33:09.487 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:33:14.833 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:33:14.938 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:33:25.382 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:33:26.827 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4741ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:33:26.958 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:34:43.899 [http-nio-8080-exec-4] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:34:49.156 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 16:34:52.518 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 16:34:55.773 [http-nio-8080-exec-4] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 16:35:02.080 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739954102079
2025-02-19 16:35:03.227 [dispatcher-event-loop-9] WARN  o.a.spark.scheduler.TaskSetManager - Stage 18 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:35:07.062 [dispatcher-event-loop-15] WARN  o.a.spark.scheduler.TaskSetManager - Stage 19 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:35:15.250 [dispatcher-event-loop-3] WARN  o.a.spark.scheduler.TaskSetManager - Stage 20 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:35:23.463 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 准备保存数据，行数: 655568
2025-02-19 16:35:23.860 [dispatcher-event-loop-13] WARN  o.a.spark.scheduler.TaskSetManager - Stage 23 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:35:23.960 [Executor task launch worker for task 0.0 in stage 23.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:35:23.961 [Executor task launch worker for task 0.0 in stage 23.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:35:23.975 [Executor task launch worker for task 0.0 in stage 23.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:35:36.564 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据已保存到Parquet文件: cleaned/common
2025-02-19 16:36:42.895 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:36:44.126 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4045ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:36:44.241 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:36:55.709 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:37:01.087 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 16:37:04.536 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 16:37:07.508 [http-nio-8080-exec-2] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 16:37:14.109 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739954234108
2025-02-19 16:37:15.297 [dispatcher-event-loop-10] WARN  o.a.spark.scheduler.TaskSetManager - Stage 18 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:37:19.033 [dispatcher-event-loop-11] WARN  o.a.spark.scheduler.TaskSetManager - Stage 19 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:37:19.360 [Executor task launch worker for task 0.0 in stage 19.0 (TID 120)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 19.0 (TID 120)
java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3469/0x000001bc1e0c75b8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3463/0x000001bc1e0b6fd8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3419/0x000001bc1e0a62c8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:37:19.380 [Executor task launch worker for task 0.0 in stage 19.0 (TID 120)] ERROR o.a.s.u.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 19.0 (TID 120),5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3469/0x000001bc1e0c75b8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3463/0x000001bc1e0b6fd8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3419/0x000001bc1e0a62c8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:37:19.384 [task-result-getter-2] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 19.0 (TID 120) (198.18.0.1 executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3469/0x000001bc1e0c75b8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3463/0x000001bc1e0b6fd8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3419/0x000001bc1e0a62c8.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-19 16:37:19.386 [task-result-getter-2] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 19.0 failed 1 times; aborting job
2025-02-19 16:37:19.390 [http-nio-8080-exec-2] ERROR s.s.c.impl.CommonDataCleanerImpl - 数据保存失败
org.apache.spark.SparkException: Job 13 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1248)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1246)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1246)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3075)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2961)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2961)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2216)
	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.Dataset.$anonfun$isEmpty$1(Dataset.scala:654)
	at org.apache.spark.sql.Dataset.$anonfun$isEmpty$1$adapted(Dataset.scala:653)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.isEmpty(Dataset.scala:653)
	at sparkanalysis.service.common.impl.CommonDataCleanerImpl.saveCleanedData(CommonDataCleanerImpl.java:100)
	at sparkanalysis.interfaces.rest.controller.clean.DataCleanController.cleanCommonData(DataCleanController.java:44)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:42:39.814 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:42:41.049 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4148ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:42:41.163 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:43:09.036 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:43:14.529 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 16:43:17.922 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 16:43:20.962 [http-nio-8080-exec-2] WARN  o.a.s.s.c.util.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-02-19 16:43:28.061 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739954608059
2025-02-19 16:43:29.371 [dispatcher-event-loop-10] WARN  o.a.spark.scheduler.TaskSetManager - Stage 18 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:43:33.188 [dispatcher-event-loop-4] WARN  o.a.spark.scheduler.TaskSetManager - Stage 19 contains a task of very large size (309968 KiB). The maximum recommended task size is 1000 KiB.
2025-02-19 16:43:33.501 [Executor task launch worker for task 0.0 in stage 19.0 (TID 120)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 19.0 (TID 120)
java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3468/0x00000193220c4be8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3462/0x00000193220af5a8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3418/0x00000193220a4ad0.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:43:33.522 [Executor task launch worker for task 0.0 in stage 19.0 (TID 120)] ERROR o.a.s.u.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 19.0 (TID 120),5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3468/0x00000193220c4be8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3462/0x00000193220af5a8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3418/0x00000193220a4ad0.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 16:43:33.525 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 19.0 (TID 120) (198.18.0.1 executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3468/0x00000193220c4be8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3462/0x00000193220af5a8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3418/0x00000193220a4ad0.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

2025-02-19 16:43:33.529 [task-result-getter-0] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 19.0 failed 1 times; aborting job
2025-02-19 16:43:33.533 [http-nio-8080-exec-2] ERROR s.s.c.impl.CommonDataCleanerImpl - 数据保存失败
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 120) (198.18.0.1 executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3468/0x00000193220c4be8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3462/0x00000193220af5a8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3418/0x00000193220a4ad0.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.Dataset.$anonfun$isEmpty$1(Dataset.scala:654)
	at org.apache.spark.sql.Dataset.$anonfun$isEmpty$1$adapted(Dataset.scala:653)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
	at org.apache.spark.sql.Dataset.isEmpty(Dataset.scala:653)
	at sparkanalysis.service.common.impl.CommonDataCleanerImpl.saveCleanedData(CommonDataCleanerImpl.java:100)
	at sparkanalysis.interfaces.rest.controller.clean.DataCleanController.cleanCommonData(DataCleanController.java:44)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.base/java.lang.StringUTF16.newBytesFor(StringUTF16.java:53)
	at java.base/java.lang.String.<init>(String.java:569)
	at java.base/java.lang.String.<init>(String.java:1389)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1378)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
	at org.apache.spark.sql.execution.SparkPlan$$Lambda$3468/0x00000193220c4be8.apply(Unknown Source)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.RDD$$Lambda$3462/0x00000193220af5a8.apply(Unknown Source)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$3418/0x00000193220a4ad0.apply(Unknown Source)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 common frames omitted
2025-02-19 16:54:50.707 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:54:51.918 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4007ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 16:54:52.020 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:55:37.507 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 16:55:37.843 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 16:56:35.581 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 16:56:40.862 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 16:56:43.405 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 16:56:50.402 [Executor task launch worker for task 3.0 in stage 22.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 6.0 in stage 22.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 5.0 in stage 22.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 9.0 in stage 22.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 2.0 in stage 22.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 0.0 in stage 22.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 8.0 in stage 22.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 7.0 in stage 22.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 1.0 in stage 22.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.402 [Executor task launch worker for task 4.0 in stage 22.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 7.0 in stage 22.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 4.0 in stage 22.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 2.0 in stage 22.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 1.0 in stage 22.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 3.0 in stage 22.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 5.0 in stage 22.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 8.0 in stage 22.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 0.0 in stage 22.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 6.0 in stage 22.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.409 [Executor task launch worker for task 9.0 in stage 22.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 16:56:50.430 [Executor task launch worker for task 2.0 in stage 22.0 (TID 117)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 8.0 in stage 22.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 6.0 in stage 22.0 (TID 121)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 9.0 in stage 22.0 (TID 124)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 7.0 in stage 22.0 (TID 122)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 3.0 in stage 22.0 (TID 118)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 0.0 in stage 22.0 (TID 115)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 4.0 in stage 22.0 (TID 119)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 1.0 in stage 22.0 (TID 116)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:50.430 [Executor task launch worker for task 5.0 in stage 22.0 (TID 120)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 16:56:51.830 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739955405980
2025-02-19 16:56:51.830 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据已保存到路径: cleaned/common/COMMON_1739955405980
2025-02-19 16:57:43.246 [http-nio-8080-exec-5] INFO  s.s.impl.ParquetToMysqlServiceImpl - 成功读取Parquet文件，数据行数: 655568
2025-02-19 16:59:40.366 [http-nio-8080-exec-5] INFO  s.s.impl.ParquetToMysqlServiceImpl - 数据已成功导入到MySQL表: clean_data
2025-02-19 17:04:31.740 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:04:45.118 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:04:45.196 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:07:11.412 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:07:11.534 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:07:15.946 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:07:16.080 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:12:29.363 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:12:29.444 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:18:30.856 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:18:30.968 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:18:33.056 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:18:33.157 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:20:14.435 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:20:15.693 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3996ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:20:15.803 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:23:44.182 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:23:45.364 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3776ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:23:45.472 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:25:20.237 [http-nio-8080-exec-5] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 17:30:25.863 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:30:25.999 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:30:52.651 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:30:53.802 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3825ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:30:53.905 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:31:28.300 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 17:32:19.070 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:32:20.228 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3781ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:32:20.334 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:32:27.770 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 17:33:34.215 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:33:34.370 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:35:12.346 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:35:12.435 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:39:57.672 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:39:57.759 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:40:14.887 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:40:16.022 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3700ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:40:16.124 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:40:40.191 [http-nio-8080-exec-6] INFO  s.s.l.impl.LotteryDataCleanerImpl - 开始竞彩数据清洗
2025-02-19 17:40:40.243 [http-nio-8080-exec-6] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 17:40:45.550 [http-nio-8080-exec-6] INFO  s.s.l.impl.LotteryDataCleanerImpl - 原始数据行数:1048575
2025-02-19 17:40:45.970 [http-nio-8080-exec-6] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据类型转换后行数: 1048575
2025-02-19 17:40:46.958 [http-nio-8080-exec-6] INFO  s.s.l.impl.LotteryDataCleanerImpl - 清洗后最终数据行数: 1048575
2025-02-19 17:40:47.266 [Executor task launch worker for task 13.0 in stage 11.0 (TID 81)] ERROR org.apache.spark.executor.Executor - Exception in task 13.0 in stage 11.0 (TID 81)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/9/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/9/3' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 12.0 in stage 11.0 (TID 80)] ERROR org.apache.spark.executor.Executor - Exception in task 12.0 in stage 11.0 (TID 80)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/2' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/2' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 8.0 in stage 11.0 (TID 76)] ERROR org.apache.spark.executor.Executor - Exception in task 8.0 in stage 11.0 (TID 76)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 5.0 in stage 11.0 (TID 73)] ERROR org.apache.spark.executor.Executor - Exception in task 5.0 in stage 11.0 (TID 73)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/12/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/12/3' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 1.0 in stage 11.0 (TID 69)] ERROR org.apache.spark.executor.Executor - Exception in task 1.0 in stage 11.0 (TID 69)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/5/5' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/5/5' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 3.0 in stage 11.0 (TID 71)] ERROR org.apache.spark.executor.Executor - Exception in task 3.0 in stage 11.0 (TID 71)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/1/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/1/3' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 9.0 in stage 11.0 (TID 77)] ERROR org.apache.spark.executor.Executor - Exception in task 9.0 in stage 11.0 (TID 77)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2024/4/27' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2024/4/27' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 11.0 in stage 11.0 (TID 79)] ERROR org.apache.spark.executor.Executor - Exception in task 11.0 in stage 11.0 (TID 79)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/12/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/12/1' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 0.0 in stage 11.0 (TID 68)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 11.0 (TID 68)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/1/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/1/1' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 6.0 in stage 11.0 (TID 74)] ERROR org.apache.spark.executor.Executor - Exception in task 6.0 in stage 11.0 (TID 74)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/8' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/8' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 4.0 in stage 11.0 (TID 72)] ERROR org.apache.spark.executor.Executor - Exception in task 4.0 in stage 11.0 (TID 72)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/11/6' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/11/6' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 2.0 in stage 11.0 (TID 70)] ERROR org.apache.spark.executor.Executor - Exception in task 2.0 in stage 11.0 (TID 70)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/4/10' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/4/10' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 15.0 in stage 11.0 (TID 83)] ERROR org.apache.spark.executor.Executor - Exception in task 15.0 in stage 11.0 (TID 83)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/8/18' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/8/18' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.266 [Executor task launch worker for task 14.0 in stage 11.0 (TID 82)] ERROR org.apache.spark.executor.Executor - Exception in task 14.0 in stage 11.0 (TID 82)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/1/9' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/1/9' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.271 [Executor task launch worker for task 7.0 in stage 11.0 (TID 75)] ERROR org.apache.spark.executor.Executor - Exception in task 7.0 in stage 11.0 (TID 75)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/11/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/11/1' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:40:47.304 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 11.0 in stage 11.0 (TID 79) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/12/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/12/1' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.305 [task-result-getter-1] ERROR o.a.spark.scheduler.TaskSetManager - Task 11 in stage 11.0 failed 1 times; aborting job
2025-02-19 17:40:47.305 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 6.0 in stage 11.0 (TID 74) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/8' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/8' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.305 [task-result-getter-2] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 11.0 (TID 68) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/1/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/1/1' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.305 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 4.0 in stage 11.0 (TID 72) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/11/6' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/11/6' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.306 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 5.0 in stage 11.0 (TID 73) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/12/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/12/3' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.306 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 8.0 in stage 11.0 (TID 76) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.307 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 14.0 in stage 11.0 (TID 82) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/1/9' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/1/9' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.309 [task-result-getter-2] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 3.0 in stage 11.0 (TID 71) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/1/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/1/3' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.309 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 11.0 (TID 69) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/5/5' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/5/5' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.309 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 12.0 in stage 11.0 (TID 80) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/2' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/2' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.309 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 15.0 in stage 11.0 (TID 83) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/8/18' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/8/18' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.309 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 13.0 in stage 11.0 (TID 81) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/9/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/9/3' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.309 [task-result-getter-2] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 2.0 in stage 11.0 (TID 70) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/4/10' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/4/10' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.311 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 7.0 in stage 11.0 (TID 75) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/11/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/11/1' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.311 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 9.0 in stage 11.0 (TID 77) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2024/4/27' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2024/4/27' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:40:47.314 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 10.0 in stage 11.0 (TID 78) (198.18.0.1 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 11 in stage 11.0 failed 1 times, most recent failure: Lost task 11.0 in stage 11.0 (TID 79) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/12/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/12/1' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

Driver stacktrace:)
2025-02-19 17:41:54.810 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:41:54.909 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:42:35.502 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:42:35.585 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:42:50.941 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:42:52.124 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3790ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:42:52.227 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:42:55.243 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 开始竞彩数据清洗
2025-02-19 17:42:55.292 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 17:43:00.508 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 17:43:00.942 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据类型转换后行数: 1048575
2025-02-19 17:43:01.927 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 清洗后最终数据行数: 1048575
2025-02-19 17:43:02.279 [Executor task launch worker for task 2.0 in stage 11.0 (TID 70)] ERROR org.apache.spark.executor.Executor - Exception in task 2.0 in stage 11.0 (TID 70)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/4/10' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/4/10' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.278 [Executor task launch worker for task 15.0 in stage 11.0 (TID 83)] ERROR org.apache.spark.executor.Executor - Exception in task 15.0 in stage 11.0 (TID 83)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/8/18' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/8/18' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.278 [Executor task launch worker for task 8.0 in stage 11.0 (TID 76)] ERROR org.apache.spark.executor.Executor - Exception in task 8.0 in stage 11.0 (TID 76)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.278 [Executor task launch worker for task 13.0 in stage 11.0 (TID 81)] ERROR org.apache.spark.executor.Executor - Exception in task 13.0 in stage 11.0 (TID 81)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/9/3 21:37:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/9/3 21:37:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.279 [Executor task launch worker for task 3.0 in stage 11.0 (TID 71)] ERROR org.apache.spark.executor.Executor - Exception in task 3.0 in stage 11.0 (TID 71)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/1/3 21:58:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/1/3 21:58:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.278 [Executor task launch worker for task 6.0 in stage 11.0 (TID 74)] ERROR org.apache.spark.executor.Executor - Exception in task 6.0 in stage 11.0 (TID 74)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/8 16:52:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/8 16:52:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.279 [Executor task launch worker for task 9.0 in stage 11.0 (TID 77)] ERROR org.apache.spark.executor.Executor - Exception in task 9.0 in stage 11.0 (TID 77)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2024/4/27' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2024/4/27' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.279 [Executor task launch worker for task 5.0 in stage 11.0 (TID 73)] ERROR org.apache.spark.executor.Executor - Exception in task 5.0 in stage 11.0 (TID 73)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/12/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/12/3' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.279 [Executor task launch worker for task 14.0 in stage 11.0 (TID 82)] ERROR org.apache.spark.executor.Executor - Exception in task 14.0 in stage 11.0 (TID 82)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/1/9 14:58:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/1/9 14:58:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.279 [Executor task launch worker for task 1.0 in stage 11.0 (TID 69)] ERROR org.apache.spark.executor.Executor - Exception in task 1.0 in stage 11.0 (TID 69)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/5/5 20:32:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/5/5 20:32:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.279 [Executor task launch worker for task 12.0 in stage 11.0 (TID 80)] ERROR org.apache.spark.executor.Executor - Exception in task 12.0 in stage 11.0 (TID 80)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/2 16:05:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/2 16:05:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.279 [Executor task launch worker for task 0.0 in stage 11.0 (TID 68)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 11.0 (TID 68)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/1/1 8:43:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/1/1 8:43:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.278 [Executor task launch worker for task 4.0 in stage 11.0 (TID 72)] ERROR org.apache.spark.executor.Executor - Exception in task 4.0 in stage 11.0 (TID 72)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/11/6' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/11/6' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.291 [Executor task launch worker for task 11.0 in stage 11.0 (TID 79)] ERROR org.apache.spark.executor.Executor - Exception in task 11.0 in stage 11.0 (TID 79)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/12/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/12/1' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:43:02.318 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 8.0 in stage 11.0 (TID 76) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.320 [task-result-getter-1] ERROR o.a.spark.scheduler.TaskSetManager - Task 8 in stage 11.0 failed 1 times; aborting job
2025-02-19 17:43:02.320 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 14.0 in stage 11.0 (TID 82) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/1/9 14:58:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/1/9 14:58:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.320 [task-result-getter-2] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 6.0 in stage 11.0 (TID 74) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/8 16:52:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/8 16:52:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.320 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 15.0 in stage 11.0 (TID 83) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/8/18' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/8/18' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.321 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 2.0 in stage 11.0 (TID 70) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/4/10' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/4/10' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.321 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 3.0 in stage 11.0 (TID 71) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/1/3 21:58:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/1/3 21:58:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.321 [task-result-getter-2] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 9.0 in stage 11.0 (TID 77) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2024/4/27' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2024/4/27' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.321 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 5.0 in stage 11.0 (TID 73) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/12/3' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/12/3' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.321 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 12.0 in stage 11.0 (TID 80) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/5/2 16:05:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/5/2 16:05:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.321 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 11.0 (TID 68) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/1/1 8:43:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/1/1 8:43:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.322 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 13.0 in stage 11.0 (TID 81) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2021/9/3 21:37:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2021/9/3 21:37:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.325 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 11.0 (TID 69) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/5/5 20:32:00' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/5/5 20:32:00' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.325 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 4.0 in stage 11.0 (TID 72) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2022/11/6' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2022/11/6' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.325 [task-result-getter-2] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 11.0 in stage 11.0 (TID 79) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2020/12/1' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2020/12/1' could not be parsed at index 8
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

2025-02-19 17:43:02.329 [task-result-getter-1] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 7.0 in stage 11.0 (TID 75) (198.18.0.1 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 11.0 failed 1 times, most recent failure: Lost task 8.0 in stage 11.0 (TID 76) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

Driver stacktrace:)
2025-02-19 17:43:02.329 [task-result-getter-3] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 10.0 in stage 11.0 (TID 78) (198.18.0.1 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 8 in stage 11.0 failed 1 times, most recent failure: Lost task 8.0 in stage 11.0 (TID 76) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

Driver stacktrace:)
2025-02-19 17:43:02.330 [http-nio-8080-exec-2] ERROR s.s.l.impl.LotteryDataCleanerImpl - 竞彩数据处理失败
org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 11.0 failed 1 times, most recent failure: Lost task 8.0 in stage 11.0 (TID 76) (198.18.0.1 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to parse '2023/7/29' in the new parser. You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0, or set to "CORRECTED" and treat it as an invalid datetime string.
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)
	at org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)
	at org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:842)
Caused by: java.time.format.DateTimeParseException: Text '2023/7/29' could not be parsed at index 5
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2052)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1880)
	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)
	... 26 common frames omitted
2025-02-19 17:45:11.392 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:45:11.482 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:45:51.835 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:45:52.974 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3689ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:45:53.077 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:45:55.693 [http-nio-8080-exec-3] INFO  s.s.l.impl.LotteryDataCleanerImpl - 开始竞彩数据清洗
2025-02-19 17:45:55.742 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 17:46:00.846 [http-nio-8080-exec-3] INFO  s.s.l.impl.LotteryDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 17:46:01.220 [http-nio-8080-exec-3] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据类型转换后行数: 1048575
2025-02-19 17:46:02.180 [http-nio-8080-exec-3] INFO  s.s.l.impl.LotteryDataCleanerImpl - 清洗后最终数据行数: 1048575
2025-02-19 17:46:03.979 [Executor task launch worker for task 6.0 in stage 13.0 (TID 90)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 8.0 in stage 13.0 (TID 92)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 4.0 in stage 13.0 (TID 88)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 7.0 in stage 13.0 (TID 91)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 9.0 in stage 13.0 (TID 93)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 1.0 in stage 13.0 (TID 85)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 0.0 in stage 13.0 (TID 84)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 5.0 in stage 13.0 (TID 89)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 2.0 in stage 13.0 (TID 86)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.979 [Executor task launch worker for task 3.0 in stage 13.0 (TID 87)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 6.0 in stage 13.0 (TID 90)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 9.0 in stage 13.0 (TID 93)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 0.0 in stage 13.0 (TID 84)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 1.0 in stage 13.0 (TID 85)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 4.0 in stage 13.0 (TID 88)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 5.0 in stage 13.0 (TID 89)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 3.0 in stage 13.0 (TID 87)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 8.0 in stage 13.0 (TID 92)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.986 [Executor task launch worker for task 7.0 in stage 13.0 (TID 91)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:03.987 [Executor task launch worker for task 2.0 in stage 13.0 (TID 86)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:46:04.009 [Executor task launch worker for task 6.0 in stage 13.0 (TID 90)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 8.0 in stage 13.0 (TID 92)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 1.0 in stage 13.0 (TID 85)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 9.0 in stage 13.0 (TID 93)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 5.0 in stage 13.0 (TID 89)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 4.0 in stage 13.0 (TID 88)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 2.0 in stage 13.0 (TID 86)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 3.0 in stage 13.0 (TID 87)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 7.0 in stage 13.0 (TID 91)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:04.009 [Executor task launch worker for task 0.0 in stage 13.0 (TID 84)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:46:05.545 [http-nio-8080-exec-3] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据清洗完成，保存到: cleaned/lottery/LOTTERY_1739958362181
2025-02-19 17:46:05.545 [http-nio-8080-exec-3] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据保存到: cleaned/lottery/LOTTERY_1739958362181
2025-02-19 17:46:56.110 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 17:46:58.345 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 17:47:04.315 [Executor task launch worker for task 1.0 in stage 36.0 (TID 210)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.315 [Executor task launch worker for task 8.0 in stage 36.0 (TID 217)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.315 [Executor task launch worker for task 6.0 in stage 36.0 (TID 215)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.315 [Executor task launch worker for task 1.0 in stage 36.0 (TID 210)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.315 [Executor task launch worker for task 8.0 in stage 36.0 (TID 217)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.315 [Executor task launch worker for task 6.0 in stage 36.0 (TID 215)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.316 [Executor task launch worker for task 0.0 in stage 36.0 (TID 209)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.316 [Executor task launch worker for task 2.0 in stage 36.0 (TID 211)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.316 [Executor task launch worker for task 0.0 in stage 36.0 (TID 209)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.316 [Executor task launch worker for task 2.0 in stage 36.0 (TID 211)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.316 [Executor task launch worker for task 4.0 in stage 36.0 (TID 213)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.317 [Executor task launch worker for task 4.0 in stage 36.0 (TID 213)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.317 [Executor task launch worker for task 1.0 in stage 36.0 (TID 210)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.317 [Executor task launch worker for task 3.0 in stage 36.0 (TID 212)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.317 [Executor task launch worker for task 7.0 in stage 36.0 (TID 216)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.317 [Executor task launch worker for task 9.0 in stage 36.0 (TID 218)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.318 [Executor task launch worker for task 0.0 in stage 36.0 (TID 209)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.318 [Executor task launch worker for task 2.0 in stage 36.0 (TID 211)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.318 [Executor task launch worker for task 7.0 in stage 36.0 (TID 216)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.318 [Executor task launch worker for task 8.0 in stage 36.0 (TID 217)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.318 [Executor task launch worker for task 9.0 in stage 36.0 (TID 218)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.318 [Executor task launch worker for task 6.0 in stage 36.0 (TID 215)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.318 [Executor task launch worker for task 3.0 in stage 36.0 (TID 212)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.318 [Executor task launch worker for task 5.0 in stage 36.0 (TID 214)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.320 [Executor task launch worker for task 4.0 in stage 36.0 (TID 213)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.319 [Executor task launch worker for task 5.0 in stage 36.0 (TID 214)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:47:04.320 [Executor task launch worker for task 3.0 in stage 36.0 (TID 212)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.320 [Executor task launch worker for task 9.0 in stage 36.0 (TID 218)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.321 [Executor task launch worker for task 7.0 in stage 36.0 (TID 216)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:04.321 [Executor task launch worker for task 5.0 in stage 36.0 (TID 214)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:47:05.292 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739958420299
2025-02-19 17:47:05.292 [http-nio-8080-exec-2] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据已保存到路径: cleaned/common/COMMON_1739958420299
2025-02-19 17:48:25.002 [http-nio-8080-exec-8] ERROR s.s.impl.ParquetToMysqlServiceImpl - 导入MySQL失败
java.lang.RuntimeException: 指定路径下没有有效的Parquet文件
	at sparkanalysis.service.impl.ParquetToMysqlServiceImpl.importToMysql(ParquetToMysqlServiceImpl.java:46)
	at sparkanalysis.interfaces.rest.controller.data.ParquetToMysqlController.importToMysql(ParquetToMysqlController.java:23)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:254)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:182)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:917)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:829)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1089)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:979)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)
	at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:205)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:174)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:149)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:482)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:340)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:391)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:896)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1744)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:842)
2025-02-19 17:48:54.555 [http-nio-8080-exec-9] INFO  s.s.impl.ParquetToMysqlServiceImpl - 成功读取Parquet文件，数据行数: 1048575
2025-02-19 17:49:20.612 [http-nio-8080-exec-9] INFO  s.s.impl.ParquetToMysqlServiceImpl - 数据已成功导入到MySQL表: clean_data
2025-02-19 17:50:11.097 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:50:12.477 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4102ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 17:50:12.592 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:50:26.966 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 开始竞彩数据清洗
2025-02-19 17:50:27.032 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 17:50:32.609 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 17:50:32.998 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据类型转换后行数: 1048575
2025-02-19 17:50:33.954 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 清洗后最终数据行数: 1048575
2025-02-19 17:50:35.782 [Executor task launch worker for task 0.0 in stage 13.0 (TID 84)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 6.0 in stage 13.0 (TID 90)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 2.0 in stage 13.0 (TID 86)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 1.0 in stage 13.0 (TID 85)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 9.0 in stage 13.0 (TID 93)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 5.0 in stage 13.0 (TID 89)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 3.0 in stage 13.0 (TID 87)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 8.0 in stage 13.0 (TID 92)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 7.0 in stage 13.0 (TID 91)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.782 [Executor task launch worker for task 4.0 in stage 13.0 (TID 88)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.790 [Executor task launch worker for task 4.0 in stage 13.0 (TID 88)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.788 [Executor task launch worker for task 5.0 in stage 13.0 (TID 89)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.788 [Executor task launch worker for task 7.0 in stage 13.0 (TID 91)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.788 [Executor task launch worker for task 3.0 in stage 13.0 (TID 87)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.788 [Executor task launch worker for task 8.0 in stage 13.0 (TID 92)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.790 [Executor task launch worker for task 1.0 in stage 13.0 (TID 85)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.788 [Executor task launch worker for task 2.0 in stage 13.0 (TID 86)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.790 [Executor task launch worker for task 9.0 in stage 13.0 (TID 93)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.788 [Executor task launch worker for task 6.0 in stage 13.0 (TID 90)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.788 [Executor task launch worker for task 0.0 in stage 13.0 (TID 84)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:50:35.814 [Executor task launch worker for task 4.0 in stage 13.0 (TID 88)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 3.0 in stage 13.0 (TID 87)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 5.0 in stage 13.0 (TID 89)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 8.0 in stage 13.0 (TID 92)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 9.0 in stage 13.0 (TID 93)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 0.0 in stage 13.0 (TID 84)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 7.0 in stage 13.0 (TID 91)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 6.0 in stage 13.0 (TID 90)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 2.0 in stage 13.0 (TID 86)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:35.814 [Executor task launch worker for task 1.0 in stage 13.0 (TID 85)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:50:37.273 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据清洗完成，保存到: cleaned/lottery/LOTTERY_1739958633954
2025-02-19 17:50:37.274 [http-nio-8080-exec-2] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据保存到: cleaned/lottery/LOTTERY_1739958633954
2025-02-19 17:50:58.718 [http-nio-8080-exec-3] INFO  s.s.impl.ParquetToMysqlServiceImpl - 成功读取Parquet文件，数据行数: 1048575
2025-02-19 17:51:24.910 [http-nio-8080-exec-3] INFO  s.s.impl.ParquetToMysqlServiceImpl - 数据已成功导入到MySQL表: clean_data2
2025-02-19 17:51:43.456 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 17:51:45.616 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 17:51:51.629 [Executor task launch worker for task 9.0 in stage 42.0 (TID 241)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.629 [Executor task launch worker for task 0.0 in stage 42.0 (TID 232)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.629 [Executor task launch worker for task 7.0 in stage 42.0 (TID 239)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.629 [Executor task launch worker for task 9.0 in stage 42.0 (TID 241)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.629 [Executor task launch worker for task 0.0 in stage 42.0 (TID 232)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.629 [Executor task launch worker for task 7.0 in stage 42.0 (TID 239)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.629 [Executor task launch worker for task 3.0 in stage 42.0 (TID 235)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.629 [Executor task launch worker for task 2.0 in stage 42.0 (TID 234)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 8.0 in stage 42.0 (TID 240)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 3.0 in stage 42.0 (TID 235)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 2.0 in stage 42.0 (TID 234)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 5.0 in stage 42.0 (TID 237)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 8.0 in stage 42.0 (TID 240)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 6.0 in stage 42.0 (TID 238)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 5.0 in stage 42.0 (TID 237)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 6.0 in stage 42.0 (TID 238)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 1.0 in stage 42.0 (TID 233)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 4.0 in stage 42.0 (TID 236)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 1.0 in stage 42.0 (TID 233)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.630 [Executor task launch worker for task 9.0 in stage 42.0 (TID 241)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 0.0 in stage 42.0 (TID 232)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 4.0 in stage 42.0 (TID 236)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 17:51:51.631 [Executor task launch worker for task 2.0 in stage 42.0 (TID 234)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 7.0 in stage 42.0 (TID 239)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 3.0 in stage 42.0 (TID 235)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 8.0 in stage 42.0 (TID 240)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 5.0 in stage 42.0 (TID 237)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 6.0 in stage 42.0 (TID 238)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.631 [Executor task launch worker for task 1.0 in stage 42.0 (TID 233)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:51.632 [Executor task launch worker for task 4.0 in stage 42.0 (TID 236)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 17:51:52.419 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739958707581
2025-02-19 17:51:52.419 [http-nio-8080-exec-4] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据已保存到路径: cleaned/common/COMMON_1739958707581
2025-02-19 17:53:33.185 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:53:33.272 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 17:55:52.089 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 17:55:52.172 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:18:13.661 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:18:15.008 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @4037ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 18:18:15.131 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:21:22.083 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:21:22.177 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:25:47.045 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:25:47.136 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:25:56.387 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:25:57.544 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3730ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 18:25:57.647 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:27:15.198 [http-nio-8080-exec-2] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 18:27:19.355 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.357 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.357 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.358 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.358 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.359 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.359 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.360 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.360 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.361 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.361 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.362 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:27:19.362 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.869 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.870 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.870 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.871 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.871 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.871 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.871 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.873 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.873 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.874 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.874 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.875 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:30:28.875 [http-nio-8080-exec-4] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.537 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.538 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.539 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.539 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.540 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.541 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.541 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.542 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.542 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.542 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.543 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.543 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:18.543 [http-nio-8080-exec-6] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.732 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.733 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.733 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.733 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.734 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.734 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.734 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.735 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.735 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.735 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.735 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.736 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:30.736 [http-nio-8080-exec-7] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.759 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.760 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.760 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.761 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.761 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.761 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.762 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.762 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.762 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.763 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.763 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.764 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:31:38.764 [http-nio-8080-exec-8] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:34:10.417 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:34:10.517 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:36:57.371 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:36:57.464 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:37:20.273 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:37:20.354 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:37:53.582 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:37:53.662 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:42:51.900 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:42:51.984 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:43:19.523 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:43:20.678 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3662ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 18:43:20.781 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:44:36.453 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-19 18:44:37.617 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @3795ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-19 18:44:37.723 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-19 18:44:51.244 [http-nio-8080-exec-3] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-19 18:44:56.753 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 18:44:59.200 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 清洗后数据行数: 655568
2025-02-19 18:45:05.940 [Executor task launch worker for task 4.0 in stage 22.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 0.0 in stage 22.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 6.0 in stage 22.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 8.0 in stage 22.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 3.0 in stage 22.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 1.0 in stage 22.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 2.0 in stage 22.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 5.0 in stage 22.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 7.0 in stage 22.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.940 [Executor task launch worker for task 9.0 in stage 22.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 0.0 in stage 22.0 (TID 115)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 2.0 in stage 22.0 (TID 117)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 1.0 in stage 22.0 (TID 116)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 6.0 in stage 22.0 (TID 121)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 7.0 in stage 22.0 (TID 122)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 4.0 in stage 22.0 (TID 119)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 3.0 in stage 22.0 (TID 118)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 8.0 in stage 22.0 (TID 123)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 9.0 in stage 22.0 (TID 124)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.946 [Executor task launch worker for task 5.0 in stage 22.0 (TID 120)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:45:05.971 [Executor task launch worker for task 4.0 in stage 22.0 (TID 119)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 3.0 in stage 22.0 (TID 118)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 8.0 in stage 22.0 (TID 123)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 7.0 in stage 22.0 (TID 122)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 5.0 in stage 22.0 (TID 120)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 2.0 in stage 22.0 (TID 117)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 0.0 in stage 22.0 (TID 115)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 1.0 in stage 22.0 (TID 116)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 6.0 in stage 22.0 (TID 121)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:05.971 [Executor task launch worker for task 9.0 in stage 22.0 (TID 124)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:45:07.686 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据清洗完成，生成的数据ID: COMMON_1739961901430
2025-02-19 18:45:07.686 [http-nio-8080-exec-3] INFO  s.s.c.impl.CommonDataCleanerImpl - 数据已保存到路径: cleaned/common/COMMON_1739961901430
2025-02-19 18:47:29.263 [http-nio-8080-exec-5] INFO  s.s.impl.ParquetToMysqlServiceImpl - 成功读取Parquet文件，数据行数: 655568
2025-02-19 18:47:47.095 [http-nio-8080-exec-5] INFO  s.s.impl.ParquetToMysqlServiceImpl - 数据已成功导入到MySQL表: demo_clean_data
2025-02-19 18:48:43.092 [http-nio-8080-exec-7] INFO  s.s.l.impl.LotteryDataCleanerImpl - 开始竞彩数据清洗
2025-02-19 18:48:44.872 [http-nio-8080-exec-7] INFO  s.s.l.impl.LotteryDataCleanerImpl - 原始数据行数: 1048575
2025-02-19 18:48:45.287 [http-nio-8080-exec-7] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据类型转换后行数: 1048575
2025-02-19 18:48:46.342 [http-nio-8080-exec-7] INFO  s.s.l.impl.LotteryDataCleanerImpl - 清洗后最终数据行数: 1048575
2025-02-19 18:48:48.401 [Executor task launch worker for task 9.0 in stage 42.0 (TID 241)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.401 [Executor task launch worker for task 8.0 in stage 42.0 (TID 240)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.401 [Executor task launch worker for task 9.0 in stage 42.0 (TID 241)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.401 [Executor task launch worker for task 5.0 in stage 42.0 (TID 237)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.401 [Executor task launch worker for task 8.0 in stage 42.0 (TID 240)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.401 [Executor task launch worker for task 5.0 in stage 42.0 (TID 237)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 2.0 in stage 42.0 (TID 234)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 0.0 in stage 42.0 (TID 232)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 7.0 in stage 42.0 (TID 239)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 6.0 in stage 42.0 (TID 238)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 2.0 in stage 42.0 (TID 234)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 0.0 in stage 42.0 (TID 232)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 7.0 in stage 42.0 (TID 239)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 6.0 in stage 42.0 (TID 238)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 9.0 in stage 42.0 (TID 241)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.402 [Executor task launch worker for task 3.0 in stage 42.0 (TID 235)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.402 [Executor task launch worker for task 8.0 in stage 42.0 (TID 240)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.403 [Executor task launch worker for task 5.0 in stage 42.0 (TID 237)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.403 [Executor task launch worker for task 1.0 in stage 42.0 (TID 233)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.403 [Executor task launch worker for task 3.0 in stage 42.0 (TID 235)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.403 [Executor task launch worker for task 1.0 in stage 42.0 (TID 233)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.403 [Executor task launch worker for task 4.0 in stage 42.0 (TID 236)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.403 [Executor task launch worker for task 2.0 in stage 42.0 (TID 234)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.403 [Executor task launch worker for task 7.0 in stage 42.0 (TID 239)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.403 [Executor task launch worker for task 0.0 in stage 42.0 (TID 232)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.403 [Executor task launch worker for task 4.0 in stage 42.0 (TID 236)] INFO  o.a.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY
2025-02-19 18:48:48.403 [Executor task launch worker for task 6.0 in stage 42.0 (TID 238)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.404 [Executor task launch worker for task 3.0 in stage 42.0 (TID 235)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.404 [Executor task launch worker for task 1.0 in stage 42.0 (TID 233)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:48.404 [Executor task launch worker for task 4.0 in stage 42.0 (TID 236)] INFO  o.a.p.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2025-02-19 18:48:49.423 [http-nio-8080-exec-7] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据清洗完成，保存到: cleaned/lottery/LOTTERY_1739962126342
2025-02-19 18:48:49.423 [http-nio-8080-exec-7] INFO  s.s.l.impl.LotteryDataCleanerImpl - 数据保存到: cleaned/lottery/LOTTERY_1739962126342
2025-02-19 18:51:11.549 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.550 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.550 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.550 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.551 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.551 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.552 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.552 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.553 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.553 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.554 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.554 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:11.554 [http-nio-8080-exec-3] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.137 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.138 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.138 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.139 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.140 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.140 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.140 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.141 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.141 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.141 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.142 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.142 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-19 18:51:44.142 [http-nio-8080-exec-2] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 17:22:57.235 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-21 17:22:58.612 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @5186ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-21 17:22:58.727 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-21 20:43:37.533 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-21 20:43:39.238 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @5416ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-21 20:43:39.385 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-21 14:17:42.923 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-21 14:17:44.859 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-21 14:17:47.808 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @18575ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-21 14:17:48.284 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-21 14:18:00.952 [driver-heartbeater] WARN  o.a.s.m.GarbageCollectionMetrics - To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
2025-02-21 14:21:25.443 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-21 14:21:27.351 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-21 14:21:29.846 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @16689ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-21 14:21:30.203 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-21 14:21:40.245 [executor-heartbeater] WARN  o.a.s.m.GarbageCollectionMetrics - To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
2025-02-21 14:24:41.230 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-21 14:24:43.005 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-21 14:24:45.387 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @16374ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-21 14:24:45.712 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-21 14:24:58.245 [executor-heartbeater] WARN  o.a.s.m.GarbageCollectionMetrics - To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
2025-02-21 14:26:44.665 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-21 14:26:44.761 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-21 14:31:00.909 [restartedMain] INFO  sparkanalysis.config.SparkConfig - Spark配置初始化完成
2025-02-21 14:31:03.004 [restartedMain] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-02-21 14:31:05.678 [restartedMain] INFO  org.sparkproject.jetty.util.log - Logging initialized @19600ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-02-21 14:31:06.096 [restartedMain] INFO  s.s.p.factory.FileProcessorFactory - 已注册的处理器:CsvFileProcessor,GenericCsvProcessor,JingcaiDataProcessor
2025-02-21 14:31:16.282 [executor-heartbeater] WARN  o.a.s.m.GarbageCollectionMetrics - To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
2025-02-21 14:31:29.306 [http-nio-8080-exec-1] WARN  o.a.spark.sql.internal.SharedState - URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory
2025-02-21 14:31:42.920 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 票号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.922 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 省中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '省'.(line 1, pos 11)

== SQL ==
percentile(省中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.924 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 市中心名称 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '市'.(line 1, pos 11)

== SQL ==
percentile(市中心名称, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.926 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 门店编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '门'.(line 1, pos 11)

== SQL ==
percentile(门店编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.927 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 销售终端编号 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售终端编号, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.928 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 销售日期 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售日期, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.930 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 销售时间 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '销'.(line 1, pos 11)

== SQL ==
percentile(销售时间, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.931 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 体育项目 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '体'.(line 1, pos 11)

== SQL ==
percentile(体育项目, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.933 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 过关方式 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '过'.(line 1, pos 11)

== SQL ==
percentile(过关方式, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.934 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 游戏 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '游'.(line 1, pos 11)

== SQL ==
percentile(游戏, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.935 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 票面金额 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '票'.(line 1, pos 11)

== SQL ==
percentile(票面金额, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.937 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 投注内容 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '投'.(line 1, pos 11)

== SQL ==
percentile(投注内容, array(0.25, 0.5, 0.75))
-----------^^^

2025-02-21 14:31:42.938 [http-nio-8080-exec-1] WARN  s.controller.FileController - 计算列 倍数 的统计信息时出错: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '倍'.(line 1, pos 11)

== SQL ==
percentile(倍数, array(0.25, 0.5, 0.75))
-----------^^^

